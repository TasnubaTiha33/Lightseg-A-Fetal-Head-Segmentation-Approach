{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import re\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ‚úÖ Constants for 224x224\n",
    "IMG_HEIGHT = 224  # Ensure height is 224\n",
    "IMG_WIDTH = 224   # Ensure width is 224\n",
    "CHANNELS = 3  # RGB images\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "# ‚úÖ Class mapping from RGB to class index\n",
    "CLASS_MAP = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0,  # Background\n",
    "}\n",
    "\n",
    "image_dir = r\"D:\\augmented_dataset\\images\"\n",
    "mask_dir = r\"D:\\augmented_dataset\\masks\"\n",
    "\n",
    "# # ‚úÖ Define destination directories\n",
    "train_image_dir = r\"D:\\Updated\\train\\images\"\n",
    "train_mask_dir = r\"D:\\Updated\\train\\masks\"\n",
    "val_image_dir = r\"D:\\Updated\\val\\images\"\n",
    "val_mask_dir = r\"D:\\Updated\\val\\masks\"\n",
    "test_image_dir = r\"D:\\Updated\\test\\images\"\n",
    "test_mask_dir = r\"D:\\Updated\\test\\masks\"\n",
    "\n",
    "# ‚úÖ Fix sorting issue using natural sorting\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Sort filenames numerically instead of lexicographically.\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "# ‚úÖ Convert RGB mask to class index mask\n",
    "def rgb_to_class(mask_array):\n",
    "    \"\"\"Convert RGB mask to single-channel class index mask.\"\"\"\n",
    "    height, width, _ = mask_array.shape\n",
    "    class_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for rgb, class_idx in CLASS_MAP.items():\n",
    "        matches = np.all(mask_array == rgb, axis=-1)  # Ensure exact match\n",
    "        class_mask[matches] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Preprocess Filtered Dataset for 224x224\n",
    "def preprocess_filtered_dataset(image_dir, mask_dir):\n",
    "    \"\"\"Preprocess images & masks: normalize, resize, and convert masks to one-hot encoding.\"\"\"\n",
    "\n",
    "    # ‚úÖ Load and sort filenames correctly\n",
    "    image_filenames = sorted(os.listdir(image_dir), key=natural_sort_key)\n",
    "    mask_filenames = sorted(os.listdir(mask_dir), key=natural_sort_key)\n",
    "\n",
    "    valid_image_paths = []\n",
    "    valid_mask_paths = []\n",
    "\n",
    "    # ‚úÖ Ensure each image has a corresponding mask\n",
    "    for img_file, mask_file in zip(image_filenames, mask_filenames):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            valid_image_paths.append(img_path)\n",
    "            valid_mask_paths.append(mask_path)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping {img_file}: Missing image or mask\")\n",
    "\n",
    "    num_images = len(valid_image_paths)\n",
    "\n",
    "    # ‚úÖ Initialize arrays\n",
    "    X = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=np.float32)\n",
    "    y = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES), dtype=np.float32)  # One-hot encoded masks\n",
    "\n",
    "    print(f\"üöÄ Processing {num_images} filtered images and masks...\")\n",
    "\n",
    "    for idx, (img_path, mask_path) in enumerate(zip(valid_image_paths, valid_mask_paths)):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"‚úÖ Processed {idx}/{num_images} images\")\n",
    "\n",
    "        # ‚úÖ Load and Resize Image\n",
    "        img = cv2.imread(img_path)  # Read image in BGR format\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # Resize to (224,224)\n",
    "        img = img.astype(np.float32) / 255.0  # Normalize\n",
    "\n",
    "        # ‚úÖ Load and Resize Mask\n",
    "        mask = cv2.imread(mask_path)  # Read mask in BGR format\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)  # Resize mask correctly\n",
    "\n",
    "        # ‚úÖ Convert RGB mask to class mask\n",
    "        class_mask = rgb_to_class(mask)\n",
    "\n",
    "        # ‚úÖ One-hot encode the class mask\n",
    "        one_hot_mask = to_categorical(class_mask, num_classes=NUM_CLASSES)\n",
    "\n",
    "        # ‚úÖ Store preprocessed data\n",
    "        X[idx] = img\n",
    "        y[idx] = one_hot_mask\n",
    "\n",
    "        # ‚úÖ Clear memory to prevent memory leaks\n",
    "        del img, mask, class_mask, one_hot_mask\n",
    "        gc.collect()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # ‚úÖ Process dataset splits\n",
    "X_train, y_train = preprocess_filtered_dataset(train_image_dir, train_mask_dir)\n",
    "X_val, y_val = preprocess_filtered_dataset(val_image_dir, val_mask_dir)\n",
    "# X_test, y_test = preprocess_filtered_dataset(test_image_dir, test_mask_dir)\n",
    "\n",
    "# ‚úÖ Print dataset information\n",
    "print(\"\\n‚úÖ Dataset Splits:\")\n",
    "print(f\"  - Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"  - Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "# print(f\"  - Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import re\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ‚úÖ Constants for 224x224\n",
    "IMG_HEIGHT = 224  # Ensure height is 224\n",
    "IMG_WIDTH = 224   # Ensure width is 224\n",
    "CHANNELS = 3  # RGB images\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "# ‚úÖ Class mapping from RGB to class index\n",
    "CLASS_MAP = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0,  # Background\n",
    "}\n",
    "\n",
    "image_dir = r\"D:\\augmented_dataset\\images\"\n",
    "mask_dir = r\"D:\\augmented_dataset\\masks\"\n",
    "\n",
    "# # ‚úÖ Define destination directories\n",
    "train_image_dir = r\"D:\\Updated\\train\\images\"\n",
    "train_mask_dir = r\"D:\\Updated\\train\\masks\"\n",
    "val_image_dir = r\"D:\\Updated\\val\\images\"\n",
    "val_mask_dir = r\"D:\\Updated\\val\\masks\"\n",
    "test_image_dir = r\"D:\\Updated\\synthetic_dataset\\images\"\n",
    "test_mask_dir = r\"D:\\Updated\\synthetic_dataset\\masks\"\n",
    "\n",
    "# ‚úÖ Fix sorting issue using natural sorting\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Sort filenames numerically instead of lexicographically.\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "# ‚úÖ Convert RGB mask to class index mask\n",
    "def rgb_to_class(mask_array):\n",
    "    \"\"\"Convert RGB mask to single-channel class index mask.\"\"\"\n",
    "    height, width, _ = mask_array.shape\n",
    "    class_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for rgb, class_idx in CLASS_MAP.items():\n",
    "        matches = np.all(mask_array == rgb, axis=-1)  # Ensure exact match\n",
    "        class_mask[matches] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Preprocess Filtered Dataset for 224x224\n",
    "def preprocess_filtered_dataset(image_dir, mask_dir):\n",
    "    \"\"\"Preprocess images & masks: normalize, resize, and convert masks to one-hot encoding.\"\"\"\n",
    "\n",
    "    # ‚úÖ Load and sort filenames correctly\n",
    "    image_filenames = sorted(os.listdir(image_dir), key=natural_sort_key)\n",
    "    mask_filenames = sorted(os.listdir(mask_dir), key=natural_sort_key)\n",
    "\n",
    "    valid_image_paths = []\n",
    "    valid_mask_paths = []\n",
    "\n",
    "    # ‚úÖ Ensure each image has a corresponding mask\n",
    "    for img_file, mask_file in zip(image_filenames, mask_filenames):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            valid_image_paths.append(img_path)\n",
    "            valid_mask_paths.append(mask_path)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping {img_file}: Missing image or mask\")\n",
    "\n",
    "    num_images = len(valid_image_paths)\n",
    "\n",
    "    # ‚úÖ Initialize arrays\n",
    "    X = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=np.float32)\n",
    "    y = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES), dtype=np.float32)  # One-hot encoded masks\n",
    "\n",
    "    print(f\"üöÄ Processing {num_images} filtered images and masks...\")\n",
    "\n",
    "    for idx, (img_path, mask_path) in enumerate(zip(valid_image_paths, valid_mask_paths)):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"‚úÖ Processed {idx}/{num_images} images\")\n",
    "\n",
    "        # ‚úÖ Load and Resize Image\n",
    "        img = cv2.imread(img_path)  # Read image in BGR format\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # Resize to (224,224)\n",
    "        img = img.astype(np.float32) / 255.0  # Normalize\n",
    "\n",
    "        # ‚úÖ Load and Resize Mask\n",
    "        mask = cv2.imread(mask_path)  # Read mask in BGR format\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)  # Resize mask correctly\n",
    "\n",
    "        # ‚úÖ Convert RGB mask to class mask\n",
    "        class_mask = rgb_to_class(mask)\n",
    "\n",
    "        # ‚úÖ One-hot encode the class mask\n",
    "        one_hot_mask = to_categorical(class_mask, num_classes=NUM_CLASSES)\n",
    "\n",
    "        # ‚úÖ Store preprocessed data\n",
    "        X[idx] = img\n",
    "        y[idx] = one_hot_mask\n",
    "\n",
    "        # ‚úÖ Clear memory to prevent memory leaks\n",
    "        del img, mask, class_mask, one_hot_mask\n",
    "        gc.collect()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # ‚úÖ Process dataset splits\n",
    "# X_train, y_train = preprocess_filtered_dataset(train_image_dir, train_mask_dir)\n",
    "# X_val, y_val = preprocess_filtered_dataset(val_image_dir, val_mask_dir)\n",
    "X_test, y_test = preprocess_filtered_dataset(test_image_dir, test_mask_dir)\n",
    "\n",
    "# ‚úÖ Print dataset information\n",
    "print(\"\\n‚úÖ Dataset Splits:\")\n",
    "# print(f\"  - Training set: {X_train.shape}, {y_train.shape}\")\n",
    "# print(f\"  - Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"  - Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = preprocess_filtered_dataset(val_image_dir, val_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Constants for 224x224 images\n",
    "IMG_HEIGHT = 224  # Changed from 256 to 224\n",
    "IMG_WIDTH = 224   # Changed from 256 to 224\n",
    "CHANNELS = 3  # RGB images\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "def conv_block(inputs, filters, kernel_size=(3, 3), padding='same', strides=1):\n",
    "    \"\"\"\n",
    "    Double convolution block with batch normalization\n",
    "    \"\"\"\n",
    "    x = layers.Conv2D(filters, kernel_size, strides=strides, padding=padding)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_segnet(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Build SegNet model\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    # Block 1\n",
    "    conv1 = conv_block(inputs, 64)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(conv1)\n",
    "    \n",
    "    # Block 2\n",
    "    conv2 = conv_block(pool1, 128)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(conv2)\n",
    "    \n",
    "    # Block 3\n",
    "    conv3 = conv_block(pool2, 256)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(conv3)\n",
    "    \n",
    "    # Block 4\n",
    "    conv4 = conv_block(pool3, 512)\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2), padding='same')(conv4)\n",
    "    \n",
    "    # Bridge\n",
    "    conv5 = conv_block(pool4, 1024)\n",
    "    \n",
    "    # Decoder\n",
    "    # Block 4\n",
    "    up4 = layers.UpSampling2D(size=(2, 2))(conv5)\n",
    "    up4 = layers.concatenate([up4, conv4], axis=-1)\n",
    "    up_conv4 = conv_block(up4, 512)\n",
    "    \n",
    "    # Block 3\n",
    "    up3 = layers.UpSampling2D(size=(2, 2))(up_conv4)\n",
    "    up3 = layers.concatenate([up3, conv3], axis=-1)\n",
    "    up_conv3 = conv_block(up3, 256)\n",
    "    \n",
    "    # Block 2\n",
    "    up2 = layers.UpSampling2D(size=(2, 2))(up_conv3)\n",
    "    up2 = layers.concatenate([up2, conv2], axis=-1)\n",
    "    up_conv2 = conv_block(up2, 128)\n",
    "    \n",
    "    # Block 1\n",
    "    up1 = layers.UpSampling2D(size=(2, 2))(up_conv2)\n",
    "    up1 = layers.concatenate([up1, conv1], axis=-1)\n",
    "    up_conv1 = conv_block(up1, 64)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(up_conv1)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_segnet(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), \n",
    "                     num_classes=NUM_CLASSES)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# # ‚úÖ Dice Coefficient Metric\n",
    "# def dice_coefficient(y_true, y_pred):\n",
    "#     smooth = 1e-15\n",
    "#     y_true = tf.cast(y_true, tf.float32)\n",
    "#     y_pred = tf.cast(y_pred, tf.float32)\n",
    "#     intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "#     union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "#     dice = (2. * intersection + smooth) / (union + smooth)\n",
    "#     return tf.reduce_mean(dice)\n",
    "\n",
    "# def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "#     class_weights = tf.constant([0.3794, 0.7521, 69.7061, 49.3458], dtype=tf.float32)\n",
    "\n",
    "#     # Ensure y_true has the same shape as y_pred\n",
    "#     y_true = tf.cast(y_true, tf.float32)  # Make sure it's float32 for numerical stability\n",
    "#     y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)  # Avoid log(0)\n",
    "\n",
    "#     # Compute categorical cross-entropy\n",
    "#     loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)  # Sum over the last axis (class axis)\n",
    "\n",
    "#     # Reshape the class weights to match the loss shape\n",
    "#     class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))  # [1, 1, 1, 4]\n",
    "\n",
    "#     # Apply the class weights\n",
    "#     weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)  # Broadcast weights over the batch and spatial dimensions\n",
    "\n",
    " #     return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# # ‚úÖ Dice Loss\n",
    "# def dice_loss(y_true, y_pred):\n",
    "#     smooth = 1e-6\n",
    "#     y_true = tf.cast(y_true, y_pred.dtype)\n",
    "#     intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "#     union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "#     dice = (2. * intersection + smooth) / (union + smooth)\n",
    "#     return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# # ‚úÖ Combined Loss Function\n",
    "# def combined_loss(y_true, y_pred):\n",
    "#     return weighted_categorical_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "# # ‚úÖ Custom Dice Coefficient Metric for Each Class\n",
    "# class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "#     def __init__(self, class_idx, name=None, **kwargs):  \n",
    "#         if name is None:\n",
    "#             name = f\"DiceClass{class_idx}\"  \n",
    "#         super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "#         self.class_idx = class_idx\n",
    "#         self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         y_true_class = y_true[..., self.class_idx]\n",
    "#         y_pred_class = y_pred[..., self.class_idx]\n",
    "\n",
    "#         intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "#         union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "\n",
    "#         dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "#         self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "#     def result(self):\n",
    "#         return self.dice\n",
    "\n",
    "# # ‚úÖ Function to Get Class-wise Metrics\n",
    "# def class_wise_metrics(num_classes=4):\n",
    "#     return [DiceCoefficient(i) for i in range(num_classes)] + [tf.keras.metrics.MeanIoU(num_classes=num_classes)]\n",
    "\n",
    "# # ‚úÖ Compile the Model\n",
    "# model.compile(\n",
    "#     optimizer=Adam(learning_rate=1e-4),\n",
    "#     loss=combined_loss,\n",
    "#     metrics=class_wise_metrics(4)  # Number of classes (adjust if needed)\n",
    "# )\n",
    "\n",
    "# # ‚úÖ Train the Model with the **Filtered Dataset**\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,  # Use the loaded and split data\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=50,\n",
    "#     batch_size=16,  # Adjust based on available resources\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(\n",
    "#             monitor='val_loss', \n",
    "#             patience=7, \n",
    "#             restore_best_weights=True\n",
    "#         ),\n",
    "#         ReduceLROnPlateau(\n",
    "#             monitor='val_loss', \n",
    "#             factor=0.5, \n",
    "#             patience=3, \n",
    "#             min_lr=1e-6\n",
    "#         ),\n",
    "#         ModelCheckpoint(\n",
    "#             '/kaggle/working/best_unet_model_filtered.keras',  # Save in Kaggle working directory with .keras extension\n",
    "#             monitor='val_loss',\n",
    "#             save_best_only=True\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ‚úÖ Dice Coefficient Metric\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Weighted Categorical Crossentropy\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    class_weights = tf.constant([0.3776, 0.7605, 65.8554, 46.2381], dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "    class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))\n",
    "    weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# ‚úÖ Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Custom Dice Coefficient Metric for Each Class\n",
    "class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "    def __init__(self, class_idx, name=None, **kwargs):  \n",
    "        if name is None:\n",
    "            name = f\"DiceClass{class_idx}\"  \n",
    "        super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.class_idx = class_idx\n",
    "        self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_class = y_true[..., self.class_idx]\n",
    "        y_pred_class = y_pred[..., self.class_idx]\n",
    "        intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "        union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "        dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "        self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "    def result(self):\n",
    "        return self.dice\n",
    "\n",
    "# ‚úÖ Function to Get Class-wise Metrics\n",
    "def class_wise_metrics(num_classes=4):\n",
    "    return [DiceCoefficient(i) for i in range(num_classes)] + [tf.keras.metrics.MeanIoU(num_classes=num_classes)]\n",
    "\n",
    "# ‚úÖ Create Data Generator\n",
    "def create_train_generator(X, y, batch_size=16):\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    seed = 42\n",
    "    image_generator = image_datagen.flow(X, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_datagen.flow(y, batch_size=batch_size, seed=seed)\n",
    "    \n",
    "    while True:\n",
    "        X_batch = next(image_generator)\n",
    "        y_batch = next(mask_generator)\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# ‚úÖ Create the generator\n",
    "train_generator = create_train_generator(X_train, y_train, batch_size=16)\n",
    "\n",
    "# ‚úÖ Compile the Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# ‚úÖ Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "def lovasz_softmax_loss(y_true, y_pred, ignore_background=False):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    num_classes = tf.shape(y_true)[-1]\n",
    "    start_class = tf.constant(1 if ignore_background else 0)\n",
    "\n",
    "    def compute_class_loss(c):\n",
    "        y_true_class = y_true[..., c]\n",
    "        y_pred_class = y_pred[..., c]\n",
    "\n",
    "        y_true_flat = tf.reshape(y_true_class, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred_class, [-1])\n",
    "\n",
    "        errors = tf.abs(y_true_flat - y_pred_flat)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], sorted=True)\n",
    "        y_true_sorted = tf.gather(y_true_flat, perm)\n",
    "\n",
    "        gts = tf.reduce_sum(y_true_sorted)\n",
    "        intersection = gts - tf.cumsum(y_true_sorted)\n",
    "        union = gts + tf.cumsum(1. - y_true_sorted)\n",
    "        jaccard = 1. - intersection / union\n",
    "        grad = tf.concat([[jaccard[0]], jaccard[1:] - jaccard[:-1]], 0)\n",
    "\n",
    "        return tf.tensordot(errors_sorted, grad, axes=1)\n",
    "\n",
    "    # Loop through each class using tf.while_loop\n",
    "    losses = tf.TensorArray(dtype=tf.float32, size=num_classes)\n",
    "\n",
    "    def loop_cond(c, losses):\n",
    "        return tf.less(c, num_classes)\n",
    "\n",
    "    def loop_body(c, losses):\n",
    "        loss_c = compute_class_loss(c)\n",
    "        losses = losses.write(c, loss_c)\n",
    "        return c + 1, losses\n",
    "\n",
    "    _, losses = tf.while_loop(loop_cond, loop_body, [start_class, losses])\n",
    "    return tf.reduce_mean(losses.stack())\n",
    "\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss_val = 1 - tf.reduce_mean(dice)\n",
    "\n",
    "    lovasz_loss_val = lovasz_softmax_loss(y_true, tf.nn.softmax(y_pred), ignore_background=False)\n",
    "\n",
    "    return lovasz_loss_val + dice_loss_val\n",
    "\n",
    "\n",
    "# Usage in model compilation\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(4)  # Number of classes\n",
    ")\n",
    "\n",
    "# ‚úÖ Train the Model with the Data Generator\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // 16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=3, \n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_unet_model_onlineDA_128_lovaszloss_segnet.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Number of classes (adjust if needed)\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# ‚úÖ Dice Coefficient (Mean across all classes)\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Weighted Categorical Crossentropy\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    class_weights = tf.constant([0.3776, 0.7605, 65.8554, 46.2381], dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "    class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))\n",
    "    weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# ‚úÖ Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Lov√°sz-Softmax Loss\n",
    "def lovasz_softmax_loss(y_true, y_pred, ignore_background=False):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    num_classes = tf.shape(y_true)[-1]\n",
    "    start_class = tf.constant(1 if ignore_background else 0)\n",
    "\n",
    "    def compute_class_loss(c):\n",
    "        y_true_class = y_true[..., c]\n",
    "        y_pred_class = y_pred[..., c]\n",
    "\n",
    "        y_true_flat = tf.reshape(y_true_class, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred_class, [-1])\n",
    "\n",
    "        errors = tf.abs(y_true_flat - y_pred_flat)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], sorted=True)\n",
    "        y_true_sorted = tf.gather(y_true_flat, perm)\n",
    "\n",
    "        gts = tf.reduce_sum(y_true_sorted)\n",
    "        intersection = gts - tf.cumsum(y_true_sorted)\n",
    "        union = gts + tf.cumsum(1. - y_true_sorted)\n",
    "        jaccard = 1. - intersection / union\n",
    "        grad = tf.concat([[jaccard[0]], jaccard[1:] - jaccard[:-1]], 0)\n",
    "\n",
    "        return tf.tensordot(errors_sorted, grad, axes=1)\n",
    "\n",
    "    # Loop through classes using tf.while_loop\n",
    "    losses = tf.TensorArray(dtype=tf.float32, size=num_classes)\n",
    "\n",
    "    def loop_cond(c, losses):\n",
    "        return tf.less(c, num_classes)\n",
    "\n",
    "    def loop_body(c, losses):\n",
    "        loss_c = compute_class_loss(c)\n",
    "        losses = losses.write(c, loss_c)\n",
    "        return c + 1, losses\n",
    "\n",
    "    _, losses = tf.while_loop(loop_cond, loop_body, [start_class, losses])\n",
    "    return tf.reduce_mean(losses.stack())\n",
    "\n",
    "# ‚úÖ Combined Loss\n",
    "def combined_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice_loss_val = 1 - (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss_val = tf.reduce_mean(dice_loss_val)\n",
    "    \n",
    "    lovasz_loss_val = lovasz_softmax_loss(y_true, tf.nn.softmax(y_pred), ignore_background=False)\n",
    "    return lovasz_loss_val + dice_loss_val\n",
    "\n",
    "class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "    def __init__(self, class_idx=0, name=None, **kwargs):  # <- default class_idx=0 to avoid missing arg\n",
    "        if name is None:\n",
    "            name = f\"DiceClass{class_idx}\"\n",
    "        super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.class_idx = class_idx\n",
    "        self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_class = y_true[..., self.class_idx]\n",
    "        y_pred_class = y_pred[..., self.class_idx]\n",
    "        intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "        union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "        dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "        self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "    def result(self):\n",
    "        return self.dice\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"class_idx\": self.class_idx})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        if \"class_idx\" not in config:\n",
    "            # Try to extract class index from name like \"DiceClass2\"\n",
    "            name = config.get(\"name\", \"DiceClass0\")\n",
    "            if name.startswith(\"DiceClass\"):\n",
    "                config[\"class_idx\"] = int(name.replace(\"DiceClass\", \"\"))\n",
    "            else:\n",
    "                config[\"class_idx\"] = 0\n",
    "        return cls(**config)\n",
    "\n",
    "# ‚úÖ Helper to load Dice metrics by name\n",
    "def dice_metric_loader(name):\n",
    "    if name.startswith(\"DiceClass\"):\n",
    "        class_idx = int(name.replace(\"DiceClass\", \"\"))\n",
    "        return DiceCoefficient(class_idx=class_idx)\n",
    "    raise ValueError(f\"Unknown Dice metric name: {name}\")\n",
    "\n",
    "# ‚úÖ Register all custom objects for loading the model\n",
    "custom_objects = {\n",
    "    'combined_loss': combined_loss,\n",
    "    'lovasz_softmax_loss': lovasz_softmax_loss,\n",
    "    'MeanIoU': tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES),\n",
    "    'DiceCoefficient': DiceCoefficient,\n",
    "}\n",
    "\n",
    "# ‚úÖ Add DiceClass0‚Äì3 dynamically\n",
    "for i in range(NUM_CLASSES):\n",
    "    custom_objects[f'DiceClass{i}'] = dice_metric_loader(f'DiceClass{i}')\n",
    "\n",
    "# ‚úÖ Load the model\n",
    "model_segnet = load_model('C:\\\\Users\\\\User\\\\best_unet_model_onlineDA_128_lovaszloss_segnet.keras', custom_objects=custom_objects)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class ImageMaskGenerator(Sequence):\n",
    "    def __init__(self, image_paths, mask_paths, batch_size=4, num_classes=4, img_size=(224, 224), shuffle=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.image_paths))\n",
    "        self.CLASS_MAP = {\n",
    "            (255, 0, 0): 1,\n",
    "            (0, 255, 0): 2,\n",
    "            (0, 0, 255): 3,\n",
    "            (0, 0, 0): 0,\n",
    "        }\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_images = []\n",
    "        batch_masks = []\n",
    "\n",
    "        for i in batch_indices:\n",
    "            img = cv2.imread(self.image_paths[i])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "\n",
    "            mask = cv2.imread(self.mask_paths[i])\n",
    "            mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "            mask = cv2.resize(mask, self.img_size, interpolation=cv2.INTER_NEAREST)\n",
    "            mask = self.rgb_to_class(mask)\n",
    "            mask = tf.keras.utils.to_categorical(mask, num_classes=self.num_classes)\n",
    "\n",
    "            batch_images.append(img)\n",
    "            batch_masks.append(mask)\n",
    "\n",
    "        return np.array(batch_images), np.array(batch_masks)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def rgb_to_class(self, mask_array):\n",
    "        h, w, _ = mask_array.shape\n",
    "        class_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        for rgb, class_idx in self.CLASS_MAP.items():\n",
    "            matches = np.all(mask_array == rgb, axis=-1)\n",
    "            class_mask[matches] = class_idx\n",
    "        return class_mask\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def load_paths(image_dir, mask_dir):\n",
    "    images = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "    masks = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.png')])\n",
    "    return images, masks\n",
    "\n",
    "train_imgs, train_masks = load_paths(train_image_dir, train_mask_dir)\n",
    "val_imgs, val_masks = load_paths(val_image_dir, val_mask_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageMaskGenerator(train_imgs, train_masks, batch_size=8)\n",
    "val_gen = ImageMaskGenerator(val_imgs, val_masks, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(X_train, y_train, X_val, y_val, batch_size=8, epochs=3, repeats=1):\n",
    "    from tensorflow.keras import backend as K\n",
    "    import gc\n",
    "\n",
    "    epoch_times_all = []\n",
    "    power_samples_all = []\n",
    "\n",
    "    for r in range(repeats):\n",
    "        print(f\"\\nüîÅ Repeat {r+1}/{repeats}\")\n",
    "\n",
    "        # Clean up previous session\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        model = tf.keras.models.load_model(\n",
    "            'C:\\\\Users\\\\User\\\\best_unet_model_onlineDA_128_lovaszloss_segnet.keras',\n",
    "            custom_objects=custom_objects\n",
    "        )\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Power monitoring\n",
    "        power_proc = subprocess.Popen(\n",
    "            ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-lms', '500'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        # # Train all epochs in one go\n",
    "        # model.fit(\n",
    "        #     X_train, y_train,\n",
    "        #     batch_size=batch_size,\n",
    "        #     epochs=epochs,\n",
    "        #     validation_data=(X_val, y_val),\n",
    "        #     verbose=1\n",
    "        # )\n",
    "        model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=epochs,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        total_time = end - start\n",
    "        avg_epoch_time = total_time / epochs\n",
    "        epoch_times_all.extend([avg_epoch_time] * epochs)\n",
    "\n",
    "        # Handle power logs\n",
    "        power_proc.terminate()\n",
    "        try:\n",
    "            power_output = power_proc.stdout.read().strip().split('\\n')\n",
    "            power_values = [float(line) for line in power_output if line.strip()]\n",
    "            avg_power = np.mean(power_values)\n",
    "            power_samples_all.extend([avg_power] * epochs)\n",
    "            print(f\"‚ö° Avg Power: {avg_power:.2f} W\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Power log failed.\")\n",
    "            power_samples_all.extend([np.nan] * epochs)\n",
    "\n",
    "        # Final cleanup\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "    return epoch_times_all, power_samples_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_times, power_vals = run_training(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Compute stats\n",
    "epoch_times = np.array(epoch_times)\n",
    "power_vals = np.array(power_vals)\n",
    "\n",
    "mean_time = np.mean(epoch_times)\n",
    "std_time = np.std(epoch_times)\n",
    "\n",
    "mean_power = np.nanmean(power_vals)\n",
    "energy_per_epoch_wh = (mean_power * mean_time) / 3600\n",
    "\n",
    "# Estimate GFLOPS per epoch (assuming 4 GFLOPs/sample)\n",
    "samples_per_epoch = len(X_train)\n",
    "estimated_flops_per_sample = 4e9  # 4 GFLOPs\n",
    "gflops = (2 * estimated_flops_per_sample * samples_per_epoch) / (mean_time * 1e9)\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"‚è±Ô∏è  Average epoch time: {mean_time:.2f} ¬± {std_time:.2f} sec\")\n",
    "print(f\"‚öôÔ∏è  Estimated GFLOPS: {gflops:.2f}\")\n",
    "print(f\"‚ö° Average power: {mean_power:.2f} W\")\n",
    "print(f\"üîã Energy per epoch: {energy_per_epoch_wh:.4f} Wh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, Concatenate, BatchNormalization, Activation\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "import gc\n",
    "\n",
    "# Constants\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "CHANNELS = 3\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "class ResizeLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom layer to resize images.\"\"\"\n",
    "    def __init__(self, target_size, **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, self.target_size, method='bilinear')\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(ResizeLayer, self).get_config()\n",
    "        config.update({\"target_size\": self.target_size})\n",
    "        return config\n",
    "\n",
    "def conv_block(x, filters, kernel_size=3, padding='same', activation='relu'):\n",
    "    \"\"\"Helper function for creating a conv block with BN and activation.\"\"\"\n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    # Add a second conv to increase parameters\n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def build_full_inceptionresnetv2_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Build a full UNet model with InceptionResNetV2 backbone with 60-70M parameters\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input shape of the image\n",
    "        num_classes: Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        Keras Model instance with UNet architecture\n",
    "    \"\"\"\n",
    "    # Input layer (no fixed batch size)\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Create a full InceptionResNetV2 model to use as backbone\n",
    "    base_model = InceptionResNetV2(\n",
    "        input_tensor=inputs,\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Make all layers trainable as requested\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Extract features from all encoder levels\n",
    "    # Standard blocks in InceptionResNetV2\n",
    "    encoder1 = base_model.get_layer('activation').output  # 111x111x64\n",
    "    encoder2 = base_model.get_layer('activation_3').output  # 55x55x192\n",
    "    encoder3 = base_model.get_layer('block35_10_ac').output  # 27x27x320\n",
    "    encoder4 = base_model.get_layer('block17_20_ac').output  # 13x13x1088\n",
    "    encoder5 = base_model.get_layer('conv_7b_ac').output  # 6x6x2080\n",
    "    \n",
    "    # Use the bottleneck as is - don't reduce its channels\n",
    "    bottleneck = encoder5  # 6x6x2080\n",
    "    \n",
    "    # First, reduce the bottleneck dimensions to control parameter count\n",
    "    bottleneck = Conv2D(512, 1, padding='same')(bottleneck)\n",
    "    bottleneck = BatchNormalization()(bottleneck)\n",
    "    bottleneck = Activation('relu')(bottleneck)\n",
    "    \n",
    "    # Level 5 to 4: 6x6 -> 13x13\n",
    "    up4 = UpSampling2D(size=(2, 2))(bottleneck)\n",
    "    up4 = ResizeLayer(target_size=(encoder4.shape[1], encoder4.shape[2]))(up4)\n",
    "    up4 = conv_block(up4, 512, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels before concatenation\n",
    "    skip4 = Conv2D(256, 1, padding='same')(encoder4)\n",
    "    skip4 = BatchNormalization()(skip4)\n",
    "    skip4 = Activation('relu')(skip4)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge4 = Concatenate()([up4, skip4])\n",
    "    merge4 = conv_block(merge4, 384)  # Reduced filters\n",
    "    \n",
    "    # Level 4 to 3: 13x13 -> 27x27\n",
    "    up3 = UpSampling2D(size=(2, 2))(merge4)\n",
    "    up3 = ResizeLayer(target_size=(encoder3.shape[1], encoder3.shape[2]))(up3)\n",
    "    up3 = conv_block(up3, 384, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels\n",
    "    skip3 = Conv2D(128, 1, padding='same')(encoder3)\n",
    "    skip3 = BatchNormalization()(skip3)\n",
    "    skip3 = Activation('relu')(skip3)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge3 = Concatenate()([up3, skip3])\n",
    "    merge3 = conv_block(merge3, 192)  # Reduced filters\n",
    "    \n",
    "    # Level 3 to 2: 27x27 -> 55x55\n",
    "    up2 = UpSampling2D(size=(2, 2))(merge3)\n",
    "    up2 = ResizeLayer(target_size=(encoder2.shape[1], encoder2.shape[2]))(up2)\n",
    "    up2 = conv_block(up2, 192, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels\n",
    "    skip2 = Conv2D(96, 1, padding='same')(encoder2)\n",
    "    skip2 = BatchNormalization()(skip2)\n",
    "    skip2 = Activation('relu')(skip2)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge2 = Concatenate()([up2, skip2])\n",
    "    merge2 = conv_block(merge2, 96)  # Reduced filters\n",
    "    \n",
    "    # Level 2 to 1: 55x55 -> 111x111\n",
    "    up1 = UpSampling2D(size=(2, 2))(merge2)\n",
    "    up1 = ResizeLayer(target_size=(encoder1.shape[1], encoder1.shape[2]))(up1)\n",
    "    up1 = conv_block(up1, 96, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels\n",
    "    skip1 = Conv2D(48, 1, padding='same')(encoder1)\n",
    "    skip1 = BatchNormalization()(skip1)\n",
    "    skip1 = Activation('relu')(skip1)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge1 = Concatenate()([up1, skip1])\n",
    "    merge1 = conv_block(merge1, 48)  # Reduced filters\n",
    "    \n",
    "    # Final upsampling to original resolution: 111x111 -> 224x224\n",
    "    up_final = UpSampling2D(size=(2, 2))(merge1)\n",
    "    up_final = conv_block(up_final, 32)  # Reduced filters\n",
    "    \n",
    "    # Ensure final size matches input\n",
    "    if up_final.shape[1] != input_shape[0] or up_final.shape[2] != input_shape[1]:\n",
    "        up_final = ResizeLayer(target_size=(input_shape[0], input_shape[1]))(up_final)\n",
    "    \n",
    "    # Add a final segmentation head\n",
    "    outputs = Conv2D(num_classes, 1, activation='softmax', dtype='float32')(up_final)\n",
    "    \n",
    "    # Create and return the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "print(\"Creating full InceptionResNetV2-UNet model...\")\n",
    "model = build_full_inceptionresnetv2_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), num_classes=NUM_CLASSES)\n",
    "print(\"Model created successfully!\")\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "class ResizeLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom layer to resize images.\"\"\"\n",
    "    def __init__(self, target_size, **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, self.target_size, method='bilinear')\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(ResizeLayer, self).get_config()\n",
    "        config.update({\"target_size\": self.target_size})\n",
    "        return config\n",
    "\n",
    "# Number of classes (adjust if needed)\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# ‚úÖ Dice Coefficient (Mean across all classes)\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Weighted Categorical Crossentropy\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    class_weights = tf.constant([0.3776, 0.7605, 65.8554, 46.2381], dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "    class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))\n",
    "    weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# ‚úÖ Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Lov√°sz-Softmax Loss\n",
    "def lovasz_softmax_loss(y_true, y_pred, ignore_background=False):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    num_classes = tf.shape(y_true)[-1]\n",
    "    start_class = tf.constant(1 if ignore_background else 0)\n",
    "\n",
    "    def compute_class_loss(c):\n",
    "        y_true_class = y_true[..., c]\n",
    "        y_pred_class = y_pred[..., c]\n",
    "\n",
    "        y_true_flat = tf.reshape(y_true_class, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred_class, [-1])\n",
    "\n",
    "        errors = tf.abs(y_true_flat - y_pred_flat)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], sorted=True)\n",
    "        y_true_sorted = tf.gather(y_true_flat, perm)\n",
    "\n",
    "        gts = tf.reduce_sum(y_true_sorted)\n",
    "        intersection = gts - tf.cumsum(y_true_sorted)\n",
    "        union = gts + tf.cumsum(1. - y_true_sorted)\n",
    "        jaccard = 1. - intersection / union\n",
    "        grad = tf.concat([[jaccard[0]], jaccard[1:] - jaccard[:-1]], 0)\n",
    "\n",
    "        return tf.tensordot(errors_sorted, grad, axes=1)\n",
    "\n",
    "    # Loop through classes using tf.while_loop\n",
    "    losses = tf.TensorArray(dtype=tf.float32, size=num_classes)\n",
    "\n",
    "    def loop_cond(c, losses):\n",
    "        return tf.less(c, num_classes)\n",
    "\n",
    "    def loop_body(c, losses):\n",
    "        loss_c = compute_class_loss(c)\n",
    "        losses = losses.write(c, loss_c)\n",
    "        return c + 1, losses\n",
    "\n",
    "    _, losses = tf.while_loop(loop_cond, loop_body, [start_class, losses])\n",
    "    return tf.reduce_mean(losses.stack())\n",
    "\n",
    "# ‚úÖ Combined Loss\n",
    "def combined_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice_loss_val = 1 - (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss_val = tf.reduce_mean(dice_loss_val)\n",
    "    \n",
    "    lovasz_loss_val = lovasz_softmax_loss(y_true, tf.nn.softmax(y_pred), ignore_background=False)\n",
    "    return lovasz_loss_val + dice_loss_val\n",
    "\n",
    "class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "    def __init__(self, class_idx=0, name=None, **kwargs):  # <- default class_idx=0 to avoid missing arg\n",
    "        if name is None:\n",
    "            name = f\"DiceClass{class_idx}\"\n",
    "        super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.class_idx = class_idx\n",
    "        self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_class = y_true[..., self.class_idx]\n",
    "        y_pred_class = y_pred[..., self.class_idx]\n",
    "        intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "        union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "        dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "        self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "    def result(self):\n",
    "        return self.dice\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"class_idx\": self.class_idx})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        if \"class_idx\" not in config:\n",
    "            # Try to extract class index from name like \"DiceClass2\"\n",
    "            name = config.get(\"name\", \"DiceClass0\")\n",
    "            if name.startswith(\"DiceClass\"):\n",
    "                config[\"class_idx\"] = int(name.replace(\"DiceClass\", \"\"))\n",
    "            else:\n",
    "                config[\"class_idx\"] = 0\n",
    "        return cls(**config)\n",
    "\n",
    "# ‚úÖ Helper to load Dice metrics by name\n",
    "def dice_metric_loader(name):\n",
    "    if name.startswith(\"DiceClass\"):\n",
    "        class_idx = int(name.replace(\"DiceClass\", \"\"))\n",
    "        return DiceCoefficient(class_idx=class_idx)\n",
    "    raise ValueError(f\"Unknown Dice metric name: {name}\")\n",
    "\n",
    "# ‚úÖ Register all custom objects for loading the model\n",
    "custom_objects = {\n",
    "    'ResizeLayer': ResizeLayer,\n",
    "    'combined_loss': combined_loss,\n",
    "    'lovasz_softmax_loss': lovasz_softmax_loss,\n",
    "    'MeanIoU': tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES),\n",
    "    'DiceCoefficient': DiceCoefficient,\n",
    "}\n",
    "\n",
    "# ‚úÖ Add DiceClass0‚Äì3 dynamically\n",
    "for i in range(NUM_CLASSES):\n",
    "    custom_objects[f'DiceClass{i}'] = dice_metric_loader(f'DiceClass{i}')\n",
    "\n",
    "# ‚úÖ Load the model\n",
    "model_inceptionresnetv2 = load_model('lovaszloss_unet++_inceptionresnetv2.keras', custom_objects=custom_objects)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAFE custom_objects (no functions returning arrays)\n",
    "custom_objects = {\n",
    "    'ResizeLayer': ResizeLayer,\n",
    "    'combined_loss': combined_loss,\n",
    "    'lovasz_softmax_loss': lovasz_softmax_loss,\n",
    "    'MeanIoU': tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES),\n",
    "    'DiceCoefficient': DiceCoefficient,  # needed to deserialize\n",
    "}\n",
    "\n",
    "# Add DiceClass0‚Äì3 safely as instances\n",
    "for i in range(NUM_CLASSES):\n",
    "    custom_objects[f'DiceClass{i}'] = DiceCoefficient(class_idx=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_gen, val_gen, model_path, custom_objects, batch_size=8, epochs=3, repeats=1):\n",
    "    import subprocess\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from tensorflow.keras import backend as K\n",
    "    import gc\n",
    "\n",
    "    epoch_times_all = []\n",
    "    power_samples_all = []\n",
    "\n",
    "    for r in range(repeats):\n",
    "        print(f\"\\nüîÅ Repeat {r+1}/{repeats}\")\n",
    "\n",
    "        # ‚úÖ Clean up previous session\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        # ‚úÖ Reload the model (InceptionResNetV2-UNet++)\n",
    "        model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)\n",
    "\n",
    "        # ‚úÖ Start timing and power monitoring\n",
    "        start = time.time()\n",
    "\n",
    "        power_proc = subprocess.Popen(\n",
    "            ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-lms', '500'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        # ‚úÖ Train using generator (saves memory)\n",
    "        model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=epochs,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # ‚úÖ Stop timing\n",
    "        end = time.time()\n",
    "        total_time = end - start\n",
    "        avg_epoch_time = total_time / epochs\n",
    "        epoch_times_all.extend([avg_epoch_time] * epochs)\n",
    "\n",
    "        # ‚úÖ Stop and process power readings\n",
    "        power_proc.terminate()\n",
    "        try:\n",
    "            power_output = power_proc.stdout.read().strip().split('\\n')\n",
    "            power_values = [float(line) for line in power_output if line.strip()]\n",
    "            avg_power = np.mean(power_values)\n",
    "            power_samples_all.extend([avg_power] * epochs)\n",
    "            print(f\"‚ö° Avg Power: {avg_power:.2f} W\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Power log failed: {e}\")\n",
    "            power_samples_all.extend([np.nan] * epochs)\n",
    "\n",
    "        # ‚úÖ Cleanup\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "    return epoch_times_all, power_samples_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'lovaszloss_unet++_inceptionresnetv2.keras'\n",
    "\n",
    "epoch_times, power_vals = run_training(\n",
    "    train_gen=train_gen,\n",
    "    val_gen=val_gen,\n",
    "    model_path=model_path,\n",
    "    custom_objects=custom_objects,\n",
    "    batch_size=4,\n",
    "    epochs=3,\n",
    "    repeats=1\n",
    ")\n",
    "\n",
    "# Compute stats\n",
    "epoch_times = np.array(epoch_times)\n",
    "power_vals = np.array(power_vals)\n",
    "\n",
    "mean_time = np.mean(epoch_times)\n",
    "std_time = np.std(epoch_times)\n",
    "\n",
    "mean_power = np.nanmean(power_vals)\n",
    "energy_per_epoch_wh = (mean_power * mean_time) / 3600\n",
    "\n",
    "# Estimate GFLOPS per epoch (assuming 4 GFLOPs/sample)\n",
    "samples_per_epoch = len(X_train)\n",
    "estimated_flops_per_sample = 4e9  # 4 GFLOPs\n",
    "gflops = (2 * estimated_flops_per_sample * samples_per_epoch) / (mean_time * 1e9)\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"‚è±Ô∏è  Average epoch time: {mean_time:.2f} ¬± {std_time:.2f} sec\")\n",
    "print(f\"‚öôÔ∏è  Estimated GFLOPS: {gflops:.2f}\")\n",
    "print(f\"‚ö° Average power: {mean_power:.2f} W\")\n",
    "print(f\"üîã Energy per epoch: {energy_per_epoch_wh:.4f} Wh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Activation, Add\n",
    "from tensorflow.keras.layers import Dense, Dropout, Layer, Reshape, Permute, Multiply, Concatenate\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, LayerNormalization, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.applications import EfficientNetB4\n",
    "\n",
    "class ResizeToMatchLayer(Layer):\n",
    "    \"\"\"Layer to resize input to match target tensor's spatial dimensions.\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ResizeToMatchLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x, target = inputs\n",
    "        # Get spatial dimensions of target tensor\n",
    "        target_shape = tf.shape(target)\n",
    "        target_height, target_width = target_shape[1], target_shape[2]\n",
    "        \n",
    "        # Resize x to match target's spatial dimensions\n",
    "        return tf.image.resize(x, [target_height, target_width], method='bilinear')\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[1][1], input_shape[1][2], input_shape[0][3])\n",
    "\n",
    "def conv_block(x, filters, kernel_size=3, strides=1, padding='same', use_bn=True, activation='relu'):\n",
    "    \"\"\"Standard convolution block with BatchNorm and activation.\"\"\"\n",
    "    x = Conv2D(filters, kernel_size, strides=strides, padding=padding)(x)\n",
    "    \n",
    "    if use_bn:\n",
    "        x = BatchNormalization()(x)\n",
    "    \n",
    "    if activation:\n",
    "        x = Activation(activation)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def attention_gate(x, g, inter_channels):\n",
    "    \"\"\"\n",
    "    Attention Gate as described in Attention U-Net paper.\n",
    "    Args:\n",
    "        x: Feature map from skip connection (from encoder)\n",
    "        g: Gating signal from previous decoder layer\n",
    "        inter_channels: Number of channels in intermediate representations\n",
    "    \"\"\"\n",
    "    # Resize gating signal to match feature map's spatial dimensions if needed\n",
    "    g = ResizeToMatchLayer()([g, x])\n",
    "    \n",
    "    # Intermediate representation for input feature map\n",
    "    theta_x = Conv2D(inter_channels, 1, use_bias=False, padding='same')(x)\n",
    "    \n",
    "    # Intermediate representation for gating signal\n",
    "    phi_g = Conv2D(inter_channels, 1, use_bias=False, padding='same')(g)\n",
    "    \n",
    "    # Element-wise sum and ReLU\n",
    "    f = Activation('relu')(Add()([theta_x, phi_g]))\n",
    "    \n",
    "    # 1x1 convolution followed by sigmoid to get attention coefficients\n",
    "    psi_f = Conv2D(1, 1, use_bias=False, padding='same')(f)\n",
    "    att_map = Activation('sigmoid')(psi_f)\n",
    "    \n",
    "    # Apply attention\n",
    "    return Multiply()([x, att_map])\n",
    "\n",
    "def decoder_block(x, skip_connection, filters, use_attention=True):\n",
    "    \"\"\"Decoder block for Attention U-Net.\"\"\"\n",
    "    # Upsampling\n",
    "    x = UpSampling2D(size=(2, 2), interpolation='bilinear')(x)\n",
    "    \n",
    "    # Ensure dimensions match for concatenation\n",
    "    x = ResizeToMatchLayer()([x, skip_connection])\n",
    "    \n",
    "    # Apply attention mechanism if specified\n",
    "    if use_attention:\n",
    "        # Generate attention-gated skip connection\n",
    "        skip_connection = attention_gate(skip_connection, x, filters // 2)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    x = Concatenate()([x, skip_connection])\n",
    "    \n",
    "    # Apply two convolution blocks\n",
    "    x = conv_block(x, filters, 3, padding='same')\n",
    "    x = conv_block(x, filters, 3, padding='same')\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_efficientnet_attention_unet(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Build an Attention U-Net model with EfficientNetB4 backbone for semantic segmentation.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input shape of the image (height, width, channels)\n",
    "        num_classes: Number of segmentation classes\n",
    "        \n",
    "    Returns:\n",
    "        A Keras Model instance\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "        \n",
    "    # Load EfficientNetB4 with pre-trained weights as encoder backbone\n",
    "    # All layers are trainable for fine-tuning\n",
    "    base_model = EfficientNetB4(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_tensor=inputs\n",
    "    )\n",
    "    \n",
    "    # Reduce filter count to control parameter count since we're not freezing any layers\n",
    "    initial_filters = 32\n",
    "    \n",
    "    # Get skip connections from appropriate layers\n",
    "    skip1 = base_model.get_layer('block1b_add').output        # 1/2 scale (112x112)\n",
    "    skip2 = base_model.get_layer('block2d_add').output        # 1/4 scale (56x56)\n",
    "    skip3 = base_model.get_layer('block3d_add').output        # 1/8 scale (28x28)\n",
    "    skip4 = base_model.get_layer('block5e_add').output        # 1/16 scale (14x14)\n",
    "    \n",
    "    # Bridge (bottleneck)\n",
    "    bridge = base_model.get_layer('top_activation').output    # 1/32 scale (7x7)\n",
    "    \n",
    "    \n",
    "    # Reduce channels for each skip connection to control parameter count\n",
    "    skip1_conv = conv_block(skip1, initial_filters)\n",
    "    skip2_conv = conv_block(skip2, initial_filters * 2)\n",
    "    skip3_conv = conv_block(skip3, initial_filters * 4)\n",
    "    skip4_conv = conv_block(skip4, initial_filters * 8)\n",
    "    \n",
    "    # Reduce channels in bridge\n",
    "    bridge_conv = conv_block(bridge, initial_filters * 16)\n",
    "    \n",
    "    # Decoder pathway with attention gates\n",
    "    d1 = decoder_block(bridge_conv, skip4_conv, initial_filters * 8, use_attention=True)  # 1/16\n",
    "    d2 = decoder_block(d1, skip3_conv, initial_filters * 4, use_attention=True)           # 1/8\n",
    "    d3 = decoder_block(d2, skip2_conv, initial_filters * 2, use_attention=True)           # 1/4\n",
    "    d4 = decoder_block(d3, skip1_conv, initial_filters, use_attention=True)               # 1/2\n",
    "    \n",
    "    # Final upsampling to original image size\n",
    "    final = UpSampling2D(size=(2, 2), interpolation='bilinear')(d4)\n",
    "    \n",
    "    # Final convolution to generate segmentation map\n",
    "    outputs = Conv2D(num_classes, 1, padding='same', activation='softmax')(final)\n",
    "    \n",
    "    # Create and return the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_efficientnet_attention_unet(input_shape=(224, 224, 3), num_classes=4)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Weighted Categorical Crossentropy\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    class_weights = tf.constant([0.3776, 0.7605, 65.8554, 46.2381], dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "    class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))\n",
    "    weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# ‚úÖ Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Lov√°sz-Softmax Loss\n",
    "def lovasz_softmax_loss(y_true, y_pred, ignore_background=False):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    num_classes = tf.shape(y_true)[-1]\n",
    "    start_class = tf.constant(1 if ignore_background else 0)\n",
    "\n",
    "    def compute_class_loss(c):\n",
    "        y_true_class = y_true[..., c]\n",
    "        y_pred_class = y_pred[..., c]\n",
    "\n",
    "        y_true_flat = tf.reshape(y_true_class, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred_class, [-1])\n",
    "\n",
    "        errors = tf.abs(y_true_flat - y_pred_flat)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], sorted=True)\n",
    "        y_true_sorted = tf.gather(y_true_flat, perm)\n",
    "\n",
    "        gts = tf.reduce_sum(y_true_sorted)\n",
    "        intersection = gts - tf.cumsum(y_true_sorted)\n",
    "        union = gts + tf.cumsum(1. - y_true_sorted)\n",
    "        jaccard = 1. - intersection / union\n",
    "        grad = tf.concat([[jaccard[0]], jaccard[1:] - jaccard[:-1]], 0)\n",
    "\n",
    "        return tf.tensordot(errors_sorted, grad, axes=1)\n",
    "\n",
    "    # Loop through classes using tf.while_loop\n",
    "    losses = tf.TensorArray(dtype=tf.float32, size=num_classes)\n",
    "\n",
    "    def loop_cond(c, losses):\n",
    "        return tf.less(c, num_classes)\n",
    "\n",
    "    def loop_body(c, losses):\n",
    "        loss_c = compute_class_loss(c)\n",
    "        losses = losses.write(c, loss_c)\n",
    "        return c + 1, losses\n",
    "\n",
    "    _, losses = tf.while_loop(loop_cond, loop_body, [start_class, losses])\n",
    "    return tf.reduce_mean(losses.stack())\n",
    "\n",
    "# ‚úÖ Combined Loss\n",
    "def combined_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice_loss_val = 1 - (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss_val = tf.reduce_mean(dice_loss_val)\n",
    "    \n",
    "    lovasz_loss_val = lovasz_softmax_loss(y_true, tf.nn.softmax(y_pred), ignore_background=False)\n",
    "    return lovasz_loss_val + dice_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Weighted Categorical Crossentropy\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    class_weights = tf.constant([0.3776, 0.7605, 65.8554, 46.2381], dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "    class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))\n",
    "    weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# ‚úÖ Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Custom Dice Coefficient Metric for Each Class\n",
    "class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "    def __init__(self, class_idx, name=None, **kwargs):  \n",
    "        if name is None:\n",
    "            name = f\"DiceClass{class_idx}\"  \n",
    "        super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.class_idx = class_idx\n",
    "        self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_class = y_true[..., self.class_idx]\n",
    "        y_pred_class = y_pred[..., self.class_idx]\n",
    "        intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "        union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "        dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "        self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "    def result(self):\n",
    "        return self.dice\n",
    "\n",
    "# ‚úÖ Function to Get Class-wise Metrics\n",
    "def class_wise_metrics(num_classes=4):\n",
    "    return [DiceCoefficient(i) for i in range(num_classes)] + [tf.keras.metrics.MeanIoU(num_classes=num_classes)]\n",
    "\n",
    "model_efficientnetb4 = build_efficientnet_attention_unet(input_shape=(224, 224, 3), num_classes=4)\n",
    "model_efficientnetb4.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(4)  # Number of classes\n",
    ")\n",
    "model_efficientnetb4.load_weights(\"efficientnet_attention_unet_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects_efficientnet = {\n",
    "    'combined_loss': combined_loss,\n",
    "    'dice_loss': dice_loss,\n",
    "    'weighted_categorical_crossentropy': weighted_categorical_crossentropy,\n",
    "    'DiceCoefficient': DiceCoefficient,\n",
    "    'MeanIoU': tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES)\n",
    "}\n",
    "\n",
    "# Dynamically add DiceClass0‚Äì3\n",
    "for i in range(NUM_CLASSES):\n",
    "    custom_objects_efficientnet[f'DiceClass{i}'] = DiceCoefficient(class_idx=i)\n",
    "\n",
    "def load_efficientnetb4_model():\n",
    "    model = build_efficientnet_attention_unet(input_shape=(224, 224, 3), num_classes=4)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "        loss=combined_loss,\n",
    "        metrics=class_wise_metrics(4)\n",
    "    )\n",
    "    model.load_weights(\"efficientnet_attention_unet_weights.h5\")\n",
    "    return model\n",
    "\n",
    "def run_training(train_gen, val_gen, model_loader_fn, custom_objects, batch_size=8, epochs=3, repeats=1):\n",
    "    import subprocess\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from tensorflow.keras import backend as K\n",
    "    import gc\n",
    "\n",
    "    epoch_times_all = []\n",
    "    power_samples_all = []\n",
    "\n",
    "    for r in range(repeats):\n",
    "        print(f\"\\nüîÅ Repeat {r+1}/{repeats}\")\n",
    "\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        # Build and load model\n",
    "        model = model_loader_fn()\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        power_proc = subprocess.Popen(\n",
    "            ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-lms', '500'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=epochs,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        total_time = end - start\n",
    "        avg_epoch_time = total_time / epochs\n",
    "        epoch_times_all.extend([avg_epoch_time] * epochs)\n",
    "\n",
    "        power_proc.terminate()\n",
    "        try:\n",
    "            power_output = power_proc.stdout.read().strip().split('\\n')\n",
    "            power_values = [float(line) for line in power_output if line.strip()]\n",
    "            avg_power = np.mean(power_values)\n",
    "            power_samples_all.extend([avg_power] * epochs)\n",
    "            print(f\"‚ö° Avg Power: {avg_power:.2f} W\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Power log failed: {e}\")\n",
    "            power_samples_all.extend([np.nan] * epochs)\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "    return epoch_times_all, power_samples_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "epoch_times, power_vals = run_training(\n",
    "    train_gen=train_gen,\n",
    "    val_gen=val_gen,\n",
    "    model_loader_fn=load_efficientnetb4_model,  # note: function, not string path\n",
    "    custom_objects=custom_objects_efficientnet,\n",
    "    batch_size=4,\n",
    "    epochs=3,\n",
    "    repeats=1\n",
    ")\n",
    "\n",
    "epoch_times = np.array(epoch_times)\n",
    "power_vals = np.array(power_vals)\n",
    "\n",
    "mean_time = np.mean(epoch_times)\n",
    "std_time = np.std(epoch_times)\n",
    "mean_power = np.nanmean(power_vals)\n",
    "energy_wh = (mean_power * mean_time) / 3600\n",
    "\n",
    "samples_per_epoch = len(train_gen) * train_gen.batch_size\n",
    "flops_per_sample = 4e9  # adjust if you have exact FLOPs\n",
    "gflops = (2 * flops_per_sample * samples_per_epoch) / (mean_time * 1e9)\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"‚è±Ô∏è  Avg epoch time: {mean_time:.2f} ¬± {std_time:.2f} sec\")\n",
    "print(f\"‚öôÔ∏è  Estimated GFLOPS: {gflops:.2f}\")\n",
    "print(f\"‚ö°  Avg power: {mean_power:.2f} W\")\n",
    "print(f\"üîã  Avg energy/epoch: {energy_wh:.4f} Wh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "# Constants for 224x224 images\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "CHANNELS = 3\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "def convolution_block(inputs, filters, kernel_size=3, dilation_rate=1, padding='same', use_bias=False):\n",
    "    \"\"\"\n",
    "    Standard convolution block with batch normalization and ReLU activation\n",
    "    \"\"\"\n",
    "    x = layers.Conv2D(\n",
    "        filters, \n",
    "        kernel_size, \n",
    "        padding=padding,\n",
    "        dilation_rate=dilation_rate,\n",
    "        use_bias=use_bias\n",
    "    )(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def ASPP(inputs):\n",
    "    \"\"\"\n",
    "    Atrous Spatial Pyramid Pooling module for DeepLabV3+\n",
    "    \"\"\"\n",
    "    # ASPP with different dilation rates\n",
    "    b0 = convolution_block(inputs, 256, kernel_size=1, dilation_rate=1)\n",
    "    b1 = convolution_block(inputs, 256, kernel_size=3, dilation_rate=6)\n",
    "    b2 = convolution_block(inputs, 256, kernel_size=3, dilation_rate=12)\n",
    "    b3 = convolution_block(inputs, 256, kernel_size=3, dilation_rate=18)\n",
    "    \n",
    "    # Global context - simplified approach\n",
    "    b4 = layers.GlobalAveragePooling2D()(inputs)\n",
    "    b4 = layers.Reshape((1, 1, inputs.shape[-1]))(b4)\n",
    "    b4 = convolution_block(b4, 256, kernel_size=1)\n",
    "    # Use fixed upsampling instead of dynamic\n",
    "    b4 = layers.UpSampling2D(size=(inputs.shape[1], inputs.shape[2]))(b4)\n",
    "    \n",
    "    # Concatenate all branches\n",
    "    x = layers.Concatenate()([b0, b1, b2, b3, b4])\n",
    "    \n",
    "    # Final 1x1 convolution\n",
    "    output = convolution_block(x, 256, kernel_size=1)\n",
    "    return output\n",
    "\n",
    "def build_deeplabv3_plus_xception(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    DeepLabV3+ model with Xception backbone\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Xception as backbone (with output stride of 16)\n",
    "    base_model = Xception(\n",
    "        input_tensor=inputs,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Don't freeze any layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Extract features from Xception\n",
    "    # The entry flow ends with 'block4_sepconv2_bn' which is a good low-level feature point\n",
    "    low_level_features = base_model.get_layer('block4_sepconv2_bn').output\n",
    "    # The final features from the exit flow\n",
    "    high_level_features = base_model.output\n",
    "    \n",
    "    # Process low-level features\n",
    "    low_level_features = convolution_block(low_level_features, 48, kernel_size=1)\n",
    "    \n",
    "    # Process high-level features with ASPP\n",
    "    x = ASPP(high_level_features)\n",
    "    \n",
    "    # Calculate upsampling factor for high-level features to match low-level features\n",
    "    hl_shape = high_level_features.shape\n",
    "    ll_shape = low_level_features.shape\n",
    "    h_factor = ll_shape[1] // hl_shape[1]\n",
    "    w_factor = ll_shape[2] // hl_shape[2]\n",
    "    \n",
    "    # Upsample high-level features to match low-level features\n",
    "    x = layers.UpSampling2D(size=(h_factor, w_factor), interpolation='bilinear')(x)\n",
    "    \n",
    "    # Concatenate features\n",
    "    x = layers.Concatenate()([x, low_level_features])\n",
    "    \n",
    "    # Apply convolution blocks\n",
    "    x = convolution_block(x, 256, kernel_size=3)\n",
    "    x = convolution_block(x, 256, kernel_size=3)\n",
    "    \n",
    "    # Calculate upsampling factor needed to reach 224x224\n",
    "    current_shape = x.shape\n",
    "    h_factor = IMG_HEIGHT // current_shape[1]\n",
    "    w_factor = IMG_WIDTH // current_shape[2]\n",
    "    \n",
    "    # Final upsampling to original size (224x224)\n",
    "    x = layers.UpSampling2D(size=(h_factor, w_factor), interpolation='bilinear')(x)\n",
    "    \n",
    "    # Ensure exact dimensions with a reshape if needed\n",
    "    x = layers.Reshape((IMG_HEIGHT, IMG_WIDTH, int(current_shape[3])))(x)\n",
    "    \n",
    "    # Final convolution for output (224, 224, 4)\n",
    "    outputs = layers.Conv2D(num_classes, kernel_size=1, padding='same', activation='softmax')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_deeplabv3_plus_xception(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), \n",
    "                                     num_classes=NUM_CLASSES)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Number of classes (adjust if needed)\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# ‚úÖ Dice Coefficient (Mean across all classes)\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Weighted Categorical Crossentropy\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    class_weights = tf.constant([0.3776, 0.7605, 65.8554, 46.2381], dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "    class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))\n",
    "    weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# ‚úÖ Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Lov√°sz-Softmax Loss\n",
    "def lovasz_softmax_loss(y_true, y_pred, ignore_background=False):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    num_classes = tf.shape(y_true)[-1]\n",
    "    start_class = tf.constant(1 if ignore_background else 0)\n",
    "\n",
    "    def compute_class_loss(c):\n",
    "        y_true_class = y_true[..., c]\n",
    "        y_pred_class = y_pred[..., c]\n",
    "\n",
    "        y_true_flat = tf.reshape(y_true_class, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred_class, [-1])\n",
    "\n",
    "        errors = tf.abs(y_true_flat - y_pred_flat)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], sorted=True)\n",
    "        y_true_sorted = tf.gather(y_true_flat, perm)\n",
    "\n",
    "        gts = tf.reduce_sum(y_true_sorted)\n",
    "        intersection = gts - tf.cumsum(y_true_sorted)\n",
    "        union = gts + tf.cumsum(1. - y_true_sorted)\n",
    "        jaccard = 1. - intersection / union\n",
    "        grad = tf.concat([[jaccard[0]], jaccard[1:] - jaccard[:-1]], 0)\n",
    "\n",
    "        return tf.tensordot(errors_sorted, grad, axes=1)\n",
    "\n",
    "    # Loop through classes using tf.while_loop\n",
    "    losses = tf.TensorArray(dtype=tf.float32, size=num_classes)\n",
    "\n",
    "    def loop_cond(c, losses):\n",
    "        return tf.less(c, num_classes)\n",
    "\n",
    "    def loop_body(c, losses):\n",
    "        loss_c = compute_class_loss(c)\n",
    "        losses = losses.write(c, loss_c)\n",
    "        return c + 1, losses\n",
    "\n",
    "    _, losses = tf.while_loop(loop_cond, loop_body, [start_class, losses])\n",
    "    return tf.reduce_mean(losses.stack())\n",
    "\n",
    "# ‚úÖ Combined Loss\n",
    "def combined_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice_loss_val = 1 - (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss_val = tf.reduce_mean(dice_loss_val)\n",
    "    \n",
    "    lovasz_loss_val = lovasz_softmax_loss(y_true, tf.nn.softmax(y_pred), ignore_background=False)\n",
    "    return lovasz_loss_val + dice_loss_val\n",
    "\n",
    "class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "    def __init__(self, class_idx=0, name=None, **kwargs):  # <- default class_idx=0 to avoid missing arg\n",
    "        if name is None:\n",
    "            name = f\"DiceClass{class_idx}\"\n",
    "        super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.class_idx = class_idx\n",
    "        self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_class = y_true[..., self.class_idx]\n",
    "        y_pred_class = y_pred[..., self.class_idx]\n",
    "        intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "        union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "        dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "        self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "    def result(self):\n",
    "        return self.dice\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"class_idx\": self.class_idx})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        if \"class_idx\" not in config:\n",
    "            # Try to extract class index from name like \"DiceClass2\"\n",
    "            name = config.get(\"name\", \"DiceClass0\")\n",
    "            if name.startswith(\"DiceClass\"):\n",
    "                config[\"class_idx\"] = int(name.replace(\"DiceClass\", \"\"))\n",
    "            else:\n",
    "                config[\"class_idx\"] = 0\n",
    "        return cls(**config)\n",
    "\n",
    "# ‚úÖ Helper to load Dice metrics by name\n",
    "def dice_metric_loader(name):\n",
    "    if name.startswith(\"DiceClass\"):\n",
    "        class_idx = int(name.replace(\"DiceClass\", \"\"))\n",
    "        return DiceCoefficient(class_idx=class_idx)\n",
    "    raise ValueError(f\"Unknown Dice metric name: {name}\")\n",
    "\n",
    "\n",
    "\n",
    "custom_objects = {\n",
    "    'combined_loss': combined_loss,\n",
    "    'lovasz_softmax_loss': lovasz_softmax_loss,\n",
    "    'MeanIoU': tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES),\n",
    "    'DiceCoefficient': DiceCoefficient,\n",
    "}\n",
    "\n",
    "# ‚úÖ Add DiceClass0‚Äì3 dynamically\n",
    "for i in range(NUM_CLASSES):\n",
    "    custom_objects[f'DiceClass{i}'] = dice_metric_loader(f'DiceClass{i}')\n",
    "\n",
    "# ‚úÖ Load the model\n",
    "model_xception = load_model('lovaszloss_deeplabv3_xception.keras', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects_xception = {\n",
    "    'combined_loss': combined_loss,\n",
    "    'lovasz_softmax_loss': lovasz_softmax_loss,\n",
    "    'MeanIoU': tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES),\n",
    "    'DiceCoefficient': DiceCoefficient,\n",
    "}\n",
    "\n",
    "# Add DiceClass0‚Äì3 directly\n",
    "for i in range(NUM_CLASSES):\n",
    "    custom_objects_xception[f'DiceClass{i}'] = DiceCoefficient(class_idx=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_gen, val_gen, model_loader_fn, custom_objects, batch_size=8, epochs=3, repeats=1):\n",
    "    import subprocess\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from tensorflow.keras import backend as K\n",
    "    import gc\n",
    "\n",
    "    epoch_times_all = []\n",
    "    power_samples_all = []\n",
    "\n",
    "    for r in range(repeats):\n",
    "        print(f\"\\nüîÅ Repeat {r+1}/{repeats}\")\n",
    "\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        # Build and load model\n",
    "        model = model_loader_fn()\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        power_proc = subprocess.Popen(\n",
    "            ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits', '-lms', '500'],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=epochs,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        end = time.time()\n",
    "        total_time = end - start\n",
    "        avg_epoch_time = total_time / epochs\n",
    "        epoch_times_all.extend([avg_epoch_time] * epochs)\n",
    "\n",
    "        power_proc.terminate()\n",
    "        try:\n",
    "            power_output = power_proc.stdout.read().strip().split('\\n')\n",
    "            power_values = [float(line) for line in power_output if line.strip()]\n",
    "            avg_power = np.mean(power_values)\n",
    "            power_samples_all.extend([avg_power] * epochs)\n",
    "            print(f\"‚ö° Avg Power: {avg_power:.2f} W\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Power log failed: {e}\")\n",
    "            power_samples_all.extend([np.nan] * epochs)\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "\n",
    "    return epoch_times_all, power_samples_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_deeplabv3_xception_model():\n",
    "    return tf.keras.models.load_model(\n",
    "        'lovaszloss_deeplabv3_xception.keras',\n",
    "        custom_objects=custom_objects_xception\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "epoch_times, power_vals = run_training(\n",
    "    train_gen=train_gen,\n",
    "    val_gen=val_gen,\n",
    "    model_loader_fn=load_deeplabv3_xception_model,\n",
    "    custom_objects=custom_objects_xception,\n",
    "    batch_size=8,\n",
    "    epochs=3,\n",
    "    repeats=1\n",
    ")\n",
    "\n",
    "epoch_times = np.array(epoch_times)\n",
    "power_vals = np.array(power_vals)\n",
    "\n",
    "mean_time = np.mean(epoch_times)\n",
    "std_time = np.std(epoch_times)\n",
    "mean_power = np.nanmean(power_vals)\n",
    "energy_wh = (mean_power * mean_time) / 3600\n",
    "\n",
    "samples_per_epoch = len(train_gen) * train_gen.batch_size\n",
    "flops_per_sample = 4e9  # Rough estimate for Xception-based DeepLab\n",
    "gflops = (2 * flops_per_sample * samples_per_epoch) / (mean_time * 1e9)\n",
    "\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(f\"‚è±Ô∏è  Avg epoch time: {mean_time:.2f} ¬± {std_time:.2f} sec\")\n",
    "print(f\"‚öôÔ∏è  Estimated GFLOPS: {gflops:.2f}\")\n",
    "print(f\"‚ö°  Avg power: {mean_power:.2f} W\")\n",
    "print(f\"üîã  Avg energy/epoch: {energy_wh:.4f} Wh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class SoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, apply_softmax=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            models (list): List of pretrained tf.keras.Model instances.\n",
    "            apply_softmax (bool): Apply softmax to logits before averaging.\n",
    "        \"\"\"\n",
    "        super(SoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        prob_sum = 0\n",
    "\n",
    "        for model in self.models:\n",
    "            logits = model(x, training=training)\n",
    "            probs = tf.nn.softmax(logits, axis=-1) if self.apply_softmax else logits\n",
    "            prob_sum += probs\n",
    "\n",
    "        avg_prob = prob_sum / len(self.models)\n",
    "        final_pred = tf.argmax(avg_prob, axis=-1)  # shape: [B, H, W]\n",
    "        return final_pred\n",
    "\n",
    "# Assume you have trained models: model1, model2, model3\n",
    "ensemble_model = SoftVotingEnsemble([model_segnet, model_inceptionresnetv2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SoftVotingEnsemble(tf.keras.Model):\n",
    "#     def __init__(self, models, apply_softmax=True, return_probs=False):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             models (list): List of pretrained tf.keras.Model instances.\n",
    "#             apply_softmax (bool): Apply softmax to logits before averaging.\n",
    "#             return_probs (bool): If True, return averaged class probabilities (for loss/metrics).\n",
    "#                                  If False, return argmax predictions (for direct inference).\n",
    "#         \"\"\"\n",
    "#         super(SoftVotingEnsemble, self).__init__()\n",
    "#         self.models = models\n",
    "#         self.apply_softmax = apply_softmax\n",
    "#         self.return_probs = return_probs\n",
    "\n",
    "#     def call(self, x, training=False):\n",
    "#         prob_sum = 0\n",
    "#         for model in self.models:\n",
    "#             logits = model(x, training=training)\n",
    "#             probs = tf.nn.softmax(logits, axis=-1) if self.apply_softmax else logits\n",
    "#             prob_sum += probs\n",
    "#         avg_prob = prob_sum / len(self.models)\n",
    "        \n",
    "#         if self.return_probs:\n",
    "#             return avg_prob  # shape: [B, H, W, C]\n",
    "#         else:\n",
    "#             return tf.argmax(avg_prob, axis=-1)  # shape: [B, H, W]\n",
    "\n",
    "class SoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, apply_softmax=True, return_probs=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            models (list): List of pretrained tf.keras.Model instances.\n",
    "            apply_softmax (bool): Apply softmax to logits before averaging.\n",
    "            return_probs (bool): If True, return averaged class probabilities (for loss/metrics).\n",
    "                                 If False, return argmax predictions (for direct inference).\n",
    "        \"\"\"\n",
    "        super(SoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "        self.return_probs = return_probs\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        prob_sum = 0\n",
    "        for model in self.models:\n",
    "            output = model(x, training=training)\n",
    "\n",
    "            # Detect EfficientNet-like model (already softmaxed)\n",
    "            is_softmaxed = (\n",
    "                hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "            )\n",
    "\n",
    "            if self.apply_softmax and not is_softmaxed:\n",
    "                probs = tf.nn.softmax(output, axis=-1)\n",
    "            else:\n",
    "                probs = output\n",
    "\n",
    "            prob_sum += probs\n",
    "\n",
    "        avg_prob = prob_sum / len(self.models)\n",
    "\n",
    "        if self.return_probs:\n",
    "            return avg_prob\n",
    "        else:\n",
    "            return tf.argmax(avg_prob, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ‚úÖ RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ Function to convert RGB masks to class index masks\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    # Create a mask initialized with zeros (for background class)\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "\n",
    "    # Loop through the RGB_TO_CLASS dictionary\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        # Identify the pixels with the current RGB value and assign them the class index\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Function to calculate Dice Similarity Coefficient (DSC)\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ‚úÖ Function to calculate IoU (Intersection over Union)\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# ‚úÖ Function to calculate Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')  # Return inf if no points for either true or pred class\n",
    "\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# ‚úÖ Function to calculate Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')  # Return inf if no points for either true or pred class\n",
    "\n",
    "    distances = []\n",
    "    for true_point in true_points:\n",
    "        distances.append(np.min(np.linalg.norm(pred_points - true_point, axis=1)))\n",
    "    return np.mean(distances)\n",
    "\n",
    "# ‚úÖ Function to evaluate the model on the test set class-wise\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16):\n",
    "    # Predict in batches\n",
    "    y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)  # Convert to class index prediction\n",
    "\n",
    "    # Convert y_test to class index format (since it's one-hot encoded)\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    # Initialize lists to store class-wise metrics\n",
    "    class_metrics = {i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []} for i in range(num_classes)}\n",
    "\n",
    "    # Calculate metrics for each test sample\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]  # one-hot -> class index\n",
    "        pred_mask = y_pred[i]\n",
    "\n",
    "        # For each class (0: Background, 1: Brain, 2: CSP, 3: LV)\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            # Dice Coefficient\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            # IoU\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            # Precision\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # Recall\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # F1 Score\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # Accuracy\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # # Hausdorff Distance\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # # Average Surface Distance\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # Print class-wise metrics in percentage\n",
    "    print(f\"{'Class':<10}{'Dice Coefficient (%)':<20}{'IoU (%)':<20}{'Precision (%)':<20}{'Recall (%)':<20}{'F1 Score (%)':<20}{'Accuracy (%)':<20}{'Hausdorff Distance':<20}{'Avg Surface Distance':<20}\")\n",
    "    print('-' * 180)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"Class {class_idx}:\")\n",
    "        print(f\"  Dice Coefficient: {np.mean(class_metrics[class_idx]['dice']) * 100:.2f}%\")\n",
    "        print(f\"  IoU: {np.mean(class_metrics[class_idx]['iou']) * 100:.2f}%\")\n",
    "        print(f\"  Precision: {np.mean(class_metrics[class_idx]['precision']) * 100:.2f}%\")\n",
    "        print(f\"  Recall: {np.mean(class_metrics[class_idx]['recall']) * 100:.2f}%\")\n",
    "        print(f\"  F1 Score: {np.mean(class_metrics[class_idx]['f1']) * 100:.2f}%\")\n",
    "        print(f\"  Accuracy: {np.mean(class_metrics[class_idx]['accuracy']) * 100:.2f}%\")\n",
    "        # print(f\"  Hausdorff Distance: {np.mean(class_metrics[class_idx]['hausdorff']):.4f}\")\n",
    "        # print(f\"  Average Surface Distance: {np.mean(class_metrics[class_idx]['asd']):.4f}\")\n",
    "        print(\"-\" * 180)\n",
    "\n",
    "    # Evaluate on test set to print overall test accuracy and loss\n",
    "    # test_loss, *test_metrics = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    # print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    for metric, value in zip(model.metrics_names[1:], test_metrics):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# ‚úÖ Call the evaluation function on the test set class-wise\n",
    "evaluate_classwise_metrics(model_segnet, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_wise_metrics(num_classes=4):\n",
    "    return [DiceCoefficient(i) for i in range(num_classes)] + [tf.keras.metrics.MeanIoU(num_classes=num_classes)]\n",
    "\n",
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_inceptionresnetv2],\n",
    "    apply_softmax=False,\n",
    "    return_probs=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "# ‚úÖ RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ RGB mask to class mask\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Dice\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ‚úÖ IoU\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# ‚úÖ Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# ‚úÖ Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    distances = [np.min(np.linalg.norm(pred_points - pt, axis=1)) for pt in true_points]\n",
    "    return np.mean(distances)\n",
    "\n",
    "# # ‚úÖ Soft Voting Ensemble Model\n",
    "# class SoftVotingEnsemble(tf.keras.Model):\n",
    "#     def __init__(self, models, apply_softmax=True):\n",
    "#         super(SoftVotingEnsemble, self).__init__()\n",
    "#         self.models = models\n",
    "#         self.apply_softmax = apply_softmax\n",
    "\n",
    "#     def call(self, x, training=False):\n",
    "#         prob_sum = 0\n",
    "#         for model in self.models:\n",
    "#             logits = model(x, training=training)\n",
    "#             probs = tf.nn.softmax(logits, axis=-1) if self.apply_softmax else logits\n",
    "#             prob_sum += probs\n",
    "#         avg_prob = prob_sum / len(self.models)\n",
    "#         final_pred = tf.argmax(avg_prob, axis=-1)\n",
    "#         return final_pred\n",
    "\n",
    "# ‚úÖ Evaluation Function (for normal + ensemble models)\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16, is_ensemble=False):\n",
    "    if is_ensemble:\n",
    "        y_pred = []\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            batch_x = X_test[i:i+batch_size]\n",
    "            preds = model(batch_x, training=False).numpy()  # [B, H, W]\n",
    "            y_pred.extend(preds)\n",
    "        y_pred = np.array(y_pred)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "        y_pred = np.argmax(y_pred, axis=-1)  # [B, H, W]\n",
    "\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    class_metrics = {\n",
    "        i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []}\n",
    "        for i in range(num_classes)\n",
    "    }\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]\n",
    "        pred_mask = y_pred[i]\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # üìä Print results\n",
    "    print(f\"{'Class':<10}{'Dice Coef (%)':<15}{'IoU (%)':<12}{'Precision (%)':<17}{'Recall (%)':<15}{'F1 Score (%)':<17}{'Accuracy (%)':<17}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"{class_idx:<10}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['dice']) * 100:>10.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['iou']) * 100:>12.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['precision']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['recall']) * 100:>15.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['f1']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['accuracy']) * 100:>17.2f}\")\n",
    "        # Optional: Print Hausdorff and ASD if needed\n",
    "\n",
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_xception],\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_xception],\n",
    "    apply_softmax=False,\n",
    "    return_probs=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_efficientnetb4],\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_efficientnetb4],\n",
    "    apply_softmax=False,      # because both models already output probabilities\n",
    "    return_probs=True         # required for custom loss/metrics to work\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_xception, model_efficientnetb4],\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_xception, model_efficientnetb4],\n",
    "    apply_softmax=False,      # because both models already output probabilities\n",
    "    return_probs=True         # required for custom loss/metrics to work\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_inceptionresnetv2, model_efficientnetb4],\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_inceptionresnetv2, model_efficientnetb4],\n",
    "    apply_softmax=False,      # because both models already output probabilities\n",
    "    return_probs=True         # required for custom loss/metrics to work\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_inceptionresnetv2, model_efficientnetb4],\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_inceptionresnetv2, model_efficientnetb4],\n",
    "    apply_softmax=False,      # because both models already output probabilities\n",
    "    return_probs=True         # required for custom loss/metrics to work\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_xception, model_inceptionresnetv2, model_efficientnetb4],\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_xception, model_inceptionresnetv2, model_efficientnetb4],\n",
    "    apply_softmax=False,      # because both models already output probabilities\n",
    "    return_probs=True         # required for custom loss/metrics to work\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_xception, model_efficientnetb4],\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_xception, model_efficientnetb4],\n",
    "    apply_softmax=False,      # because both models already output probabilities\n",
    "    return_probs=True         # required for custom loss/metrics to work\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_xception, model_inceptionresnetv2, model_efficientnetb4],\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_segnet, model_xception, model_inceptionresnetv2, model_efficientnetb4],\n",
    "    apply_softmax=False,      # because both models already output probabilities\n",
    "    return_probs=True         # required for custom loss/metrics to work\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "# ‚úÖ RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ RGB mask to class mask\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Dice\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ‚úÖ IoU\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# ‚úÖ Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# ‚úÖ Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    distances = [np.min(np.linalg.norm(pred_points - pt, axis=1)) for pt in true_points]\n",
    "    return np.mean(distances)\n",
    "\n",
    "# ‚úÖ Soft Voting Ensemble Model\n",
    "class SoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, apply_softmax=True):\n",
    "        super(SoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        prob_sum = 0\n",
    "        for model in self.models:\n",
    "            logits = model(x, training=training)\n",
    "            probs = tf.nn.softmax(logits, axis=-1) if self.apply_softmax else logits\n",
    "            prob_sum += probs\n",
    "        avg_prob = prob_sum / len(self.models)\n",
    "        final_pred = tf.argmax(avg_prob, axis=-1)\n",
    "        return final_pred\n",
    "\n",
    "# ‚úÖ Evaluation Function (for normal + ensemble models)\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16, is_ensemble=False):\n",
    "    if is_ensemble:\n",
    "        y_pred = []\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            batch_x = X_test[i:i+batch_size]\n",
    "            preds = model(batch_x, training=False).numpy()  # [B, H, W]\n",
    "            y_pred.extend(preds)\n",
    "        y_pred = np.array(y_pred)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "        y_pred = np.argmax(y_pred, axis=-1)  # [B, H, W]\n",
    "\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    class_metrics = {\n",
    "        i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []}\n",
    "        for i in range(num_classes)\n",
    "    }\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]\n",
    "        pred_mask = y_pred[i]\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # üìä Print results\n",
    "    print(f\"{'Class':<10}{'Dice Coef (%)':<15}{'IoU (%)':<12}{'Precision (%)':<17}{'Recall (%)':<15}{'F1 Score (%)':<17}{'Accuracy (%)':<17}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"{class_idx:<10}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['dice']) * 100:>10.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['iou']) * 100:>12.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['precision']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['recall']) * 100:>15.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['f1']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['accuracy']) * 100:>17.2f}\")\n",
    "        # Optional: Print Hausdorff and ASD if needed\n",
    "\n",
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_xception, model_inceptionresnetv2],\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_xception, model_inceptionresnetv2],\n",
    "    apply_softmax=False,\n",
    "    return_probs=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "# ‚úÖ RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ RGB mask to class mask\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Dice\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ‚úÖ IoU\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# ‚úÖ Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# ‚úÖ Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    distances = [np.min(np.linalg.norm(pred_points - pt, axis=1)) for pt in true_points]\n",
    "    return np.mean(distances)\n",
    "\n",
    "# ‚úÖ Soft Voting Ensemble Model\n",
    "class SoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, apply_softmax=True):\n",
    "        super(SoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        prob_sum = 0\n",
    "        for model in self.models:\n",
    "            logits = model(x, training=training)\n",
    "            probs = tf.nn.softmax(logits, axis=-1) if self.apply_softmax else logits\n",
    "            prob_sum += probs\n",
    "        avg_prob = prob_sum / len(self.models)\n",
    "        final_pred = tf.argmax(avg_prob, axis=-1)\n",
    "        return final_pred\n",
    "\n",
    "# ‚úÖ Evaluation Function (for normal + ensemble models)\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16, is_ensemble=False):\n",
    "    if is_ensemble:\n",
    "        y_pred = []\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            batch_x = X_test[i:i+batch_size]\n",
    "            preds = model(batch_x, training=False).numpy()  # [B, H, W]\n",
    "            y_pred.extend(preds)\n",
    "        y_pred = np.array(y_pred)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "        y_pred = np.argmax(y_pred, axis=-1)  # [B, H, W]\n",
    "\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    class_metrics = {\n",
    "        i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []}\n",
    "        for i in range(num_classes)\n",
    "    }\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]\n",
    "        pred_mask = y_pred[i]\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # üìä Print results\n",
    "    print(f\"{'Class':<10}{'Dice Coef (%)':<15}{'IoU (%)':<12}{'Precision (%)':<17}{'Recall (%)':<15}{'F1 Score (%)':<17}{'Accuracy (%)':<17}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"{class_idx:<10}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['dice']) * 100:>10.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['iou']) * 100:>12.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['precision']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['recall']) * 100:>15.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['f1']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['accuracy']) * 100:>17.2f}\")\n",
    "\n",
    "# Optional: Print Hausdorff and ASD if needed\n",
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_xception, model_segnet, model_inceptionresnetv2],\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_xception, model_segnet, model_inceptionresnetv2],\n",
    "    apply_softmax=False,\n",
    "    return_probs=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = SoftVotingEnsemble(\n",
    "    models=[model_xception, model_segnet, model_inceptionresnetv2],\n",
    "    apply_softmax=False,\n",
    "    return_probs=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Majority Voting</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import stats  # for mode\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class HardVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models):\n",
    "        super(HardVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            logits = model(x, training=training)\n",
    "            pred_mask = tf.argmax(logits, axis=-1)  # [B, H, W]\n",
    "            predictions.append(pred_mask)\n",
    "\n",
    "        stacked_preds = tf.stack(predictions, axis=0)  # [N_models, B, H, W]\n",
    "        stacked_preds = tf.transpose(stacked_preds, [1, 2, 3, 0])  # [B, H, W, N_models]\n",
    "\n",
    "        mode_preds = tf.numpy_function(\n",
    "            func=lambda x: stats.mode(x, axis=-1)[0],\n",
    "            inp=[stacked_preds],\n",
    "            Tout=tf.int64\n",
    "        )\n",
    "\n",
    "        return mode_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "\n",
    "# class HardVotingEnsemble(tf.keras.Model):\n",
    "#     def __init__(self, models, num_classes):\n",
    "#         super(HardVotingEnsemble, self).__init__()\n",
    "#         self.models = models\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#     def call(self, x, training=False):\n",
    "#         predictions = []\n",
    "#         for model in self.models:\n",
    "#             logits = model(x, training=training)               # [B, H, W, C]\n",
    "#             pred_mask = tf.argmax(logits, axis=-1)             # [B, H, W]\n",
    "#             predictions.append(pred_mask)\n",
    "\n",
    "#         stacked_preds = tf.stack(predictions, axis=0)          # [N_models, B, H, W]\n",
    "#         stacked_preds = tf.transpose(stacked_preds, [1, 2, 3, 0])  # [B, H, W, N_models]\n",
    "\n",
    "#         # Use numpy + scipy mode\n",
    "#         def compute_mode(x):\n",
    "#             mode, _ = stats.mode(x, axis=-1, keepdims=False)\n",
    "#             return mode.astype(np.int32)\n",
    "\n",
    "#         mode_preds = tf.numpy_function(\n",
    "#             func=compute_mode,\n",
    "#             inp=[stacked_preds],\n",
    "#             Tout=tf.int32\n",
    "#         )\n",
    "\n",
    "#         # Manually set output shape: [B, H, W]\n",
    "#         batch_size = tf.shape(x)[0]\n",
    "#         height = tf.shape(x)[1]\n",
    "#         width = tf.shape(x)[2]\n",
    "#         mode_preds.set_shape([None, None, None])  # Symbolic shape for [B, H, W]\n",
    "\n",
    "#         one_hot_preds = tf.one_hot(mode_preds, depth=self.num_classes)  # [B, H, W, C]\n",
    "#         return one_hot_preds\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "\n",
    "# class HardVotingEnsemble(tf.keras.Model):\n",
    "#     def __init__(self, models, num_classes, return_probs=True):\n",
    "#         super(HardVotingEnsemble, self).__init__()\n",
    "#         self.models = models\n",
    "#         self.num_classes = num_classes\n",
    "#         self.return_probs = return_probs\n",
    "\n",
    "#     def call(self, x, training=False):\n",
    "#         predictions = []\n",
    "\n",
    "#         for model in self.models:\n",
    "#             output = model(x, training=training)\n",
    "\n",
    "#             # Handle EfficientNet-like models with built-in softmax\n",
    "#             is_softmaxed = hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "#             probs = output if is_softmaxed else tf.nn.softmax(output, axis=-1)\n",
    "\n",
    "#             pred_mask = tf.argmax(probs, axis=-1)  # [B, H, W]\n",
    "#             predictions.append(pred_mask)\n",
    "\n",
    "#         stacked_preds = tf.stack(predictions, axis=0)  # [N_models, B, H, W]\n",
    "#         stacked_preds = tf.transpose(stacked_preds, [1, 2, 3, 0])  # [B, H, W, N_models]\n",
    "\n",
    "#         # Use scipy mode to find majority vote\n",
    "#         def compute_mode(x):\n",
    "#             mode, _ = stats.mode(x, axis=-1, keepdims=False)\n",
    "#             return mode.astype(np.int32)\n",
    "\n",
    "#         mode_preds = tf.numpy_function(\n",
    "#             func=compute_mode,\n",
    "#             inp=[stacked_preds],\n",
    "#             Tout=tf.int32\n",
    "#         )\n",
    "#         mode_preds.set_shape([None, None, None])  # [B, H, W]\n",
    "\n",
    "#         if self.return_probs:\n",
    "#             return tf.one_hot(mode_preds, depth=self.num_classes)  # [B, H, W, C]\n",
    "#         else:\n",
    "#             return mode_preds  # [B, H, W]\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class HardVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, num_classes):\n",
    "        super(HardVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            logits = model(x, training=training)               # [B, H, W, C]\n",
    "            pred_mask = tf.argmax(logits, axis=-1)             # [B, H, W]\n",
    "            predictions.append(pred_mask)\n",
    "\n",
    "        stacked_preds = tf.stack(predictions, axis=0)          # [N_models, B, H, W]\n",
    "        stacked_preds = tf.transpose(stacked_preds, [1, 2, 3, 0])  # [B, H, W, N_models]\n",
    "\n",
    "        # Use numpy + scipy mode\n",
    "        def compute_mode(x):\n",
    "            mode, _ = stats.mode(x, axis=-1, keepdims=False)\n",
    "            return mode.astype(np.int32)\n",
    "\n",
    "        mode_preds = tf.numpy_function(\n",
    "            func=compute_mode,\n",
    "            inp=[stacked_preds],\n",
    "            Tout=tf.int32\n",
    "        )\n",
    "\n",
    "        # Manually set output shape: [B, H, W]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        height = tf.shape(x)[1]\n",
    "        width = tf.shape(x)[2]\n",
    "        mode_preds.set_shape([None, None, None])  # Symbolic shape for [B, H, W]\n",
    "\n",
    "        one_hot_preds = tf.one_hot(mode_preds, depth=self.num_classes)  # [B, H, W, C]\n",
    "        return one_hot_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "# ‚úÖ RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ RGB mask to class mask\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Dice\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ‚úÖ IoU\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# ‚úÖ Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# ‚úÖ Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    distances = [np.min(np.linalg.norm(pred_points - pt, axis=1)) for pt in true_points]\n",
    "    return np.mean(distances)\n",
    "\n",
    "# ‚úÖ Soft Voting Ensemble Model\n",
    "class SoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, apply_softmax=True):\n",
    "        super(SoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        prob_sum = 0\n",
    "        for model in self.models:\n",
    "            logits = model(x, training=training)\n",
    "            probs = tf.nn.softmax(logits, axis=-1) if self.apply_softmax else logits\n",
    "            prob_sum += probs\n",
    "        avg_prob = prob_sum / len(self.models)\n",
    "        final_pred = tf.argmax(avg_prob, axis=-1)\n",
    "        return final_pred\n",
    "\n",
    "# ‚úÖ Evaluation Function (for normal + ensemble models)\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16, is_ensemble=False):\n",
    "    if is_ensemble:\n",
    "        y_pred = []\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            batch_x = X_test[i:i+batch_size]\n",
    "            preds = model(batch_x, training=False).numpy()  # [B, H, W]\n",
    "            y_pred.extend(preds)\n",
    "        y_pred = np.array(y_pred)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "        y_pred = np.argmax(y_pred, axis=-1)  # [B, H, W]\n",
    "\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    class_metrics = {\n",
    "        i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []}\n",
    "        for i in range(num_classes)\n",
    "    }\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]\n",
    "        pred_mask = y_pred[i]\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # üìä Print results\n",
    "    print(f\"{'Class':<10}{'Dice Coef (%)':<15}{'IoU (%)':<12}{'Precision (%)':<17}{'Recall (%)':<15}{'F1 Score (%)':<17}{'Accuracy (%)':<17}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"{class_idx:<10}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['dice']) * 100:>10.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['iou']) * 100:>12.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['precision']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['recall']) * 100:>15.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['f1']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['accuracy']) * 100:>17.2f}\")\n",
    "        # Optional: Print Hausdorff and ASD if needed\n",
    "\n",
    "ensemble_model = HardVotingEnsemble([model_segnet, model_inceptionresnetv2])\n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "\n",
    "    dice_loss_val = 1 - (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss_val = tf.reduce_mean(dice_loss_val)\n",
    "\n",
    "    lovasz_loss_val = lovasz_softmax_loss(y_true, tf.nn.softmax(y_pred), ignore_background=False)\n",
    "    return lovasz_loss_val + dice_loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble = HardVotingEnsemble(\n",
    "    models=[model_segnet, model_inceptionresnetv2],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "hard_ensemble.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "# ‚úÖ RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ RGB mask to class mask\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Dice\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ‚úÖ IoU\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# ‚úÖ Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# ‚úÖ Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    distances = [np.min(np.linalg.norm(pred_points - pt, axis=1)) for pt in true_points]\n",
    "    return np.mean(distances)\n",
    "\n",
    "# ‚úÖ Soft Voting Ensemble Model\n",
    "class SoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, apply_softmax=True):\n",
    "        super(SoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        prob_sum = 0\n",
    "        for model in self.models:\n",
    "            logits = model(x, training=training)\n",
    "            probs = tf.nn.softmax(logits, axis=-1) if self.apply_softmax else logits\n",
    "            prob_sum += probs\n",
    "        avg_prob = prob_sum / len(self.models)\n",
    "        final_pred = tf.argmax(avg_prob, axis=-1)\n",
    "        return final_pred\n",
    "\n",
    "# ‚úÖ Evaluation Function (for normal + ensemble models)\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16, is_ensemble=False):\n",
    "    if is_ensemble:\n",
    "        y_pred = []\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            batch_x = X_test[i:i+batch_size]\n",
    "            preds = model(batch_x, training=False).numpy()  # [B, H, W]\n",
    "            y_pred.extend(preds)\n",
    "        y_pred = np.array(y_pred)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "        y_pred = np.argmax(y_pred, axis=-1)  # [B, H, W]\n",
    "\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    class_metrics = {\n",
    "        i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []}\n",
    "        for i in range(num_classes)\n",
    "    }\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]\n",
    "        pred_mask = y_pred[i]\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # üìä Print results\n",
    "    print(f\"{'Class':<10}{'Dice Coef (%)':<15}{'IoU (%)':<12}{'Precision (%)':<17}{'Recall (%)':<15}{'F1 Score (%)':<17}{'Accuracy (%)':<17}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"{class_idx:<10}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['dice']) * 100:>10.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['iou']) * 100:>12.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['precision']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['recall']) * 100:>15.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['f1']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['accuracy']) * 100:>17.2f}\")\n",
    "        # Optional: Print Hausdorff and ASD if needed\n",
    "\n",
    "ensemble_model = HardVotingEnsemble([model_segnet, model_xception])\n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "# ‚úÖ RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ RGB mask to class mask\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Dice\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ‚úÖ IoU\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# ‚úÖ Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# ‚úÖ Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    distances = [np.min(np.linalg.norm(pred_points - pt, axis=1)) for pt in true_points]\n",
    "    return np.mean(distances)\n",
    "\n",
    "# ‚úÖ Soft Voting Ensemble Model\n",
    "class SoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, apply_softmax=True):\n",
    "        super(SoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        prob_sum = 0\n",
    "        for model in self.models:\n",
    "            logits = model(x, training=training)\n",
    "            probs = tf.nn.softmax(logits, axis=-1) if self.apply_softmax else logits\n",
    "            prob_sum += probs\n",
    "        avg_prob = prob_sum / len(self.models)\n",
    "        final_pred = tf.argmax(avg_prob, axis=-1)\n",
    "        return final_pred\n",
    "\n",
    "# ‚úÖ Evaluation Function (for normal + ensemble models)\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16, is_ensemble=False):\n",
    "    if is_ensemble:\n",
    "        y_pred = []\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            batch_x = X_test[i:i+batch_size]\n",
    "            preds = model(batch_x, training=False).numpy()  # [B, H, W]\n",
    "            y_pred.extend(preds)\n",
    "        y_pred = np.array(y_pred)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "        y_pred = np.argmax(y_pred, axis=-1)  # [B, H, W]\n",
    "\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    class_metrics = {\n",
    "        i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []}\n",
    "        for i in range(num_classes)\n",
    "    }\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]\n",
    "        pred_mask = y_pred[i]\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # üìä Print results\n",
    "    print(f\"{'Class':<10}{'Dice Coef (%)':<15}{'IoU (%)':<12}{'Precision (%)':<17}{'Recall (%)':<15}{'F1 Score (%)':<17}{'Accuracy (%)':<17}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"{class_idx:<10}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['dice']) * 100:>10.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['iou']) * 100:>12.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['precision']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['recall']) * 100:>15.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['f1']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['accuracy']) * 100:>17.2f}\")\n",
    "        # Optional: Print Hausdorff and ASD if needed\n",
    "\n",
    "ensemble_model = HardVotingEnsemble([model_xception, model_inceptionresnetv2])\n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble = HardVotingEnsemble(\n",
    "    models=[model_xception, model_inceptionresnetv2],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "hard_ensemble.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "# ‚úÖ RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ RGB mask to class mask\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Dice\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ‚úÖ IoU\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# ‚úÖ Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# ‚úÖ Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')\n",
    "    distances = [np.min(np.linalg.norm(pred_points - pt, axis=1)) for pt in true_points]\n",
    "    return np.mean(distances)\n",
    "\n",
    "# ‚úÖ Soft Voting Ensemble Model\n",
    "class SoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, apply_softmax=True):\n",
    "        super(SoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        prob_sum = 0\n",
    "        for model in self.models:\n",
    "            logits = model(x, training=training)\n",
    "            probs = tf.nn.softmax(logits, axis=-1) if self.apply_softmax else logits\n",
    "            prob_sum += probs\n",
    "        avg_prob = prob_sum / len(self.models)\n",
    "        final_pred = tf.argmax(avg_prob, axis=-1)\n",
    "        return final_pred\n",
    "\n",
    "# ‚úÖ Evaluation Function (for normal + ensemble models)\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16, is_ensemble=False):\n",
    "    if is_ensemble:\n",
    "        y_pred = []\n",
    "        for i in range(0, len(X_test), batch_size):\n",
    "            batch_x = X_test[i:i+batch_size]\n",
    "            preds = model(batch_x, training=False).numpy()  # [B, H, W]\n",
    "            y_pred.extend(preds)\n",
    "        y_pred = np.array(y_pred)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "        y_pred = np.argmax(y_pred, axis=-1)  # [B, H, W]\n",
    "\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    class_metrics = {\n",
    "        i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []}\n",
    "        for i in range(num_classes)\n",
    "    }\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]\n",
    "        pred_mask = y_pred[i]\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # üìä Print results\n",
    "    print(f\"{'Class':<10}{'Dice Coef (%)':<15}{'IoU (%)':<12}{'Precision (%)':<17}{'Recall (%)':<15}{'F1 Score (%)':<17}{'Accuracy (%)':<17}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"{class_idx:<10}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['dice']) * 100:>10.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['iou']) * 100:>12.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['precision']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['recall']) * 100:>15.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['f1']) * 100:>17.2f}\"\n",
    "              f\"{np.mean(class_metrics[class_idx]['accuracy']) * 100:>17.2f}\")\n",
    "        # Optional: Print Hausdorff and ASD if needed\n",
    "\n",
    "ensemble_model = HardVotingEnsemble([model_xception, model_segnet, model_inceptionresnetv2])\n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble = HardVotingEnsemble(\n",
    "    models=[model_xception, model_segnet, model_inceptionresnetv2],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "hard_ensemble.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = HardVotingEnsemble(\n",
    "    models=[model_segnet, model_efficientnetb4],\n",
    "    # num_classes=4,\n",
    "    # return_probs=False\n",
    ")\n",
    "\n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = HardVotingEnsemble(\n",
    "    models=[model_segnet, model_efficientnetb4],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "hard_ensemble.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = HardVotingEnsemble([model_xception, model_efficientnetb4], num_classes=4)\n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble = HardVotingEnsemble(\n",
    "    models=[model_xception, model_efficientnetb4],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "hard_ensemble.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = HardVotingEnsemble([model_efficientnetb4, model_inceptionresnetv2])\n",
    "    # num_classes=4,\n",
    "    # return_probs=False \n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble = HardVotingEnsemble(\n",
    "    models=[model_efficientnetb4, model_inceptionresnetv2],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "hard_ensemble.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = HardVotingEnsemble([model_xception, model_segnet, model_efficientnetb4])\n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble = HardVotingEnsemble(\n",
    "    models=[model_xception, model_segnet, model_efficientnetb4],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "hard_ensemble.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = HardVotingEnsemble([model_xception, model_efficientnetb4, model_inceptionresnetv2])\n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble = HardVotingEnsemble(\n",
    "    models=[model_xception, model_efficientnetb4, model_inceptionresnetv2],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "hard_ensemble.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = HardVotingEnsemble([model_efficientnetb4, model_segnet, model_inceptionresnetv2])\n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble = HardVotingEnsemble(\n",
    "    models=[model_efficientnetb4, model_segnet, model_inceptionresnetv2],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "hard_ensemble.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = HardVotingEnsemble([model_xception, model_segnet, model_inceptionresnetv2, model_efficientnetb4])\n",
    "\n",
    "# Evaluate like before (same evaluation function as used with soft voting)\n",
    "evaluate_classwise_metrics(ensemble_model, X_test, y_test, is_ensemble=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble = HardVotingEnsemble(\n",
    "    models=[model_xception, model_segnet, model_inceptionresnetv2, model_efficientnetb4],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "hard_ensemble.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Weighted Soft Voting</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# ‚úÖ Softmax wrapper\n",
    "def get_softmax_preds(model, dataset):\n",
    "    preds = []\n",
    "    for x_batch, _ in dataset:\n",
    "        logits = model(x_batch, training=False)\n",
    "        probs = tf.nn.softmax(logits)  # [B, H, W, C]\n",
    "        preds.append(probs)\n",
    "    return tf.concat(preds, axis=0)\n",
    "\n",
    "# ‚úÖ One-hot true labels from dataset\n",
    "def get_ground_truth(dataset):\n",
    "    y_true_list = [y for _, y in dataset]\n",
    "    return tf.concat(y_true_list, axis=0)\n",
    "\n",
    "# ‚úÖ Dice calculation\n",
    "def dice_score(y_true, y_pred, smooth=1e-6):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice).numpy()\n",
    "\n",
    "# ‚úÖ Optuna objective function\n",
    "def objective(trial):\n",
    "    # Sample weights and normalize them\n",
    "    raw_weights = [trial.suggest_float(f'w{i}', 0.0, 1.0) for i in range(len(models))]\n",
    "    total = sum(raw_weights)\n",
    "    weights = [w / total for w in raw_weights]\n",
    "\n",
    "    # Weighted ensemble\n",
    "    ensemble_pred = sum(w * p for w, p in zip(weights, all_softmax_preds))\n",
    "    dice = dice_score(y_true, ensemble_pred)\n",
    "    return dice\n",
    "\n",
    "# Step 1: Get predictions from each model\n",
    "models = [model_xception, model_segnet, model_inceptionresnetv2, model_efficientnetb4]\n",
    "all_softmax_preds = [get_softmax_preds(m, val_dataset) for m in models]\n",
    "\n",
    "# Step 2: Get validation ground truth\n",
    "y_true = get_ground_truth(val_dataset)\n",
    "\n",
    "# Step 3: Optimize with Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Step 4: Get best weights, rounded\n",
    "best_raw = [study.best_trial.params[f'w{i}'] for i in range(len(models))]\n",
    "total = sum(best_raw)\n",
    "best_weights = [round(w / total, 4) for w in best_raw]\n",
    "\n",
    "print(\"‚úÖ Best weights (normalized, rounded):\", best_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class WeightedSoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights=None, apply_softmax=True):\n",
    "        super(WeightedSoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        else:\n",
    "            total = sum(weights)\n",
    "            weights = [w / total for w in weights]\n",
    "\n",
    "        self.model_weights = tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        weighted_sum = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            output = model(x, training=training)\n",
    "\n",
    "            is_softmaxed = (\n",
    "                hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "            )\n",
    "\n",
    "            if self.apply_softmax and not is_softmaxed:\n",
    "                probs = tf.nn.softmax(output, axis=-1)\n",
    "            else:\n",
    "                probs = output\n",
    "\n",
    "            weighted_sum += self.model_weights[i] * probs\n",
    "\n",
    "        avg_prob = weighted_sum  # shape: [B, H, W, C]\n",
    "\n",
    "        # üîÅ Convert to one-hot for metric compatibility\n",
    "        one_hot_pred = tf.one_hot(tf.argmax(avg_prob, axis=-1), depth=avg_prob.shape[-1])\n",
    "        return one_hot_pred  # [B, H, W, C]\n",
    "\n",
    "        \n",
    "weights = [1, 1, 1, 1]\n",
    "\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=[model_xception, model_segnet, model_inceptionresnetv2, model_efficientnetb4],\n",
    "    weights=weights\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "# === YOUR TRAINED MODELS HERE ===\n",
    "models = [\n",
    "    model_xception,\n",
    "    model_segnet,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "# === INPUTS ===\n",
    "# models: your list of trained models\n",
    "# X_val, y_val: validation data as numpy arrays or tensors (one-hot encoded y_val)\n",
    "# Example: y_val shape = [B, H, W, C], with one-hot encoding\n",
    "\n",
    "# === STEP 1: Batch-wise softmax predictions from each model ===\n",
    "def get_softmax_preds_from_array(model, X, batch_size=16):\n",
    "    soft_preds = []\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        x_batch = X[i:i+batch_size]\n",
    "        logits = model(x_batch, training=False)\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        soft_preds.append(probs)\n",
    "    return tf.concat(soft_preds, axis=0)\n",
    "\n",
    "# === STEP 2: Mean Dice (one-hot predictions only) ===\n",
    "def mean_dice_per_class(y_true, y_pred_soft, smooth=1e-6):\n",
    "    y_pred_argmax = tf.argmax(y_pred_soft, axis=-1)                     # [B, H, W]\n",
    "    y_pred = tf.one_hot(y_pred_argmax, depth=y_pred_soft.shape[-1])    # [B, H, W, C]\n",
    "\n",
    "    dice_scores = []\n",
    "    for i in range(y_true.shape[-1]):\n",
    "        y_true_c = y_true[..., i]\n",
    "        y_pred_c = y_pred[..., i]\n",
    "        intersection = tf.reduce_sum(y_true_c * y_pred_c)\n",
    "        union = tf.reduce_sum(y_true_c) + tf.reduce_sum(y_pred_c)\n",
    "        dice = (2. * intersection + smooth) / (union + smooth)\n",
    "        dice_scores.append(dice)\n",
    "    return tf.reduce_mean(tf.stack(dice_scores)).numpy()\n",
    "\n",
    "# === STEP 3: Precompute softmax predictions ===\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "# === STEP 4: Optuna Objective Function ===\n",
    "def objective(trial):\n",
    "    raw_weights = [trial.suggest_float(f'w{i}', 0.0, 1.0) for i in range(len(models))]\n",
    "    total = sum(raw_weights)\n",
    "    weights = [w / total for w in raw_weights]\n",
    "\n",
    "    # Weighted average of soft predictions\n",
    "    weighted_avg = sum(w * p for w, p in zip(weights, soft_preds_all))\n",
    "\n",
    "    # ‚úÖ Compute mean Dice with one-hot predictions\n",
    "    score = mean_dice_per_class(y_true_val, weighted_avg)\n",
    "    return score\n",
    "\n",
    "# === STEP 5: Run Optuna Search ===\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "# === Final WeightedSoftVotingEnsemble using best weights ===\n",
    "class WeightedSoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights=None, apply_softmax=True):\n",
    "        super(WeightedSoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        else:\n",
    "            total = sum(weights)\n",
    "            weights = [w / total for w in weights]\n",
    "\n",
    "        self.model_weights = tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        weighted_sum = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            output = model(x, training=training)\n",
    "\n",
    "            is_softmaxed = (\n",
    "                hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "            )\n",
    "\n",
    "            if self.apply_softmax and not is_softmaxed:\n",
    "                probs = tf.nn.softmax(output, axis=-1)\n",
    "            else:\n",
    "                probs = output\n",
    "\n",
    "            weighted_sum += self.model_weights[i] * probs\n",
    "\n",
    "        avg_prob = weighted_sum  # shape: [B, H, W, C]\n",
    "\n",
    "        # üîÅ Convert to one-hot for metric compatibility\n",
    "        one_hot_pred = tf.one_hot(tf.argmax(avg_prob, axis=-1), depth=avg_prob.shape[-1])\n",
    "        return one_hot_pred  # [B, H, W, C]\n",
    "\n",
    "# === Create the final ensemble model ===\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Final WeightedSoftVotingEnsemble using best weights ===\n",
    "class WeightedSoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights=None, apply_softmax=True):\n",
    "        super(WeightedSoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        else:\n",
    "            total = sum(weights)\n",
    "            weights = [w / total for w in weights]\n",
    "\n",
    "        self.model_weights = tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        weighted_sum = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            output = model(x, training=training)\n",
    "\n",
    "            is_softmaxed = (\n",
    "                hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "            )\n",
    "\n",
    "            if self.apply_softmax and not is_softmaxed:\n",
    "                probs = tf.nn.softmax(output, axis=-1)\n",
    "            else:\n",
    "                probs = output\n",
    "\n",
    "            weighted_sum += self.model_weights[i] * probs\n",
    "\n",
    "        avg_prob = weighted_sum  # shape: [B, H, W, C]\n",
    "\n",
    "        # üîÅ Convert to one-hot for metric compatibility\n",
    "        one_hot_pred = tf.one_hot(tf.argmax(avg_prob, axis=-1), depth=avg_prob.shape[-1])\n",
    "        return one_hot_pred  # [B, H, W, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_segnet,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "final_weights = [0.3717, 0.301, 0.1892, 0.1381]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_inceptionresnetv2,\n",
    "    model_segnet\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_segnet,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_segnet,\n",
    "    model_xception,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_inceptionresnetv2\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_inceptionresnetv2,\n",
    "    model_segnet\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_segnet\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_segnet,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    model_xception,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Precomputing predictions from all models...\")\n",
    "soft_preds_all = [get_softmax_preds_from_array(m, X_val) for m in models]\n",
    "y_true_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
    "\n",
    "print(\"üéØ Running Optuna for best ensemble weights...\")\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# === STEP 6: Extract & Format Best Weights ===\n",
    "best_raw_weights = [study.best_trial.params[f\"w{i}\"] for i in range(len(models))]\n",
    "total = sum(best_raw_weights)\n",
    "final_weights = [round(w / total, 4) for w in best_raw_weights]\n",
    "\n",
    "print(\"\\n‚úÖ Best Ensemble Weights (rounded):\", final_weights)\n",
    "print(\"‚úÖ Best Mean Dice Score:\", round(study.best_value, 5))\n",
    "\n",
    "print(final_weights)\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def decode_segmentation(mask, num_classes=4):\n",
    "    \"\"\"\n",
    "    Converts a one-hot encoded mask or softmax prediction into a color image.\n",
    "    \"\"\"\n",
    "    label_mask = np.argmax(mask, axis=-1)  # shape: (H, W)\n",
    "    colors = [\n",
    "        (0, 0, 0),         # Background - Black\n",
    "        (255, 0, 0),       # Brain - Red\n",
    "        (0, 255, 0),       # CSP - Green\n",
    "        (0, 0, 255),       # LV - Blue\n",
    "    ]\n",
    "\n",
    "    color_mask = np.zeros((*label_mask.shape, 3), dtype=np.uint8)\n",
    "    for cls_idx, color in enumerate(colors):\n",
    "        color_mask[label_mask == cls_idx] = color\n",
    "    return color_mask\n",
    "\n",
    "def display_predictions(model, X, y_true, num_samples=5):\n",
    "    indices = random.sample(range(len(X)), num_samples)\n",
    "    X_samples = X[indices]\n",
    "    y_samples = y_true[indices]\n",
    "\n",
    "    preds = model.predict(X_samples)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        image = X_samples[i]\n",
    "        true_mask = decode_segmentation(y_samples[i])\n",
    "        pred_mask = decode_segmentation(preds[i])\n",
    "\n",
    "        # Plot side-by-side\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Ground truth overlay\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image.astype(np.uint8))\n",
    "        plt.imshow(true_mask, alpha=0.5)\n",
    "        plt.title(\"Ground Truth Overlay\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Prediction overlay\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(image.astype(np.uint8))\n",
    "        plt.imshow(pred_mask, alpha=0.5)\n",
    "        plt.title(\"Prediction Overlay\")\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_predictions(model, X_test, y_test, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Function to decode one-hot encoded mask or softmax prediction into RGB\n",
    "def decode_segmentation(mask, num_classes=4):\n",
    "    label_mask = np.argmax(mask, axis=-1)  # shape: (H, W)\n",
    "    \n",
    "    colors = [\n",
    "        (0, 0, 0),         # Background - Black\n",
    "        (255, 0, 0),       # Brain - Red\n",
    "        (0, 255, 0),       # CSP - Green\n",
    "        (0, 0, 255),       # LV - Blue\n",
    "    ]\n",
    "    \n",
    "    color_mask = np.zeros((*label_mask.shape, 3), dtype=np.uint8)\n",
    "    for cls_idx, color in enumerate(colors):\n",
    "        color_mask[label_mask == cls_idx] = color\n",
    "    return color_mask\n",
    "\n",
    "# Function to overlay mask on an image\n",
    "def overlay_mask_on_image(image, mask, alpha=0.5):\n",
    "    overlay = image.copy()\n",
    "    if overlay.max() <= 1.0:\n",
    "        overlay = (overlay * 255).astype(np.uint8)\n",
    "\n",
    "    if mask.max() <= 1.0:\n",
    "        mask = (mask * 255).astype(np.uint8)\n",
    "    \n",
    "    overlay = overlay.astype(np.float32)\n",
    "    mask = mask.astype(np.float32)\n",
    "    \n",
    "    combined = cv2.addWeighted(overlay, 1 - alpha, mask, alpha, 0)\n",
    "    return combined.astype(np.uint8)\n",
    "\n",
    "# Main display function for N samples\n",
    "def display_overlay_predictions(model, X, y_true, num_samples=5):\n",
    "    import cv2\n",
    "    indices = random.sample(range(len(X)), num_samples)\n",
    "    \n",
    "    X_batch = X[indices]\n",
    "    y_batch = y_true[indices]\n",
    "    y_pred_batch = model.predict(X_batch)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image = X_batch[i]\n",
    "        true_mask = decode_segmentation(y_batch[i])\n",
    "        pred_mask = decode_segmentation(y_pred_batch[i])\n",
    "\n",
    "        # Make sure image is uint8\n",
    "        if image.max() <= 1.0:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "\n",
    "        gt_overlay = overlay_mask_on_image(image, true_mask, alpha=0.5)\n",
    "        pred_overlay = overlay_mask_on_image(image, pred_mask, alpha=0.5)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(gt_overlay)\n",
    "        plt.title(\"Ground Truth Overlay\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(pred_overlay)\n",
    "        plt.title(\"Predicted Overlay\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "display_overlay_predictions(model, X_test, y_test, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Knowledge Distillation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_dir = r\"D:\\augmented_dataset\\images\"\n",
    "mask_dir = r\"D:\\augmented_dataset\\masks\"\n",
    "\n",
    "# # ‚úÖ Define destination directories\n",
    "train_image_dir = r\"D:\\Updated\\train\\images\"\n",
    "train_mask_dir = r\"D:\\Updated\\train\\masks\"\n",
    "val_image_dir = r\"D:\\Updated\\val\\images\"\n",
    "val_mask_dir = r\"D:\\Updated\\val\\masks\"\n",
    "test_image_dir = r\"D:\\Updated\\test\\images\"\n",
    "test_mask_dir = r\"D:\\Updated\\test\\masks\"\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import re\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ‚úÖ Constants for 224x224\n",
    "IMG_HEIGHT = 224  # Ensure height is 224\n",
    "IMG_WIDTH = 224   # Ensure width is 224\n",
    "CHANNELS = 3  # RGB images\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "# ‚úÖ Class mapping from RGB to class index\n",
    "CLASS_MAP = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0,  # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ Fix sorting issue using natural sorting\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Sort filenames numerically instead of lexicographically.\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "# ‚úÖ Convert RGB mask to class index mask\n",
    "def rgb_to_class(mask_array):\n",
    "    \"\"\"Convert RGB mask to single-channel class index mask.\"\"\"\n",
    "    height, width, _ = mask_array.shape\n",
    "    class_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for rgb, class_idx in CLASS_MAP.items():\n",
    "        matches = np.all(mask_array == rgb, axis=-1)  # Ensure exact match\n",
    "        class_mask[matches] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Preprocess Filtered Dataset for 224x224\n",
    "def preprocess_filtered_dataset(image_dir, mask_dir):\n",
    "    \"\"\"Preprocess images & masks: normalize, resize, and convert masks to one-hot encoding.\"\"\"\n",
    "\n",
    "    # ‚úÖ Load and sort filenames correctly\n",
    "    image_filenames = sorted(os.listdir(image_dir), key=natural_sort_key)\n",
    "    mask_filenames = sorted(os.listdir(mask_dir), key=natural_sort_key)\n",
    "\n",
    "    valid_image_paths = []\n",
    "    valid_mask_paths = []\n",
    "\n",
    "    # ‚úÖ Ensure each image has a corresponding mask\n",
    "    for img_file, mask_file in zip(image_filenames, mask_filenames):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            valid_image_paths.append(img_path)\n",
    "            valid_mask_paths.append(mask_path)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping {img_file}: Missing image or mask\")\n",
    "\n",
    "    num_images = len(valid_image_paths)\n",
    "\n",
    "    # ‚úÖ Initialize arrays\n",
    "    X = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=np.float32)\n",
    "    y = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES), dtype=np.float32)  # One-hot encoded masks\n",
    "\n",
    "    print(f\"üöÄ Processing {num_images} filtered images and masks...\")\n",
    "\n",
    "    for idx, (img_path, mask_path) in enumerate(zip(valid_image_paths, valid_mask_paths)):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"‚úÖ Processed {idx}/{num_images} images\")\n",
    "\n",
    "        # ‚úÖ Load and Resize Image\n",
    "        img = cv2.imread(img_path)  # Read image in BGR format\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # Resize to (224,224)\n",
    "        img = img.astype(np.float32) / 255.0  # Normalize\n",
    "\n",
    "        # ‚úÖ Load and Resize Mask\n",
    "        mask = cv2.imread(mask_path)  # Read mask in BGR format\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)  # Resize mask correctly\n",
    "\n",
    "        # ‚úÖ Convert RGB mask to class mask\n",
    "        class_mask = rgb_to_class(mask)\n",
    "\n",
    "        # ‚úÖ One-hot encode the class mask\n",
    "        one_hot_mask = to_categorical(class_mask, num_classes=NUM_CLASSES)\n",
    "\n",
    "        # ‚úÖ Store preprocessed data\n",
    "        X[idx] = img\n",
    "        y[idx] = one_hot_mask\n",
    "\n",
    "        # ‚úÖ Clear memory to prevent memory leaks\n",
    "        del img, mask, class_mask, one_hot_mask\n",
    "        gc.collect()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ‚úÖ Process dataset splits\n",
    "X_train, y_train = preprocess_filtered_dataset(train_image_dir, train_mask_dir)\n",
    "X_val, y_val = preprocess_filtered_dataset(val_image_dir, val_mask_dir)\n",
    "X_test, y_test = preprocess_filtered_dataset(test_image_dir, test_mask_dir)\n",
    "\n",
    "# ‚úÖ Print dataset information\n",
    "print(\"\\n‚úÖ Dataset Splits:\")\n",
    "print(f\"  - Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"  - Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"  - Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Augmentation configuration for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# Fit the augmentation parameters on the training data\n",
    "train_datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Activation, UpSampling2D, Concatenate\n",
    "\n",
    "def conv_block(inputs, filters, kernel_size=(3, 3), padding=\"same\", use_batch_norm=True):\n",
    "    \"\"\"\n",
    "    Convolutional block with optional batch normalization\n",
    "    \"\"\"\n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(inputs)\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def UNetPlusPlus(input_shape=(224, 224, 3), num_classes=4, filters=[24, 48, 96, 192], use_batch_norm=True):\n",
    "    \"\"\"\n",
    "    UNet++ (Nested U-Net) model for multiclass segmentation\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input image dimensions (height, width, channels)\n",
    "        num_classes: Number of output classes for segmentation\n",
    "        filters: List of filter dimensions for each level\n",
    "        use_batch_norm: Whether to use batch normalization\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model: UNet++ model\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder (Downsampling path)\n",
    "    conv0_0 = conv_block(inputs, filters[0], use_batch_norm=use_batch_norm)\n",
    "    pool0 = MaxPooling2D(pool_size=(2, 2))(conv0_0)\n",
    "    \n",
    "    conv1_0 = conv_block(pool0, filters[1], use_batch_norm=use_batch_norm)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_0)\n",
    "    \n",
    "    conv2_0 = conv_block(pool1, filters[2], use_batch_norm=use_batch_norm)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2_0)\n",
    "    \n",
    "    conv3_0 = conv_block(pool2, filters[3], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    # Decoder (Upsampling path with nested dense skip connections)\n",
    "    # Level 1 skip connections\n",
    "    up1_0 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv3_0)\n",
    "    conv2_1 = conv_block(Concatenate()([up1_0, conv2_0]), filters[2], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    up0_1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv2_0)\n",
    "    conv1_1 = conv_block(Concatenate()([up0_1, conv1_0]), filters[1], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    up0_2 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv1_0)\n",
    "    conv0_1 = conv_block(Concatenate()([up0_2, conv0_0]), filters[0], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    # Level 2 skip connections\n",
    "    up1_1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv2_1)\n",
    "    conv1_2 = conv_block(Concatenate()([up1_1, conv1_0, conv1_1]), filters[1], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    up0_3 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv1_1)\n",
    "    conv0_2 = conv_block(Concatenate()([up0_3, conv0_0, conv0_1]), filters[0], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    # Level 3 skip connections\n",
    "    up0_4 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv1_2)\n",
    "    conv0_3 = conv_block(Concatenate()([up0_4, conv0_0, conv0_1, conv0_2]), filters[0], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    # Output segmentation map\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(conv0_3)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "student_model = UNetPlusPlus(input_shape=(224, 224, 3), num_classes=4)\n",
    "student_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Activation, UpSampling2D, Concatenate, Add\n",
    "\n",
    "def res_conv_block(inputs, filters, kernel_size=(3, 3), padding=\"same\", use_batch_norm=True):\n",
    "    \"\"\"\n",
    "    Residual convolutional block with skip connections\n",
    "    \"\"\"\n",
    "    # Store input for residual connection\n",
    "    shortcut = inputs\n",
    "    \n",
    "    # First convolution\n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(inputs)\n",
    "    if use_batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    # Second convolution\n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    if use_batch_norm:\n",
    "        x = BatchNormalization()(x)\n",
    "    \n",
    "    # If input channels don't match output channels, use 1x1 conv to match dimensions\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv2D(filters, (1, 1), padding=padding)(shortcut)\n",
    "        if use_batch_norm:\n",
    "            shortcut = BatchNormalization()(shortcut)\n",
    "    \n",
    "    # Add residual connection\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation(\"relu\")(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def UNetPlusPlus(input_shape=(224, 224, 3), num_classes=4, filters=[24, 48, 96, 192], use_batch_norm=True):\n",
    "    \"\"\"\n",
    "    Enhanced UNet++ with residual connections\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input image dimensions (height, width, channels)\n",
    "        num_classes: Number of output classes for segmentation\n",
    "        filters: List of filter dimensions for each level\n",
    "        use_batch_norm: Whether to use batch normalization\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder (Downsampling path)\n",
    "    conv0_0 = res_conv_block(inputs, filters[0], use_batch_norm=use_batch_norm)\n",
    "    pool0 = MaxPooling2D(pool_size=(2, 2))(conv0_0)\n",
    "    \n",
    "    conv1_0 = res_conv_block(pool0, filters[1], use_batch_norm=use_batch_norm)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1_0)\n",
    "    \n",
    "    conv2_0 = res_conv_block(pool1, filters[2], use_batch_norm=use_batch_norm)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2_0)\n",
    "    \n",
    "    conv3_0 = res_conv_block(pool2, filters[3], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    # Decoder (Upsampling path with nested dense skip connections)\n",
    "    # Level 1 skip connections\n",
    "    up1_0 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv3_0)\n",
    "    concat2_1 = Concatenate()([up1_0, conv2_0])\n",
    "    conv2_1 = res_conv_block(concat2_1, filters[2], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    up0_1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv2_0)\n",
    "    concat1_1 = Concatenate()([up0_1, conv1_0])\n",
    "    conv1_1 = res_conv_block(concat1_1, filters[1], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    up0_2 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv1_0)\n",
    "    concat0_1 = Concatenate()([up0_2, conv0_0])\n",
    "    conv0_1 = res_conv_block(concat0_1, filters[0], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    # Level 2 skip connections\n",
    "    up1_1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv2_1)\n",
    "    concat1_2 = Concatenate()([up1_1, conv1_0, conv1_1])\n",
    "    conv1_2 = res_conv_block(concat1_2, filters[1], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    up0_3 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv1_1)\n",
    "    concat0_2 = Concatenate()([up0_3, conv0_0, conv0_1])\n",
    "    conv0_2 = res_conv_block(concat0_2, filters[0], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    # Level 3 skip connections\n",
    "    up0_4 = UpSampling2D(size=(2, 2), interpolation='bilinear')(conv1_2)\n",
    "    concat0_3 = Concatenate()([up0_4, conv0_0, conv0_1, conv0_2])\n",
    "    conv0_3 = res_conv_block(concat0_3, filters[0], use_batch_norm=use_batch_norm)\n",
    "    \n",
    "    # Output segmentation map (single output)\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax')(conv0_3)\n",
    "    \n",
    "    # Create model with single output\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    return model\n",
    "\n",
    "student_model = UNetPlusPlus(input_shape=(224, 224, 3), num_classes=4)\n",
    "student_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "# === Config ===\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "CHANNELS = 3\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "\n",
    "# === Resize Layer ===\n",
    "class ResizeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_size, **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, self.target_size, method='bilinear')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'target_size': self.target_size})\n",
    "        return config\n",
    "\n",
    "\n",
    "# === Conv Block ===\n",
    "def conv_block(x, filters, kernel_size=3, strides=1):\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', strides=strides, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# === MBConv Block ===\n",
    "def mbconv_block(x, in_ch, out_ch, stride=1, expansion=4):\n",
    "    hidden_dim = in_ch * expansion\n",
    "    res = x\n",
    "    x = layers.Conv2D(hidden_dim, 1, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "\n",
    "    x = layers.DepthwiseConv2D(3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "\n",
    "    x = layers.Conv2D(out_ch, 1, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if stride == 1 and in_ch == out_ch:\n",
    "        x = layers.Add()([res, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "# === MLA Block ===\n",
    "def mla_block(x, channels, stride=1):\n",
    "    residual = x\n",
    "\n",
    "    if stride > 1:\n",
    "        residual = layers.AveragePooling2D(pool_size=stride, strides=stride, padding='same')(residual)\n",
    "        x = layers.AveragePooling2D(pool_size=stride, strides=stride, padding='same')(x)\n",
    "\n",
    "    if x.shape[-1] != channels:\n",
    "        residual = layers.Conv2D(channels, 1, padding='same')(residual)\n",
    "\n",
    "    attn = layers.DepthwiseConv2D(5, padding='same')(x)\n",
    "    attn = layers.Conv2D(channels, 1, padding='same')(attn)\n",
    "    attn = layers.ReLU()(attn)\n",
    "\n",
    "    return layers.Add()([residual, attn])\n",
    "\n",
    "\n",
    "# === EfficientViT Block ===\n",
    "def efficientvit_block(x, in_ch, out_ch, stride=1):\n",
    "    local = mbconv_block(x, in_ch, out_ch, stride=stride)\n",
    "    global_ = mla_block(x, out_ch, stride=stride)\n",
    "    return layers.Add()([local, global_])\n",
    "\n",
    "\n",
    "# EfficientViT-B0 Encoder\n",
    "def efficientvit_b0_encoder(inputs):\n",
    "    x = layers.Conv2D(16, 3, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "    e0 = x  # 112x112\n",
    "\n",
    "    x = efficientvit_block(x, 16, 32, stride=2)\n",
    "    e1 = x  # 56x56\n",
    "\n",
    "    x = efficientvit_block(x, 32, 64, stride=2)\n",
    "    e2 = x  # 28x28\n",
    "\n",
    "    x = efficientvit_block(x, 64, 96, stride=2)\n",
    "    e3 = x  # 14x14\n",
    "\n",
    "    x = efficientvit_block(x, 96, 128, stride=2)\n",
    "    e4 = x  # 7x7\n",
    "\n",
    "    return [e0, e1, e2, e3, e4]\n",
    "\n",
    "\n",
    "def attention_gate(x, g, inter_channels):\n",
    "    \"\"\"Standard attention gate block\"\"\"\n",
    "    x_shape = tf.keras.backend.int_shape(x)\n",
    "    g_shape = tf.keras.backend.int_shape(g)\n",
    "\n",
    "    if x_shape[1] != g_shape[1] or x_shape[2] != g_shape[2]:\n",
    "        g = ResizeLayer((x_shape[1], x_shape[2]))(g)\n",
    "\n",
    "    theta_x = layers.Conv2D(inter_channels, 1, padding='same')(x)\n",
    "    phi_g = layers.Conv2D(inter_channels, 1, padding='same')(g)\n",
    "    f = layers.Add()([theta_x, phi_g])\n",
    "    f = layers.Activation('relu')(f)\n",
    "    psi = layers.Conv2D(1, 1, padding='same')(f)\n",
    "    alpha = layers.Activation('sigmoid')(psi)\n",
    "    return layers.Multiply()([x, alpha])\n",
    "\n",
    "\n",
    "def build_attention_unet_with_efficientvit(input_shape=(224, 224, 3), num_classes=4):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # === EfficientViT Encoder ===\n",
    "    encoder_features = efficientvit_b0_encoder(inputs)\n",
    "    e0, e1, e2, e3, e4 = encoder_features  # e0: shallowest, e4: bottleneck\n",
    "\n",
    "    # === Decoder with Attention U-Net style ===\n",
    "    up_filters = [96, 64, 32, 16]\n",
    "    skip_connections = [e3, e2, e1, e0]  # deepest to shallowest\n",
    "    up = e4  # bottleneck\n",
    "\n",
    "    for i in range(4):\n",
    "        up = layers.Conv2DTranspose(up_filters[i], 3, strides=2, padding='same')(up)\n",
    "        up = layers.BatchNormalization()(up)\n",
    "        up = layers.Activation('relu')(up)\n",
    "        up = layers.Dropout(0.2)(up)\n",
    "\n",
    "        skip = skip_connections[i]\n",
    "        att_skip = attention_gate(skip, up, up_filters[i] // 2)\n",
    "\n",
    "        # Resize up to match skip if needed\n",
    "        if tf.keras.backend.int_shape(att_skip)[1:3] != tf.keras.backend.int_shape(up)[1:3]:\n",
    "            up = ResizeLayer((att_skip.shape[1], att_skip.shape[2]))(up)\n",
    "\n",
    "        up = layers.Concatenate()([up, att_skip])\n",
    "        up = conv_block(up, up_filters[i])\n",
    "        up = conv_block(up, up_filters[i])\n",
    "\n",
    "    # Final Upsampling to full resolution\n",
    "    up = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(up)\n",
    "    up = conv_block(up, 64)\n",
    "    up = conv_block(up, 32)\n",
    "\n",
    "    if tf.keras.backend.int_shape(up)[1:3] != (input_shape[0], input_shape[1]):\n",
    "        up = ResizeLayer((input_shape[0], input_shape[1]))(up)\n",
    "\n",
    "    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(up)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# === Build and Print Model ===\n",
    "tf.keras.backend.clear_session()\n",
    "student_model = build_attention_unet_with_efficientvit(\n",
    "    input_shape=(224, 224, 3),\n",
    "    num_classes=4\n",
    ")\n",
    "student_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def clone_metric(metric):\n",
    "    if isinstance(metric, DiceCoefficient):\n",
    "        return DiceCoefficient(class_idx=metric.class_idx)\n",
    "    else:\n",
    "        return type(metric)(**metric.get_config())\n",
    "\n",
    "class Distiller(tf.keras.Model):\n",
    "    def __init__(self, student, teacher, temperature=3.0, alpha=0.5, metrics=None):\n",
    "        super().__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.kl_loss_fn = tf.keras.losses.KLDivergence()\n",
    "        self.train_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"train_accuracy\")\n",
    "        self.val_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"val_accuracy\")\n",
    "        self.train_metrics = metrics or []\n",
    "        self.val_metrics = [clone_metric(m) for m in self.train_metrics]\n",
    "\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y_true = data\n",
    "        teacher_soft = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_logits = self.student(x, training=True)\n",
    "            student_soft = tf.nn.softmax(student_logits / self.temperature)\n",
    "            distill_loss = self.kl_loss_fn(teacher_soft, student_soft)\n",
    "            supervised_loss = combined_loss(y_true, tf.nn.softmax(student_logits))\n",
    "            loss = self.alpha * supervised_loss + (1 - self.alpha) * distill_loss\n",
    "\n",
    "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
    "\n",
    "        self.train_accuracy.update_state(y_true, tf.nn.softmax(student_logits))\n",
    "        for m in self.train_metrics:\n",
    "            m.update_state(y_true, tf.nn.softmax(student_logits))\n",
    "\n",
    "        logs = {\"loss\": loss, \"accuracy\": self.train_accuracy.result()}\n",
    "        for m in self.train_metrics:\n",
    "            logs[m.name] = m.result()\n",
    "        return logs\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y_true = data\n",
    "        y_pred = self.student(x, training=False)\n",
    "\n",
    "        self.val_accuracy.update_state(y_true, tf.nn.softmax(y_pred))\n",
    "        for m in self.val_metrics:\n",
    "            m.update_state(y_true, tf.nn.softmax(y_pred))\n",
    "\n",
    "        logs = {\"accuracy\": self.val_accuracy.result()}\n",
    "        for m in self.val_metrics:\n",
    "            logs[m.name] = m.result()\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',  # You can also use 'val_loss' if you log it manually\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',  # Or 'val_loss'\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='best_student_unetplusplus.keras',\n",
    "        monitor='val_loss',  # Or 'val_loss'\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "def create_train_generator(X, y, batch_size=16):\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    seed = 42\n",
    "    image_generator = image_datagen.flow(X, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_datagen.flow(y, batch_size=batch_size, seed=seed)\n",
    "    \n",
    "    while True:\n",
    "        X_batch = next(image_generator)\n",
    "        y_batch = next(mask_generator)\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "train_generator = create_train_generator(X_train, y_train, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "\n",
    "    dice_loss_val = 1 - (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss_val = tf.reduce_mean(dice_loss_val)\n",
    "\n",
    "    lovasz_loss_val = lovasz_softmax_loss(y_true, tf.nn.softmax(y_pred), ignore_background=False)\n",
    "    return lovasz_loss_val + dice_loss_val\n",
    "\n",
    "\n",
    "class HardVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, num_classes):\n",
    "        super(HardVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            logits = model(x, training=training)               # [B, H, W, C]\n",
    "            pred_mask = tf.argmax(logits, axis=-1)             # [B, H, W]\n",
    "            predictions.append(pred_mask)\n",
    "\n",
    "        stacked_preds = tf.stack(predictions, axis=0)          # [N_models, B, H, W]\n",
    "        stacked_preds = tf.transpose(stacked_preds, [1, 2, 3, 0])  # [B, H, W, N_models]\n",
    "\n",
    "        # Use numpy + scipy mode\n",
    "        def compute_mode(x):\n",
    "            mode, _ = stats.mode(x, axis=-1, keepdims=False)\n",
    "            return mode.astype(np.int32)\n",
    "\n",
    "        mode_preds = tf.numpy_function(\n",
    "            func=compute_mode,\n",
    "            inp=[stacked_preds],\n",
    "            Tout=tf.int32\n",
    "        )\n",
    "\n",
    "        # Manually set output shape: [B, H, W]\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        height = tf.shape(x)[1]\n",
    "        width = tf.shape(x)[2]\n",
    "        mode_preds.set_shape([None, None, None])  # Symbolic shape for [B, H, W]\n",
    "\n",
    "        one_hot_preds = tf.one_hot(mode_preds, depth=self.num_classes)  # [B, H, W, C]\n",
    "        return one_hot_preds\n",
    "\n",
    "hard_ensemble = HardVotingEnsemble(\n",
    "    models=[model_xception, model_segnet, model_inceptionresnetv2, model_efficientnetb4],\n",
    "    num_classes=4\n",
    ")\n",
    "\n",
    "hard_ensemble.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "# === Config ===\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "CHANNELS = 3\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "\n",
    "# === Resize Layer ===\n",
    "class ResizeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_size, **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, self.target_size, method='bilinear')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'target_size': self.target_size})\n",
    "        return config\n",
    "\n",
    "\n",
    "# === Conv Block ===\n",
    "def conv_block(x, filters, kernel_size=3, strides=1):\n",
    "    x = layers.Conv2D(filters, kernel_size, padding='same', strides=strides, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# === MBConv Block ===\n",
    "def mbconv_block(x, in_ch, out_ch, stride=1, expansion=4):\n",
    "    hidden_dim = in_ch * expansion\n",
    "    res = x\n",
    "    x = layers.Conv2D(hidden_dim, 1, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "\n",
    "    x = layers.DepthwiseConv2D(3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "\n",
    "    x = layers.Conv2D(out_ch, 1, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if stride == 1 and in_ch == out_ch:\n",
    "        x = layers.Add()([res, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "# === MLA Block ===\n",
    "def mla_block(x, channels, stride=1):\n",
    "    residual = x\n",
    "\n",
    "    if stride > 1:\n",
    "        residual = layers.AveragePooling2D(pool_size=stride, strides=stride, padding='same')(residual)\n",
    "        x = layers.AveragePooling2D(pool_size=stride, strides=stride, padding='same')(x)\n",
    "\n",
    "    if x.shape[-1] != channels:\n",
    "        residual = layers.Conv2D(channels, 1, padding='same')(residual)\n",
    "\n",
    "    attn = layers.DepthwiseConv2D(5, padding='same')(x)\n",
    "    attn = layers.Conv2D(channels, 1, padding='same')(attn)\n",
    "    attn = layers.ReLU()(attn)\n",
    "\n",
    "    return layers.Add()([residual, attn])\n",
    "\n",
    "\n",
    "# === EfficientViT Block ===\n",
    "def efficientvit_block(x, in_ch, out_ch, stride=1):\n",
    "    local = mbconv_block(x, in_ch, out_ch, stride=stride)\n",
    "    global_ = mla_block(x, out_ch, stride=stride)\n",
    "    return layers.Add()([local, global_])\n",
    "\n",
    "\n",
    "# EfficientViT-B0 Encoder\n",
    "def efficientvit_b0_encoder(inputs):\n",
    "    x = layers.Conv2D(16, 3, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('swish')(x)\n",
    "    e0 = x  # 112x112\n",
    "\n",
    "    x = efficientvit_block(x, 16, 32, stride=2)\n",
    "    e1 = x  # 56x56\n",
    "\n",
    "    x = efficientvit_block(x, 32, 64, stride=2)\n",
    "    e2 = x  # 28x28\n",
    "\n",
    "    x = efficientvit_block(x, 64, 96, stride=2)\n",
    "    e3 = x  # 14x14\n",
    "\n",
    "    x = efficientvit_block(x, 96, 128, stride=2)\n",
    "    e4 = x  # 7x7\n",
    "\n",
    "    return [e0, e1, e2, e3, e4]\n",
    "\n",
    "\n",
    "def attention_gate(x, g, inter_channels):\n",
    "    \"\"\"Standard attention gate block\"\"\"\n",
    "    x_shape = tf.keras.backend.int_shape(x)\n",
    "    g_shape = tf.keras.backend.int_shape(g)\n",
    "\n",
    "    if x_shape[1] != g_shape[1] or x_shape[2] != g_shape[2]:\n",
    "        g = ResizeLayer((x_shape[1], x_shape[2]))(g)\n",
    "\n",
    "    theta_x = layers.Conv2D(inter_channels, 1, padding='same')(x)\n",
    "    phi_g = layers.Conv2D(inter_channels, 1, padding='same')(g)\n",
    "    f = layers.Add()([theta_x, phi_g])\n",
    "    f = layers.Activation('relu')(f)\n",
    "    psi = layers.Conv2D(1, 1, padding='same')(f)\n",
    "    alpha = layers.Activation('sigmoid')(psi)\n",
    "    return layers.Multiply()([x, alpha])\n",
    "\n",
    "\n",
    "def build_attention_unet_with_efficientvit(input_shape=(224, 224, 3), num_classes=4):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # === EfficientViT Encoder ===\n",
    "    encoder_features = efficientvit_b0_encoder(inputs)\n",
    "    e0, e1, e2, e3, e4 = encoder_features  # e0: shallowest, e4: bottleneck\n",
    "\n",
    "    # === Decoder with Attention U-Net style ===\n",
    "    up_filters = [96, 64, 32, 16]\n",
    "    skip_connections = [e3, e2, e1, e0]  # deepest to shallowest\n",
    "    up = e4  # bottleneck\n",
    "\n",
    "    for i in range(4):\n",
    "        up = layers.Conv2DTranspose(up_filters[i], 3, strides=2, padding='same')(up)\n",
    "        up = layers.BatchNormalization()(up)\n",
    "        up = layers.Activation('relu')(up)\n",
    "        up = layers.Dropout(0.2)(up)\n",
    "\n",
    "        skip = skip_connections[i]\n",
    "        att_skip = attention_gate(skip, up, up_filters[i] // 2)\n",
    "\n",
    "        # Resize up to match skip if needed\n",
    "        if tf.keras.backend.int_shape(att_skip)[1:3] != tf.keras.backend.int_shape(up)[1:3]:\n",
    "            up = ResizeLayer((att_skip.shape[1], att_skip.shape[2]))(up)\n",
    "\n",
    "        up = layers.Concatenate()([up, att_skip])\n",
    "        up = conv_block(up, up_filters[i])\n",
    "        up = conv_block(up, up_filters[i])\n",
    "\n",
    "    # Final Upsampling to full resolution\n",
    "    up = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(up)\n",
    "    up = conv_block(up, 64)\n",
    "    up = conv_block(up, 32)\n",
    "\n",
    "    if tf.keras.backend.int_shape(up)[1:3] != (input_shape[0], input_shape[1]):\n",
    "        up = ResizeLayer((input_shape[0], input_shape[1]))(up)\n",
    "\n",
    "    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(up)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "student_model = build_attention_unet_with_efficientvit(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS),\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "student_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "models = [\n",
    "    model_xception,\n",
    "    model_segnet,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "class WeightedSoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights=None, apply_softmax=True):\n",
    "        super(WeightedSoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        else:\n",
    "            total = sum(weights)\n",
    "            weights = [w / total for w in weights]\n",
    "\n",
    "        self.model_weights = tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        weighted_sum = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            output = model(x, training=training)\n",
    "\n",
    "            is_softmaxed = (\n",
    "                hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "            )\n",
    "\n",
    "            if self.apply_softmax and not is_softmaxed:\n",
    "                probs = tf.nn.softmax(output, axis=-1)\n",
    "            else:\n",
    "                probs = output\n",
    "\n",
    "            weighted_sum += self.model_weights[i] * probs\n",
    "\n",
    "        avg_prob = weighted_sum  # shape: [B, H, W, C]\n",
    "\n",
    "        # üîÅ Convert to one-hot for metric compatibility\n",
    "        one_hot_pred = tf.one_hot(tf.argmax(avg_prob, axis=-1), depth=avg_prob.shape[-1])\n",
    "        return one_hot_pred  # [B, H, W, C]\n",
    "\n",
    "final_weights = [0.255, 0.2427, 0.2515, 0.2508]\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "teacher_model = ensemble_model \n",
    "\n",
    "def distillation_loss(y_true, y_student_logits, y_teacher_probs, alpha=0.5, temperature=3.0):\n",
    "    # Softened predictions for KL\n",
    "    student_soft = tf.nn.softmax(y_student_logits / temperature)\n",
    "    teacher_soft = tf.nn.softmax(y_teacher_probs / temperature)\n",
    "\n",
    "    # Soft loss: KL divergence\n",
    "    kl_loss = tf.keras.losses.KLDivergence()(teacher_soft, student_soft)\n",
    "\n",
    "    # Hard loss: Use your custom combined loss (Dice + Lovasz)\n",
    "    ce_loss = combined_loss(y_true, y_student_logits)\n",
    "\n",
    "    # Combine them\n",
    "    return alpha * ce_loss + (1 - alpha) * (temperature ** 2) * kl_loss\n",
    "\n",
    "# === KD Wrapper Model ===\n",
    "class KDTrainer(tf.keras.Model):\n",
    "    def __init__(self, student, teacher, alpha=0.5, temperature=3.0):\n",
    "        super(KDTrainer, self).__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compile(self, optimizer, metrics):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics_list = metrics\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y_true = data\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_logits = self.student(x, training=True)               # [B, H, W, C]\n",
    "            teacher_probs = self.teacher(x, training=False)               # Soft probs\n",
    "\n",
    "            loss = distillation_loss(\n",
    "                y_true, student_logits, teacher_probs,\n",
    "                alpha=self.alpha, temperature=self.temperature\n",
    "            )\n",
    "\n",
    "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
    "\n",
    "        for metric in self.metrics_list:\n",
    "            metric.update_state(y_true, student_logits)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics_list} | {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y_true = data\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = self.student(x, training=False)\n",
    "        loss = combined_loss(y_true, y_pred)\n",
    "\n",
    "        for metric in self.metrics_list:\n",
    "            metric.update_state(y_true, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics_list} | {\"loss\": loss}\n",
    "\n",
    "# === Instantiate KDTrainer ===\n",
    "kd_model = KDTrainer(\n",
    "    student=student_model,\n",
    "    teacher=teacher_model,\n",
    "    alpha=0.5,\n",
    "    temperature=3.0\n",
    ")\n",
    "\n",
    "# === Compile ===\n",
    "kd_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_path = f\"best_student_unetplusplus_{timestamp}\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    ), \n",
    "    ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    save_format='tf'  # ‚úÖ use TF SavedModel format\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_train_generator(X, y, batch_size=16):\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    seed = 42\n",
    "    image_generator = image_datagen.flow(X, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_datagen.flow(y, batch_size=batch_size, seed=seed)\n",
    "\n",
    "    while True:\n",
    "        X_batch = next(image_generator)\n",
    "        y_batch = next(mask_generator)\n",
    "        yield X_batch.astype('float32'), y_batch.astype('float32')\n",
    "\n",
    "batch_size = 16\n",
    "train_generator = create_train_generator(X_train, y_train, batch_size=batch_size)\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "\n",
    "history = kd_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# === Your ensemble setup ===\n",
    "models = [\n",
    "    model_xception,\n",
    "    model_segnet,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "class WeightedSoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights=None, apply_softmax=True):\n",
    "        super(WeightedSoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        else:\n",
    "            total = sum(weights)\n",
    "            weights = [w / total for w in weights]\n",
    "\n",
    "        self.model_weights = tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        weighted_sum = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            output = model(x, training=training)\n",
    "\n",
    "            is_softmaxed = (\n",
    "                hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "            )\n",
    "\n",
    "            if self.apply_softmax and not is_softmaxed:\n",
    "                probs = tf.nn.softmax(output, axis=-1)\n",
    "            else:\n",
    "                probs = output\n",
    "\n",
    "            weighted_sum += self.model_weights[i] * probs\n",
    "\n",
    "        avg_prob = weighted_sum\n",
    "        one_hot_pred = tf.one_hot(tf.argmax(avg_prob, axis=-1), depth=avg_prob.shape[-1])\n",
    "        return one_hot_pred  # ‚ö†Ô∏è returns one-hot\n",
    "\n",
    "final_weights = [0.255, 0.2427, 0.2515, 0.2508]\n",
    "ensemble_model = WeightedSoftVotingEnsemble(models=models, weights=final_weights, apply_softmax=True)\n",
    "\n",
    "# === SOFT OUTPUT EXTRACTOR (helper workaround) ===\n",
    "def get_teacher_soft_output(ensemble_model, x):\n",
    "    weighted_sum = 0\n",
    "    for i, model in enumerate(ensemble_model.models):\n",
    "        output = model(x, training=False)\n",
    "\n",
    "        is_softmaxed = (\n",
    "            hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "        )\n",
    "\n",
    "        if ensemble_model.apply_softmax and not is_softmaxed:\n",
    "            probs = tf.nn.softmax(output, axis=-1)\n",
    "        else:\n",
    "            probs = output\n",
    "\n",
    "        weighted_sum += ensemble_model.model_weights[i] * probs\n",
    "\n",
    "    return weighted_sum  # soft probabilities\n",
    "\n",
    "# === Distillation Loss ===\n",
    "def distillation_loss(y_true, y_student_logits, y_teacher_probs, alpha=0.2, temperature=5.0):\n",
    "    student_soft = tf.nn.softmax(y_student_logits / temperature)\n",
    "    teacher_soft = tf.nn.softmax(y_teacher_probs / temperature)\n",
    "\n",
    "    kl_loss = tf.keras.losses.KLDivergence()(teacher_soft, student_soft)\n",
    "\n",
    "    # Option 1: Combined loss on hard labels\n",
    "    ce_loss = combined_loss(y_true, tf.nn.softmax(y_student_logits))\n",
    "    \n",
    "    # Option 2: Standard CE (more stable for KD) ‚Äî you can switch if needed\n",
    "    # ce_loss = tf.keras.losses.CategoricalCrossentropy()(y_true, tf.nn.softmax(y_student_logits))\n",
    "\n",
    "    return alpha * ce_loss + (1 - alpha) * (temperature ** 2) * kl_loss\n",
    "\n",
    "# === KD Wrapper ===\n",
    "class KDTrainer(tf.keras.Model):\n",
    "    def __init__(self, student, teacher, alpha=0.2, temperature=5.0):\n",
    "        super(KDTrainer, self).__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compile(self, optimizer, metrics):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics_list = metrics\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y_true = data\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_logits = self.student(x, training=True)\n",
    "            teacher_probs = get_teacher_soft_output(self.teacher, x)  # ‚úÖ SOFT OUTPUT FIX\n",
    "\n",
    "            loss = distillation_loss(\n",
    "                y_true, student_logits, teacher_probs,\n",
    "                alpha=self.alpha, temperature=self.temperature\n",
    "            )\n",
    "\n",
    "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
    "\n",
    "        student_soft = tf.nn.softmax(student_logits)\n",
    "        for metric in self.metrics_list:\n",
    "            metric.update_state(y_true, student_soft)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics_list} | {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y_true = data\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        student_logits = self.student(x, training=False)\n",
    "        student_soft = tf.nn.softmax(student_logits)\n",
    "        loss = combined_loss(y_true, student_soft)\n",
    "\n",
    "        for metric in self.metrics_list:\n",
    "            metric.update_state(y_true, student_soft)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics_list} | {\"loss\": loss}\n",
    "\n",
    "# === Compile KD Model ===\n",
    "kd_model = KDTrainer(\n",
    "    student=student_model,\n",
    "    teacher=ensemble_model,\n",
    "    alpha=0.2,\n",
    "    temperature=5.0\n",
    ")\n",
    "\n",
    "kd_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "# === Callbacks ===\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_path = f\"best_student_unetplusplus_{timestamp}\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6),\n",
    "    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True,\n",
    "                    save_weights_only=True, save_format='tf')\n",
    "]\n",
    "\n",
    "# === Data Generator ===\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_train_generator(X, y, batch_size=16):\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    image_gen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_gen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    seed = 42\n",
    "    image_generator = image_gen.flow(X, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_gen.flow(y, batch_size=batch_size, seed=seed)\n",
    "\n",
    "    while True:\n",
    "        X_batch = next(image_generator)\n",
    "        y_batch = next(mask_generator)\n",
    "        yield X_batch.astype('float32'), y_batch.astype('float32')\n",
    "\n",
    "# === Train ===\n",
    "batch_size = 16\n",
    "train_generator = create_train_generator(X_train, y_train, batch_size=batch_size)\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "history = kd_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "models = [\n",
    "    model_xception,\n",
    "    model_segnet,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "class WeightedSoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights=None, apply_softmax=True):\n",
    "        super(WeightedSoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        else:\n",
    "            total = sum(weights)\n",
    "            weights = [w / total for w in weights]\n",
    "\n",
    "        self.model_weights = tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        weighted_sum = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            output = model(x, training=training)\n",
    "\n",
    "            is_softmaxed = (\n",
    "                hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "            )\n",
    "\n",
    "            if self.apply_softmax and not is_softmaxed:\n",
    "                probs = tf.nn.softmax(output, axis=-1)\n",
    "            else:\n",
    "                probs = output\n",
    "\n",
    "            weighted_sum += self.model_weights[i] * probs\n",
    "\n",
    "        avg_prob = weighted_sum  # shape: [B, H, W, C]\n",
    "\n",
    "        # üîÅ Convert to one-hot for metric compatibility\n",
    "        one_hot_pred = tf.one_hot(tf.argmax(avg_prob, axis=-1), depth=avg_prob.shape[-1])\n",
    "        return one_hot_pred  # [B, H, W, C]\n",
    "\n",
    "final_weights = [0.255, 0.2427, 0.2515, 0.2508]\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "teacher_model = ensemble_model \n",
    "\n",
    "def distillation_loss(y_true, y_student_logits, y_teacher_probs, alpha=0.5, temperature=3.0):\n",
    "    # Softened predictions for KL\n",
    "    student_soft = tf.nn.softmax(y_student_logits / temperature)\n",
    "    teacher_soft = tf.nn.softmax(y_teacher_probs / temperature)\n",
    "\n",
    "    # Soft loss: KL divergence\n",
    "    kl_loss = tf.keras.losses.KLDivergence()(teacher_soft, student_soft)\n",
    "\n",
    "    # Hard loss: Use your custom combined loss (Dice + Lovasz)\n",
    "    ce_loss = combined_loss(y_true, y_student_logits) + tf.keras.losses.CategoricalCrossentropy()(y_true, y_student_logits)\n",
    "\n",
    "    # Combine them\n",
    "    return alpha * ce_loss + (1 - alpha) * (temperature ** 2) * kl_loss\n",
    "\n",
    "# === KD Wrapper Model ===\n",
    "class KDTrainer(tf.keras.Model):\n",
    "    def __init__(self, student, teacher, alpha=0.5, temperature=3.0):\n",
    "        super(KDTrainer, self).__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compile(self, optimizer, metrics):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics_list = metrics\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y_true = data\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_logits = self.student(x, training=True)               # [B, H, W, C]\n",
    "            teacher_probs = self.teacher(x, training=False)               # Soft probs\n",
    "\n",
    "            loss = distillation_loss(\n",
    "                y_true, student_logits, teacher_probs,\n",
    "                alpha=self.alpha, temperature=self.temperature\n",
    "            )\n",
    "\n",
    "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
    "\n",
    "        for metric in self.metrics_list:\n",
    "            metric.update_state(y_true, student_logits)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics_list} | {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y_true = data\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = self.student(x, training=False)\n",
    "        loss = combined_loss(y_true, y_pred)\n",
    "\n",
    "        for metric in self.metrics_list:\n",
    "            metric.update_state(y_true, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics_list} | {\"loss\": loss}\n",
    "\n",
    "# === Instantiate KDTrainer ===\n",
    "kd_model = KDTrainer(\n",
    "    student=student_model,\n",
    "    teacher=teacher_model,\n",
    "    alpha=0.5,\n",
    "    temperature=3.0\n",
    ")\n",
    "\n",
    "# === Compile ===\n",
    "kd_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_path = f\"best_student_unetplusplus_{timestamp}\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    ), \n",
    "    ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    save_format='tf'  # ‚úÖ use TF SavedModel format\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_train_generator(X, y, batch_size=16):\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    seed = 42\n",
    "    image_generator = image_datagen.flow(X, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_datagen.flow(y, batch_size=batch_size, seed=seed)\n",
    "\n",
    "    while True:\n",
    "        X_batch = next(image_generator)\n",
    "        y_batch = next(mask_generator)\n",
    "        yield X_batch.astype('float32'), y_batch.astype('float32')\n",
    "\n",
    "batch_size = 16\n",
    "train_generator = create_train_generator(X_train, y_train, batch_size=batch_size)\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "\n",
    "# history = kd_model.fit(\n",
    "#     train_generator,\n",
    "#     steps_per_epoch=steps_per_epoch,\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=100,\n",
    "#     callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    temperature = trial.suggest_categorical(\"temperature\", [1, 3, 5, 10])\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"Nadam\", \"SGD\"])\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.1, 0.9, step=0.2)\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    if optimizer_name == \"Adam\":\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer_name == \"RMSprop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    elif optimizer_name == \"SGD\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    elif optimizer_name == \"Nadam\":\n",
    "        optimizer = tf.keras.optimizers.Nadam(learning_rate=lr)\n",
    "\n",
    "    \n",
    "    student_copy = tf.keras.models.clone_model(student_model)\n",
    "\n",
    "    kd_model = KDTrainer(\n",
    "        student=student_copy,\n",
    "        teacher=teacher_model,\n",
    "        alpha=alpha,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    kd_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        metrics=class_wise_metrics(num_classes=4)\n",
    "    )\n",
    "\n",
    "    train_gen = create_train_generator(X_train, y_train, batch_size=batch_size)\n",
    "    val_data = (X_val, y_val)\n",
    "\n",
    "    try:\n",
    "        history = kd_model.fit(\n",
    "            train_gen,\n",
    "            steps_per_epoch=len(X_train) // batch_size,\n",
    "            validation_data=val_data,\n",
    "            epochs=5,\n",
    "            verbose=0\n",
    "        )\n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        print(f\"OOM at batch_size={batch_size}\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return history.history['val_loss'][-1]\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=48)  # or 48 for full grid coverage\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "models = [\n",
    "    model_xception,\n",
    "    model_segnet,\n",
    "    model_inceptionresnetv2,\n",
    "    model_efficientnetb4\n",
    "]\n",
    "\n",
    "class WeightedSoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights=None, apply_softmax=True):\n",
    "        super(WeightedSoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        else:\n",
    "            total = sum(weights)\n",
    "            weights = [w / total for w in weights]\n",
    "\n",
    "        self.model_weights = tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        weighted_sum = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            output = model(x, training=training)\n",
    "\n",
    "            is_softmaxed = (\n",
    "                hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "            )\n",
    "\n",
    "            if self.apply_softmax and not is_softmaxed:\n",
    "                probs = tf.nn.softmax(output, axis=-1)\n",
    "            else:\n",
    "                probs = output\n",
    "\n",
    "            weighted_sum += self.model_weights[i] * probs\n",
    "\n",
    "        avg_prob = weighted_sum  # shape: [B, H, W, C]\n",
    "\n",
    "        # üîÅ Convert to one-hot for metric compatibility\n",
    "        one_hot_pred = tf.one_hot(tf.argmax(avg_prob, axis=-1), depth=avg_prob.shape[-1])\n",
    "        return one_hot_pred  # [B, H, W, C]\n",
    "\n",
    "final_weights = [0.255, 0.2427, 0.2515, 0.2508]\n",
    "ensemble_model = WeightedSoftVotingEnsemble(\n",
    "    models=models,\n",
    "    weights=final_weights,\n",
    "    apply_softmax=True\n",
    ")\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "teacher_model = ensemble_model \n",
    "\n",
    "def distillation_loss(y_true, y_student_logits, y_teacher_probs, alpha=0.5, temperature=3.0):\n",
    "    # Softened predictions for KL\n",
    "    student_soft = tf.nn.softmax(y_student_logits / temperature)\n",
    "    teacher_soft = tf.nn.softmax(y_teacher_probs / temperature)\n",
    "\n",
    "    # Soft loss: KL divergence\n",
    "    kl_loss = tf.keras.losses.KLDivergence()(teacher_soft, student_soft)\n",
    "\n",
    "    # Hard loss: Use your custom combined loss (Dice + Lovasz)\n",
    "    ce_loss = combined_loss(y_true, y_student_logits) + tf.keras.losses.CategoricalCrossentropy()(y_true, y_student_logits)\n",
    "\n",
    "    # Combine them\n",
    "    return alpha * ce_loss + (1 - alpha) * (temperature ** 2) * kl_loss\n",
    "\n",
    "# === KD Wrapper Model ===\n",
    "class KDTrainer(tf.keras.Model):\n",
    "    def __init__(self, student, teacher, alpha=0.5, temperature=3.0):\n",
    "        super(KDTrainer, self).__init__()\n",
    "        self.student = student\n",
    "        self.teacher = teacher\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compile(self, optimizer, metrics):\n",
    "        super().compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics_list = metrics\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y_true = data\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_logits = self.student(x, training=True)               # [B, H, W, C]\n",
    "            teacher_probs = self.teacher(x, training=False)               # Soft probs\n",
    "\n",
    "            loss = distillation_loss(\n",
    "                y_true, student_logits, teacher_probs,\n",
    "                alpha=self.alpha, temperature=self.temperature\n",
    "            )\n",
    "\n",
    "        grads = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.student.trainable_variables))\n",
    "\n",
    "        for metric in self.metrics_list:\n",
    "            metric.update_state(y_true, student_logits)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics_list} | {\"loss\": loss}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y_true = data\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = self.student(x, training=False)\n",
    "        loss = combined_loss(y_true, y_pred)\n",
    "\n",
    "        for metric in self.metrics_list:\n",
    "            metric.update_state(y_true, y_pred)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics_list} | {\"loss\": loss}\n",
    "\n",
    "# === Instantiate KDTrainer ===\n",
    "kd_model = KDTrainer(\n",
    "    student=student_model,\n",
    "    teacher=teacher_model,\n",
    "    alpha=0.5,\n",
    "    temperature=3.0\n",
    ")\n",
    "\n",
    "# === Compile ===\n",
    "kd_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "checkpoint_path = f\"best_student_unetplusplus_{timestamp}\"\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    ), \n",
    "    ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    save_format='tf'  # ‚úÖ use TF SavedModel format\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_train_generator(X, y, batch_size=16):\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    seed = 42\n",
    "    image_generator = image_datagen.flow(X, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_datagen.flow(y, batch_size=batch_size, seed=seed)\n",
    "\n",
    "    while True:\n",
    "        X_batch = next(image_generator)\n",
    "        y_batch = next(mask_generator)\n",
    "        yield X_batch.astype('float32'), y_batch.astype('float32')\n",
    "\n",
    "batch_size = 16\n",
    "train_generator = create_train_generator(X_train, y_train, batch_size=batch_size)\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "\n",
    "\n",
    "history = kd_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting data from the history object\n",
    "history_dict = history.history\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict['loss'], label='Training Loss')\n",
    "plt.plot(history_dict['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# If accuracy is available, plot training and validation accuracy\n",
    "if 'accuracy' in history_dict:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_dict['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history_dict['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ‚úÖ RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ Function to convert RGB masks to class index masks\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    # Create a mask initialized with zeros (for background class)\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "\n",
    "    # Loop through the RGB_TO_CLASS dictionary\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        # Identify the pixels with the current RGB value and assign them the class index\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Function to calculate Dice Similarity Coefficient (DSC)\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ‚úÖ Function to calculate IoU (Intersection over Union)\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# ‚úÖ Function to calculate Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')  # Return inf if no points for either true or pred class\n",
    "\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# ‚úÖ Function to calculate Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')  # Return inf if no points for either true or pred class\n",
    "\n",
    "    distances = []\n",
    "    for true_point in true_points:\n",
    "        distances.append(np.min(np.linalg.norm(pred_points - true_point, axis=1)))\n",
    "    return np.mean(distances)\n",
    "\n",
    "# ‚úÖ Function to evaluate the model on the test set class-wise\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16):\n",
    "    # Predict in batches\n",
    "    y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)  # Convert to class index prediction\n",
    "\n",
    "    # Convert y_test to class index format (since it's one-hot encoded)\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    # Initialize lists to store class-wise metrics\n",
    "    class_metrics = {i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []} for i in range(num_classes)}\n",
    "\n",
    "    # Calculate metrics for each test sample\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]  # one-hot -> class index\n",
    "        pred_mask = y_pred[i]\n",
    "\n",
    "        # For each class (0: Background, 1: Brain, 2: CSP, 3: LV)\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            # Dice Coefficient\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            # IoU\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            # Precision\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # Recall\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # F1 Score\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # Accuracy\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # # Hausdorff Distance\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # # Average Surface Distance\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # Print class-wise metrics in percentage\n",
    "    print(f\"{'Class':<10}{'Dice Coefficient (%)':<20}{'IoU (%)':<20}{'Precision (%)':<20}{'Recall (%)':<20}{'F1 Score (%)':<20}{'Accuracy (%)':<20}{'Hausdorff Distance':<20}{'Avg Surface Distance':<20}\")\n",
    "    print('-' * 180)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"Class {class_idx}:\")\n",
    "        print(f\"  Dice Coefficient: {np.mean(class_metrics[class_idx]['dice']) * 100:.2f}%\")\n",
    "        print(f\"  IoU: {np.mean(class_metrics[class_idx]['iou']) * 100:.2f}%\")\n",
    "        print(f\"  Precision: {np.mean(class_metrics[class_idx]['precision']) * 100:.2f}%\")\n",
    "        print(f\"  Recall: {np.mean(class_metrics[class_idx]['recall']) * 100:.2f}%\")\n",
    "        print(f\"  F1 Score: {np.mean(class_metrics[class_idx]['f1']) * 100:.2f}%\")\n",
    "        print(f\"  Accuracy: {np.mean(class_metrics[class_idx]['accuracy']) * 100:.2f}%\")\n",
    "        # print(f\"  Hausdorff Distance: {np.mean(class_metrics[class_idx]['hausdorff']):.4f}\")\n",
    "        # print(f\"  Average Surface Distance: {np.mean(class_metrics[class_idx]['asd']):.4f}\")\n",
    "        print(\"-\" * 180)\n",
    "\n",
    "    # Evaluate on test set to print overall test accuracy and loss\n",
    "    test_loss, *test_metrics = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    for metric, value in zip(model.metrics_names[1:], test_metrics):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# ‚úÖ Call the evaluation function on the test set class-wise\n",
    "evaluate_classwise_metrics(kd_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSoftVotingEnsemble(tf.keras.Model):\n",
    "    def __init__(self, models, weights=None, apply_softmax=True):\n",
    "        super(WeightedSoftVotingEnsemble, self).__init__()\n",
    "        self.models = models\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(models)] * len(models)\n",
    "        else:\n",
    "            total = sum(weights)\n",
    "            weights = [w / total for w in weights]\n",
    "\n",
    "        self.model_weights = tf.constant(weights, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        weighted_sum = 0\n",
    "        for i, model in enumerate(self.models):\n",
    "            output = model(x, training=training)\n",
    "\n",
    "            is_softmaxed = (\n",
    "                hasattr(model, \"name\") and \"efficientnet\" in model.name.lower()\n",
    "            )\n",
    "\n",
    "            if self.apply_softmax and not is_softmaxed:\n",
    "                probs = tf.nn.softmax(output, axis=-1)\n",
    "            else:\n",
    "                probs = output\n",
    "\n",
    "            weighted_sum += self.model_weights[i] * probs\n",
    "\n",
    "        avg_prob = weighted_sum\n",
    "        one_hot_pred = tf.one_hot(tf.argmax(avg_prob, axis=-1), depth=avg_prob.shape[-1])\n",
    "        return one_hot_pred \n",
    "\n",
    "final_weights = [0.255, 0.2427, 0.2515, 0.2508]\n",
    "ensemble_model = WeightedSoftVotingEnsemble(models=models, weights=final_weights, apply_softmax=True)\n",
    "\n",
    "ensemble_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")\n",
    "\n",
    "ensemble_model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"best_student_unetplusplus_distilled.weights.h5\"):\n",
    "    os.remove(\"best_student_unetplusplus_distilled.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.load_weights(\"student_model_weights_final.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_wise_metrics(num_classes=4):\n",
    "    return [DiceCoefficient(i) for i in range(num_classes)] + [tf.keras.metrics.MeanIoU(num_classes=num_classes)]\n",
    "\n",
    "def student_eval_loss(y_true, y_pred):\n",
    "    return [combined_loss(y_true, y_pred) + tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred)]\n",
    "\n",
    "student_model.compile(\n",
    "    optimizer= tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
    "    loss=student_eval_loss,\n",
    "    metrics=class_wise_metrics(num_classes=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.evaluate(X_test, y_test, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.evaluate(X_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model.evaluate(X_test, y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6804412,
     "sourceId": 10941215,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
