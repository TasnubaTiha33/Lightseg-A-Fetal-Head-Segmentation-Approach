{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T21:17:57.109707Z",
     "iopub.status.busy": "2025-03-23T21:17:57.109097Z",
     "iopub.status.idle": "2025-03-23T21:18:11.006510Z",
     "shell.execute_reply": "2025-03-23T21:18:11.005590Z",
     "shell.execute_reply.started": "2025-03-23T21:17:57.109673Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import re\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# âœ… Constants for 224x224\n",
    "IMG_HEIGHT = 224  # Ensure height is 224\n",
    "IMG_WIDTH = 224   # Ensure width is 224\n",
    "CHANNELS = 3  # RGB images\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "# âœ… Class mapping from RGB to class index\n",
    "CLASS_MAP = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0,  # Background\n",
    "}\n",
    "\n",
    "image_dir = r\"D:\\augmented_dataset\\images\"\n",
    "mask_dir = r\"D:\\augmented_dataset\\masks\"\n",
    "\n",
    "# âœ… Define destination directories\n",
    "train_image_dir = r\"D:\\Updated\\train\\images\"\n",
    "train_mask_dir = r\"D:\\Updated\\train\\masks\"\n",
    "val_image_dir = r\"D:\\Updated\\val\\images\"\n",
    "val_mask_dir = r\"D:\\Updated\\val\\masks\"\n",
    "test_image_dir = r\"D:\\Updated\\test\\images\"\n",
    "test_mask_dir = r\"D:\\Updated\\test\\masks\"\n",
    "\n",
    "# âœ… Fix sorting issue using natural sorting\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Sort filenames numerically instead of lexicographically.\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "# âœ… Convert RGB mask to class index mask\n",
    "def rgb_to_class(mask_array):\n",
    "    \"\"\"Convert RGB mask to single-channel class index mask.\"\"\"\n",
    "    height, width, _ = mask_array.shape\n",
    "    class_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for rgb, class_idx in CLASS_MAP.items():\n",
    "        matches = np.all(mask_array == rgb, axis=-1)  # Ensure exact match\n",
    "        class_mask[matches] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# âœ… Preprocess Filtered Dataset for 224x224\n",
    "def preprocess_filtered_dataset(image_dir, mask_dir):\n",
    "    \"\"\"Preprocess images & masks: normalize, resize, and convert masks to one-hot encoding.\"\"\"\n",
    "\n",
    "    # âœ… Load and sort filenames correctly\n",
    "    image_filenames = sorted(os.listdir(image_dir), key=natural_sort_key)\n",
    "    mask_filenames = sorted(os.listdir(mask_dir), key=natural_sort_key)\n",
    "\n",
    "    valid_image_paths = []\n",
    "    valid_mask_paths = []\n",
    "\n",
    "    # âœ… Ensure each image has a corresponding mask\n",
    "    for img_file, mask_file in zip(image_filenames, mask_filenames):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            valid_image_paths.append(img_path)\n",
    "            valid_mask_paths.append(mask_path)\n",
    "        else:\n",
    "            print(f\"âš ï¸ Skipping {img_file}: Missing image or mask\")\n",
    "\n",
    "    num_images = len(valid_image_paths)\n",
    "\n",
    "    # âœ… Initialize arrays\n",
    "    X = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=np.float32)\n",
    "    y = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES), dtype=np.float32)  # One-hot encoded masks\n",
    "\n",
    "    print(f\"ðŸš€ Processing {num_images} filtered images and masks...\")\n",
    "\n",
    "    for idx, (img_path, mask_path) in enumerate(zip(valid_image_paths, valid_mask_paths)):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"âœ… Processed {idx}/{num_images} images\")\n",
    "\n",
    "        # âœ… Load and Resize Image\n",
    "        img = cv2.imread(img_path)  # Read image in BGR format\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # Resize to (224,224)\n",
    "        img = img.astype(np.float32) / 255.0  # Normalize\n",
    "\n",
    "        # âœ… Load and Resize Mask\n",
    "        mask = cv2.imread(mask_path)  # Read mask in BGR format\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)  # Resize mask correctly\n",
    "\n",
    "        # âœ… Convert RGB mask to class mask\n",
    "        class_mask = rgb_to_class(mask)\n",
    "\n",
    "        # âœ… One-hot encode the class mask\n",
    "        one_hot_mask = to_categorical(class_mask, num_classes=NUM_CLASSES)\n",
    "\n",
    "        # âœ… Store preprocessed data\n",
    "        X[idx] = img\n",
    "        y[idx] = one_hot_mask\n",
    "\n",
    "        # âœ… Clear memory to prevent memory leaks\n",
    "        del img, mask, class_mask, one_hot_mask\n",
    "        gc.collect()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# âœ… Process dataset splits\n",
    "X_train, y_train = preprocess_filtered_dataset(train_image_dir, train_mask_dir)\n",
    "X_val, y_val = preprocess_filtered_dataset(val_image_dir, val_mask_dir)\n",
    "X_test, y_test = preprocess_filtered_dataset(test_image_dir, test_mask_dir)\n",
    "\n",
    "# âœ… Print dataset information\n",
    "print(\"\\nâœ… Dataset Splits:\")\n",
    "print(f\"  - Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"  - Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"  - Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T21:35:50.014268Z",
     "iopub.status.busy": "2025-03-23T21:35:50.013916Z",
     "iopub.status.idle": "2025-03-23T21:35:50.819228Z",
     "shell.execute_reply": "2025-03-23T21:35:50.818539Z",
     "shell.execute_reply.started": "2025-03-23T21:35:50.014242Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Augmentation configuration for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# Fit the augmentation parameters on the training data\n",
    "train_datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T21:36:07.380916Z",
     "iopub.status.busy": "2025-03-23T21:36:07.380627Z",
     "iopub.status.idle": "2025-03-23T21:36:29.796132Z",
     "shell.execute_reply": "2025-03-23T21:36:29.795257Z",
     "shell.execute_reply.started": "2025-03-23T21:36:07.380896Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Unique values in y_train:\", np.unique(np.argmax(y_train, axis=-1)))\n",
    "print(\"Unique values in y_val:\", np.unique(np.argmax(y_val, axis=-1)))\n",
    "print(\"Unique values in y_test:\", np.unique(np.argmax(y_test, axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T21:37:48.088683Z",
     "iopub.status.busy": "2025-03-23T21:37:48.088310Z",
     "iopub.status.idle": "2025-03-23T21:37:52.628170Z",
     "shell.execute_reply": "2025-03-23T21:37:52.627283Z",
     "shell.execute_reply.started": "2025-03-23T21:37:48.088658Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, Dense, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, MultiHeadAttention, Concatenate\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "class ResizeLayer(Layer):\n",
    "    \"\"\"Custom layer to resize images.\"\"\"\n",
    "    def __init__(self, target_size, **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, self.target_size, method='bilinear')\n",
    "\n",
    "def build_segformer_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Build a SegFormer model with MobileNetV3 as backbone for semantic segmentation.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input shape of the image (height, width, channels)\n",
    "        num_classes: Number of segmentation classes\n",
    "        \n",
    "    Returns:\n",
    "        A Keras Model instance\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Use MobileNetV3Large as the backbone\n",
    "    backbone = MobileNetV3Large(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Make the backbone trainable\n",
    "    backbone.trainable = True\n",
    "    \n",
    "    # Define extraction points for different resolution features\n",
    "    # We'll use specific layers based on the architecture to get multi-scale features\n",
    "    \n",
    "    # Create a new model to extract features at multiple scales\n",
    "    # We're using index-based access to make it more robust across different environments\n",
    "    \n",
    "    # Extract features at different resolutions\n",
    "    # These indices are chosen based on common network architecture patterns\n",
    "    # but may need to be adjusted based on your specific backbone\n",
    "    \n",
    "    # Print info for debugging\n",
    "    print(\"Building SegFormer with MobileNetV3 backbone\")\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    \n",
    "    # First, connect the input to the backbone\n",
    "    x = backbone(inputs)\n",
    "    \n",
    "    # Now extract features at different resolutions\n",
    "    # We'll create a separate feature extraction model using functional API\n",
    "    \n",
    "    # Select appropriate layers for multi-resolution feature extraction\n",
    "    # Using approximate positions in the network for features at different scales\n",
    "    \n",
    "    # Early layer for high-resolution features (1/4)\n",
    "    high_res_layer_idx = len(backbone.layers) // 8\n",
    "    # Mid layer for medium resolution (1/8)\n",
    "    mid_res_layer_idx = len(backbone.layers) // 4\n",
    "    # Deep layer for low resolution (1/16)\n",
    "    low_res_layer_idx = len(backbone.layers) // 2\n",
    "    # Final layer for lowest resolution (1/32)\n",
    "    final_layer_idx = -1\n",
    "    \n",
    "    print(f\"Using layers at indices: {high_res_layer_idx}, {mid_res_layer_idx}, {low_res_layer_idx}, {final_layer_idx}\")\n",
    "    \n",
    "    # Create the feature extraction model\n",
    "    feature_model = Model(\n",
    "        inputs=backbone.input,\n",
    "        outputs=[\n",
    "            backbone.layers[high_res_layer_idx].output,\n",
    "            backbone.layers[mid_res_layer_idx].output,\n",
    "            backbone.layers[low_res_layer_idx].output,\n",
    "            backbone.layers[final_layer_idx].output\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract features\n",
    "    features = feature_model(inputs)\n",
    "    \n",
    "    # Get the features from each resolution level\n",
    "    feature_map_high_res = features[0]    # ~1/4 resolution\n",
    "    feature_map_mid_res = features[1]     # ~1/8 resolution\n",
    "    feature_map_low_res = features[2]     # ~1/16 resolution\n",
    "    feature_map_lowest_res = features[3]  # ~1/32 resolution\n",
    "    \n",
    "    # Calculate expected feature dimensions\n",
    "    f_high_h, f_high_w = input_shape[0] // 4, input_shape[1] // 4\n",
    "    \n",
    "    # Print feature map shapes for debugging\n",
    "    print(f\"High res feature shape: {feature_map_high_res.shape}\")\n",
    "    print(f\"Mid res feature shape: {feature_map_mid_res.shape}\")\n",
    "    print(f\"Low res feature shape: {feature_map_low_res.shape}\")\n",
    "    print(f\"Lowest res feature shape: {feature_map_lowest_res.shape}\")\n",
    "    \n",
    "    # Adjust channel dimensions with 1x1 convolutions\n",
    "    c1 = Conv2D(64, 1, padding='same')(feature_map_high_res)\n",
    "    c2 = Conv2D(128, 1, padding='same')(feature_map_mid_res)\n",
    "    c3 = Conv2D(192, 1, padding='same')(feature_map_low_res)\n",
    "    c4 = Conv2D(256, 1, padding='same')(feature_map_lowest_res)\n",
    "    \n",
    "    # Upscale all feature maps to the highest resolution using Keras-compatible resize layer\n",
    "    n4 = Conv2D(256, 1, padding='same')(c4)\n",
    "    n4 = ResizeLayer(target_size=(f_high_h, f_high_w))(n4)\n",
    "    \n",
    "    n3 = Conv2D(256, 1, padding='same')(c3)\n",
    "    n3 = ResizeLayer(target_size=(f_high_h, f_high_w))(n3)\n",
    "    \n",
    "    n2 = Conv2D(256, 1, padding='same')(c2)\n",
    "    n2 = ResizeLayer(target_size=(f_high_h, f_high_w))(n2)\n",
    "    \n",
    "    n1 = Conv2D(256, 1, padding='same')(c1)\n",
    "    \n",
    "    # Concatenate all feature maps\n",
    "    fused = Concatenate(axis=-1)([n1, n2, n3, n4])\n",
    "    \n",
    "    # Apply the decoder MLP\n",
    "    fused = Conv2D(512, 3, padding='same', activation='relu')(fused)\n",
    "    fused = Dropout(0.1)(fused)\n",
    "    fused = Conv2D(256, 1, padding='same', activation='relu')(fused)\n",
    "    \n",
    "    # Upscale to the original image resolution\n",
    "    fused = ResizeLayer(target_size=(input_shape[0], input_shape[1]))(fused)\n",
    "    \n",
    "    # Final segmentation layer\n",
    "    outputs = Conv2D(num_classes, 1, padding='same', activation='softmax')(fused)\n",
    "    \n",
    "    # Create and return the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the SegFormer model\n",
    "model = build_segformer_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), num_classes=NUM_CLASSES)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Compilation can be done separately\n",
    "# model.compile(\n",
    "#     optimizer=Adam(learning_rate=1e-4),\n",
    "#     loss=CategoricalCrossentropy(),\n",
    "#     metrics=['accuracy']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-23T21:14:47.594Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "def calculate_class_weights(y):\n",
    "    \"\"\"\n",
    "    Compute class weights based on the frequency of each class in the dataset.\n",
    "    Works with one-hot encoded masks.\n",
    "    \"\"\"\n",
    "    # âœ… Convert one-hot encoded masks back to class indices\n",
    "    y_flat = np.argmax(y, axis=-1).flatten()\n",
    "\n",
    "    # âœ… Get unique class labels present in y_train\n",
    "    unique_classes = np.unique(y_flat)\n",
    "\n",
    "    # âœ… Compute class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=unique_classes,\n",
    "        y=y_flat\n",
    "    )\n",
    "\n",
    "    # âœ… Create dictionary mapping class index to weight\n",
    "    class_weight_dict = {i: class_weights[i] for i in unique_classes}\n",
    "    return class_weight_dict\n",
    "\n",
    "# âœ… Compute class weights using filtered dataset\n",
    "class_weights = calculate_class_weights(y_train)\n",
    "\n",
    "# âœ… Print class weights\n",
    "print(\"\\nðŸ”¹ Class Weights:\")\n",
    "for class_idx, weight in class_weights.items():\n",
    "    print(f\"  - Class {class_idx}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T21:37:58.312883Z",
     "iopub.status.busy": "2025-03-23T21:37:58.312535Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# âœ… Dice Coefficient Metric\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "# âœ… Weighted Categorical Crossentropy\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    class_weights = tf.constant([0.3776, 0.7605, 65.8554, 46.2381], dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "    class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))\n",
    "    weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# âœ… Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# âœ… Combined Loss Function\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return weighted_categorical_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "# âœ… Custom Dice Coefficient Metric for Each Class\n",
    "class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "    def __init__(self, class_idx, name=None, **kwargs):  \n",
    "        if name is None:\n",
    "            name = f\"DiceClass{class_idx}\"  \n",
    "        super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.class_idx = class_idx\n",
    "        self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_class = y_true[..., self.class_idx]\n",
    "        y_pred_class = y_pred[..., self.class_idx]\n",
    "        intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "        union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "        dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "        self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "    def result(self):\n",
    "        return self.dice\n",
    "\n",
    "# âœ… Function to Get Class-wise Metrics\n",
    "def class_wise_metrics(num_classes=4):\n",
    "    return [DiceCoefficient(i) for i in range(num_classes)] + [tf.keras.metrics.MeanIoU(num_classes=num_classes)]\n",
    "\n",
    "# âœ… Create Data Generator\n",
    "def create_train_generator(X, y, batch_size=16):\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    seed = 42\n",
    "    image_generator = image_datagen.flow(X, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_datagen.flow(y, batch_size=batch_size, seed=seed)\n",
    "    \n",
    "    while True:\n",
    "        X_batch = next(image_generator)\n",
    "        y_batch = next(mask_generator)\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# âœ… Create the generator\n",
    "train_generator = create_train_generator(X_train, y_train, batch_size=16)\n",
    "\n",
    "# âœ… Compile the Model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(4)\n",
    ")\n",
    "\n",
    "# âœ… Train the Model with the Data Generator\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // 16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=7, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=3, \n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'best_segformer_model_mobilev3.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-23T21:14:47.594Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting data from the history object\n",
    "history_dict = history.history\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict['loss'], label='Training Loss')\n",
    "plt.plot(history_dict['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# If accuracy is available, plot training and validation accuracy\n",
    "if 'accuracy' in history_dict:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_dict['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history_dict['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-23T21:14:47.594Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# âœ… RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# âœ… Function to convert RGB masks to class index masks\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    # Create a mask initialized with zeros (for background class)\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "\n",
    "    # Loop through the RGB_TO_CLASS dictionary\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        # Identify the pixels with the current RGB value and assign them the class index\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# âœ… Function to calculate Dice Similarity Coefficient (DSC)\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# âœ… Function to calculate IoU (Intersection over Union)\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# âœ… Function to calculate Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')  # Return inf if no points for either true or pred class\n",
    "\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# âœ… Function to calculate Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')  # Return inf if no points for either true or pred class\n",
    "\n",
    "    distances = []\n",
    "    for true_point in true_points:\n",
    "        distances.append(np.min(np.linalg.norm(pred_points - true_point, axis=1)))\n",
    "    return np.mean(distances)\n",
    "\n",
    "# âœ… Function to evaluate the model on the test set class-wise\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16):\n",
    "    # Predict in batches\n",
    "    y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)  # Convert to class index prediction\n",
    "\n",
    "    # Convert y_test to class index format (since it's one-hot encoded)\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    # Initialize lists to store class-wise metrics\n",
    "    class_metrics = {i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []} for i in range(num_classes)}\n",
    "\n",
    "    # Calculate metrics for each test sample\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]  # one-hot -> class index\n",
    "        pred_mask = y_pred[i]\n",
    "\n",
    "        # For each class (0: Background, 1: Brain, 2: CSP, 3: LV)\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            # Dice Coefficient\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            # IoU\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            # Precision\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # Recall\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # F1 Score\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # Accuracy\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # # Hausdorff Distance\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # # Average Surface Distance\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # Print class-wise metrics in percentage\n",
    "    print(f\"{'Class':<10}{'Dice Coefficient (%)':<20}{'IoU (%)':<20}{'Precision (%)':<20}{'Recall (%)':<20}{'F1 Score (%)':<20}{'Accuracy (%)':<20}{'Hausdorff Distance':<20}{'Avg Surface Distance':<20}\")\n",
    "    print('-' * 180)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"Class {class_idx}:\")\n",
    "        print(f\"  Dice Coefficient: {np.mean(class_metrics[class_idx]['dice']) * 100:.2f}%\")\n",
    "        print(f\"  IoU: {np.mean(class_metrics[class_idx]['iou']) * 100:.2f}%\")\n",
    "        print(f\"  Precision: {np.mean(class_metrics[class_idx]['precision']) * 100:.2f}%\")\n",
    "        print(f\"  Recall: {np.mean(class_metrics[class_idx]['recall']) * 100:.2f}%\")\n",
    "        print(f\"  F1 Score: {np.mean(class_metrics[class_idx]['f1']) * 100:.2f}%\")\n",
    "        print(f\"  Accuracy: {np.mean(class_metrics[class_idx]['accuracy']) * 100:.2f}%\")\n",
    "        print(f\"  Hausdorff Distance: {np.mean(class_metrics[class_idx]['hausdorff']):.4f}\")\n",
    "        print(f\"  Average Surface Distance: {np.mean(class_metrics[class_idx]['asd']):.4f}\")\n",
    "        print(\"-\" * 180)\n",
    "\n",
    "    # Evaluate on test set to print overall test accuracy and loss\n",
    "    test_loss, *test_metrics = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    for metric, value in zip(model.metrics_names[1:], test_metrics):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# âœ… Call the evaluation function on the test set class-wise\n",
    "evaluate_classwise_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6940010,
     "sourceId": 11127979,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
