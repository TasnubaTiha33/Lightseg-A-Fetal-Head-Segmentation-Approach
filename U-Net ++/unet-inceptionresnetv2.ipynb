{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T01:20:55.711978Z",
     "iopub.status.busy": "2025-04-07T01:20:55.711978Z",
     "iopub.status.idle": "2025-04-07T01:28:25.542864Z",
     "shell.execute_reply": "2025-04-07T01:28:25.542864Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import re\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# âœ… Constants for 224x224\n",
    "IMG_HEIGHT = 224  # Ensure height is 224\n",
    "IMG_WIDTH = 224   # Ensure width is 224\n",
    "CHANNELS = 3  # RGB images\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "# âœ… Class mapping from RGB to class index\n",
    "CLASS_MAP = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0,  # Background\n",
    "}\n",
    "\n",
    "image_dir = r\"D:\\augmented_dataset\\images\"\n",
    "mask_dir = r\"D:\\augmented_dataset\\masks\"\n",
    "\n",
    "# âœ… Define destination directories\n",
    "train_image_dir = r\"D:\\Updated\\train\\images\"\n",
    "train_mask_dir = r\"D:\\Updated\\train\\masks\"\n",
    "val_image_dir = r\"D:\\Updated\\val\\images\"\n",
    "val_mask_dir = r\"D:\\Updated\\val\\masks\"\n",
    "test_image_dir = r\"D:\\Updated\\test\\images\"\n",
    "test_mask_dir = r\"D:\\Updated\\test\\masks\"\n",
    "\n",
    "# âœ… Fix sorting issue using natural sorting\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Sort filenames numerically instead of lexicographically.\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "# âœ… Convert RGB mask to class index mask\n",
    "def rgb_to_class(mask_array):\n",
    "    \"\"\"Convert RGB mask to single-channel class index mask.\"\"\"\n",
    "    height, width, _ = mask_array.shape\n",
    "    class_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for rgb, class_idx in CLASS_MAP.items():\n",
    "        matches = np.all(mask_array == rgb, axis=-1)  # Ensure exact match\n",
    "        class_mask[matches] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# âœ… Preprocess Filtered Dataset for 224x224\n",
    "def preprocess_filtered_dataset(image_dir, mask_dir):\n",
    "    \"\"\"Preprocess images & masks: normalize, resize, and convert masks to one-hot encoding.\"\"\"\n",
    "\n",
    "    # âœ… Load and sort filenames correctly\n",
    "    image_filenames = sorted(os.listdir(image_dir), key=natural_sort_key)\n",
    "    mask_filenames = sorted(os.listdir(mask_dir), key=natural_sort_key)\n",
    "\n",
    "    valid_image_paths = []\n",
    "    valid_mask_paths = []\n",
    "\n",
    "    # âœ… Ensure each image has a corresponding mask\n",
    "    for img_file, mask_file in zip(image_filenames, mask_filenames):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            valid_image_paths.append(img_path)\n",
    "            valid_mask_paths.append(mask_path)\n",
    "        else:\n",
    "            print(f\"âš ï¸ Skipping {img_file}: Missing image or mask\")\n",
    "\n",
    "    num_images = len(valid_image_paths)\n",
    "\n",
    "    # âœ… Initialize arrays\n",
    "    X = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=np.float32)\n",
    "    y = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES), dtype=np.float32)  # One-hot encoded masks\n",
    "\n",
    "    print(f\"ðŸš€ Processing {num_images} filtered images and masks...\")\n",
    "\n",
    "    for idx, (img_path, mask_path) in enumerate(zip(valid_image_paths, valid_mask_paths)):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"âœ… Processed {idx}/{num_images} images\")\n",
    "\n",
    "        # âœ… Load and Resize Image\n",
    "        img = cv2.imread(img_path)  # Read image in BGR format\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # Resize to (224,224)\n",
    "        img = img.astype(np.float32) / 255.0  # Normalize\n",
    "\n",
    "        # âœ… Load and Resize Mask\n",
    "        mask = cv2.imread(mask_path)  # Read mask in BGR format\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)  # Resize mask correctly\n",
    "\n",
    "        # âœ… Convert RGB mask to class mask\n",
    "        class_mask = rgb_to_class(mask)\n",
    "\n",
    "        # âœ… One-hot encode the class mask\n",
    "        one_hot_mask = to_categorical(class_mask, num_classes=NUM_CLASSES)\n",
    "\n",
    "        # âœ… Store preprocessed data\n",
    "        X[idx] = img\n",
    "        y[idx] = one_hot_mask\n",
    "\n",
    "        # âœ… Clear memory to prevent memory leaks\n",
    "        del img, mask, class_mask, one_hot_mask\n",
    "        gc.collect()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# âœ… Process dataset splits\n",
    "X_train, y_train = preprocess_filtered_dataset(train_image_dir, train_mask_dir)\n",
    "X_val, y_val = preprocess_filtered_dataset(val_image_dir, val_mask_dir)\n",
    "X_test, y_test = preprocess_filtered_dataset(test_image_dir, test_mask_dir)\n",
    "\n",
    "# âœ… Print dataset information\n",
    "print(\"\\nâœ… Dataset Splits:\")\n",
    "print(f\"  - Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"  - Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"  - Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:28:25.544864Z",
     "iopub.status.busy": "2025-04-07T01:28:25.544864Z",
     "iopub.status.idle": "2025-04-07T01:28:25.982962Z",
     "shell.execute_reply": "2025-04-07T01:28:25.982962Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Augmentation configuration for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# Fit the augmentation parameters on the training data\n",
    "train_datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:28:25.984962Z",
     "iopub.status.busy": "2025-04-07T01:28:25.984962Z",
     "iopub.status.idle": "2025-04-07T01:28:31.908291Z",
     "shell.execute_reply": "2025-04-07T01:28:31.908291Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Unique values in y_train:\", np.unique(np.argmax(y_train, axis=-1)))\n",
    "print(\"Unique values in y_val:\", np.unique(np.argmax(y_val, axis=-1)))\n",
    "print(\"Unique values in y_test:\", np.unique(np.argmax(y_test, axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:28:31.911292Z",
     "iopub.status.busy": "2025-04-07T01:28:31.910292Z",
     "iopub.status.idle": "2025-04-07T01:28:37.261193Z",
     "shell.execute_reply": "2025-04-07T01:28:37.261193Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, Concatenate, BatchNormalization, Activation\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "import gc\n",
    "\n",
    "# Constants\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "CHANNELS = 3\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "class ResizeLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom layer to resize images.\"\"\"\n",
    "    def __init__(self, target_size, **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, self.target_size, method='bilinear')\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(ResizeLayer, self).get_config()\n",
    "        config.update({\"target_size\": self.target_size})\n",
    "        return config\n",
    "\n",
    "def conv_block(x, filters, kernel_size=3, padding='same', activation='relu'):\n",
    "    \"\"\"Helper function for creating a conv block with BN and activation.\"\"\"\n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    # Add a second conv to increase parameters\n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def build_full_inceptionresnetv2_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Build a full UNet model with InceptionResNetV2 backbone with 60-70M parameters\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input shape of the image\n",
    "        num_classes: Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        Keras Model instance with UNet architecture\n",
    "    \"\"\"\n",
    "    # Input layer (no fixed batch size)\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Create a full InceptionResNetV2 model to use as backbone\n",
    "    base_model = InceptionResNetV2(\n",
    "        input_tensor=inputs,\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Make all layers trainable as requested\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Extract features from all encoder levels\n",
    "    # Standard blocks in InceptionResNetV2\n",
    "    encoder1 = base_model.get_layer('activation').output  # 111x111x64\n",
    "    encoder2 = base_model.get_layer('activation_3').output  # 55x55x192\n",
    "    encoder3 = base_model.get_layer('block35_10_ac').output  # 27x27x320\n",
    "    encoder4 = base_model.get_layer('block17_20_ac').output  # 13x13x1088\n",
    "    encoder5 = base_model.get_layer('conv_7b_ac').output  # 6x6x2080\n",
    "    \n",
    "    # Use the bottleneck as is - don't reduce its channels\n",
    "    bottleneck = encoder5  # 6x6x2080\n",
    "    \n",
    "    # First, reduce the bottleneck dimensions to control parameter count\n",
    "    bottleneck = Conv2D(512, 1, padding='same')(bottleneck)\n",
    "    bottleneck = BatchNormalization()(bottleneck)\n",
    "    bottleneck = Activation('relu')(bottleneck)\n",
    "    \n",
    "    # Level 5 to 4: 6x6 -> 13x13\n",
    "    up4 = UpSampling2D(size=(2, 2))(bottleneck)\n",
    "    up4 = ResizeLayer(target_size=(encoder4.shape[1], encoder4.shape[2]))(up4)\n",
    "    up4 = conv_block(up4, 512, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels before concatenation\n",
    "    skip4 = Conv2D(256, 1, padding='same')(encoder4)\n",
    "    skip4 = BatchNormalization()(skip4)\n",
    "    skip4 = Activation('relu')(skip4)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge4 = Concatenate()([up4, skip4])\n",
    "    merge4 = conv_block(merge4, 384)  # Reduced filters\n",
    "    \n",
    "    # Level 4 to 3: 13x13 -> 27x27\n",
    "    up3 = UpSampling2D(size=(2, 2))(merge4)\n",
    "    up3 = ResizeLayer(target_size=(encoder3.shape[1], encoder3.shape[2]))(up3)\n",
    "    up3 = conv_block(up3, 384, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels\n",
    "    skip3 = Conv2D(128, 1, padding='same')(encoder3)\n",
    "    skip3 = BatchNormalization()(skip3)\n",
    "    skip3 = Activation('relu')(skip3)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge3 = Concatenate()([up3, skip3])\n",
    "    merge3 = conv_block(merge3, 192)  # Reduced filters\n",
    "    \n",
    "    # Level 3 to 2: 27x27 -> 55x55\n",
    "    up2 = UpSampling2D(size=(2, 2))(merge3)\n",
    "    up2 = ResizeLayer(target_size=(encoder2.shape[1], encoder2.shape[2]))(up2)\n",
    "    up2 = conv_block(up2, 192, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels\n",
    "    skip2 = Conv2D(96, 1, padding='same')(encoder2)\n",
    "    skip2 = BatchNormalization()(skip2)\n",
    "    skip2 = Activation('relu')(skip2)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge2 = Concatenate()([up2, skip2])\n",
    "    merge2 = conv_block(merge2, 96)  # Reduced filters\n",
    "    \n",
    "    # Level 2 to 1: 55x55 -> 111x111\n",
    "    up1 = UpSampling2D(size=(2, 2))(merge2)\n",
    "    up1 = ResizeLayer(target_size=(encoder1.shape[1], encoder1.shape[2]))(up1)\n",
    "    up1 = conv_block(up1, 96, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels\n",
    "    skip1 = Conv2D(48, 1, padding='same')(encoder1)\n",
    "    skip1 = BatchNormalization()(skip1)\n",
    "    skip1 = Activation('relu')(skip1)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge1 = Concatenate()([up1, skip1])\n",
    "    merge1 = conv_block(merge1, 48)  # Reduced filters\n",
    "    \n",
    "    # Final upsampling to original resolution: 111x111 -> 224x224\n",
    "    up_final = UpSampling2D(size=(2, 2))(merge1)\n",
    "    up_final = conv_block(up_final, 32)  # Reduced filters\n",
    "    \n",
    "    # Ensure final size matches input\n",
    "    if up_final.shape[1] != input_shape[0] or up_final.shape[2] != input_shape[1]:\n",
    "        up_final = ResizeLayer(target_size=(input_shape[0], input_shape[1]))(up_final)\n",
    "    \n",
    "    # Add a final segmentation head\n",
    "    outputs = Conv2D(num_classes, 1, activation='softmax', dtype='float32')(up_final)\n",
    "    \n",
    "    # Create and return the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "print(\"Creating full InceptionResNetV2-UNet model...\")\n",
    "model = build_full_inceptionresnetv2_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), num_classes=NUM_CLASSES)\n",
    "print(\"Model created successfully!\")\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:28:37.438233Z",
     "iopub.status.busy": "2025-04-07T01:28:37.437233Z",
     "iopub.status.idle": "2025-04-07T01:28:41.917237Z",
     "shell.execute_reply": "2025-04-07T01:28:41.917237Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, Concatenate, BatchNormalization, Activation\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "import gc\n",
    "\n",
    "# Constants\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "CHANNELS = 3\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "class ResizeLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom layer to resize images.\"\"\"\n",
    "    def __init__(self, target_size, **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, self.target_size, method='bilinear')\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(ResizeLayer, self).get_config()\n",
    "        config.update({\"target_size\": self.target_size})\n",
    "        return config\n",
    "\n",
    "def conv_block(x, filters, kernel_size=3, padding='same', activation='relu'):\n",
    "    \"\"\"Helper function for creating a conv block with BN and activation.\"\"\"\n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    # Add a second conv to increase parameters\n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def build_full_inceptionresnetv2_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Build a full UNet model with InceptionResNetV2 backbone with 60-70M parameters\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input shape of the image\n",
    "        num_classes: Number of output classes\n",
    "        \n",
    "    Returns:\n",
    "        Keras Model instance with UNet architecture\n",
    "    \"\"\"\n",
    "    # Input layer (no fixed batch size)\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Create a full InceptionResNetV2 model to use as backbone\n",
    "    base_model = InceptionResNetV2(\n",
    "        input_tensor=inputs,\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling=None\n",
    "    )\n",
    "    \n",
    "    # Make all layers trainable as requested\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # Extract features from all encoder levels\n",
    "    # Standard blocks in InceptionResNetV2\n",
    "    encoder1 = base_model.get_layer('activation').output  # 111x111x64\n",
    "    encoder2 = base_model.get_layer('activation_3').output  # 55x55x192\n",
    "    encoder3 = base_model.get_layer('block35_10_ac').output  # 27x27x320\n",
    "    encoder4 = base_model.get_layer('block17_20_ac').output  # 13x13x1088\n",
    "    encoder5 = base_model.get_layer('conv_7b_ac').output  # 6x6x2080\n",
    "    \n",
    "    # Use the bottleneck as is - don't reduce its channels\n",
    "    bottleneck = encoder5  # 6x6x2080\n",
    "    \n",
    "    # First, reduce the bottleneck dimensions to control parameter count\n",
    "    bottleneck = Conv2D(512, 1, padding='same')(bottleneck)\n",
    "    bottleneck = BatchNormalization()(bottleneck)\n",
    "    bottleneck = Activation('relu')(bottleneck)\n",
    "    \n",
    "    # Level 5 to 4: 6x6 -> 13x13\n",
    "    up4 = UpSampling2D(size=(2, 2))(bottleneck)\n",
    "    up4 = ResizeLayer(target_size=(encoder4.shape[1], encoder4.shape[2]))(up4)\n",
    "    up4 = conv_block(up4, 512, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels before concatenation\n",
    "    skip4 = Conv2D(256, 1, padding='same')(encoder4)\n",
    "    skip4 = BatchNormalization()(skip4)\n",
    "    skip4 = Activation('relu')(skip4)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge4 = Concatenate()([up4, skip4])\n",
    "    merge4 = conv_block(merge4, 384)  # Reduced filters\n",
    "    \n",
    "    # Level 4 to 3: 13x13 -> 27x27\n",
    "    up3 = UpSampling2D(size=(2, 2))(merge4)\n",
    "    up3 = ResizeLayer(target_size=(encoder3.shape[1], encoder3.shape[2]))(up3)\n",
    "    up3 = conv_block(up3, 384, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels\n",
    "    skip3 = Conv2D(128, 1, padding='same')(encoder3)\n",
    "    skip3 = BatchNormalization()(skip3)\n",
    "    skip3 = Activation('relu')(skip3)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge3 = Concatenate()([up3, skip3])\n",
    "    merge3 = conv_block(merge3, 192)  # Reduced filters\n",
    "    \n",
    "    # Level 3 to 2: 27x27 -> 55x55\n",
    "    up2 = UpSampling2D(size=(2, 2))(merge3)\n",
    "    up2 = ResizeLayer(target_size=(encoder2.shape[1], encoder2.shape[2]))(up2)\n",
    "    up2 = conv_block(up2, 192, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels\n",
    "    skip2 = Conv2D(96, 1, padding='same')(encoder2)\n",
    "    skip2 = BatchNormalization()(skip2)\n",
    "    skip2 = Activation('relu')(skip2)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge2 = Concatenate()([up2, skip2])\n",
    "    merge2 = conv_block(merge2, 96)  # Reduced filters\n",
    "    \n",
    "    # Level 2 to 1: 55x55 -> 111x111\n",
    "    up1 = UpSampling2D(size=(2, 2))(merge2)\n",
    "    up1 = ResizeLayer(target_size=(encoder1.shape[1], encoder1.shape[2]))(up1)\n",
    "    up1 = conv_block(up1, 96, kernel_size=3)  # Reduced filters\n",
    "    \n",
    "    # Reduce skip connection channels\n",
    "    skip1 = Conv2D(48, 1, padding='same')(encoder1)\n",
    "    skip1 = BatchNormalization()(skip1)\n",
    "    skip1 = Activation('relu')(skip1)\n",
    "    \n",
    "    # Concatenate with skip connection\n",
    "    merge1 = Concatenate()([up1, skip1])\n",
    "    merge1 = conv_block(merge1, 48)  # Reduced filters\n",
    "    \n",
    "    # Final upsampling to original resolution: 111x111 -> 224x224\n",
    "    up_final = UpSampling2D(size=(2, 2))(merge1)\n",
    "    up_final = conv_block(up_final, 32)  # Reduced filters\n",
    "    \n",
    "    # Ensure final size matches input\n",
    "    if up_final.shape[1] != input_shape[0] or up_final.shape[2] != input_shape[1]:\n",
    "        up_final = ResizeLayer(target_size=(input_shape[0], input_shape[1]))(up_final)\n",
    "    \n",
    "    # Add a final segmentation head\n",
    "    outputs = Conv2D(num_classes, 1, activation='softmax', dtype='float32')(up_final)\n",
    "    \n",
    "    # Create and return the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "print(\"Creating full InceptionResNetV2-UNet model...\")\n",
    "model = build_full_inceptionresnetv2_unet(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), num_classes=NUM_CLASSES)\n",
    "print(\"Model created successfully!\")\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T01:28:42.105279Z",
     "iopub.status.busy": "2025-04-07T01:28:42.104280Z",
     "iopub.status.idle": "2025-04-07T03:02:15.964962Z",
     "shell.execute_reply": "2025-04-07T03:02:15.964962Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# # âœ… Dice Coefficient Metric\n",
    "# def dice_coefficient(y_true, y_pred):\n",
    "#     smooth = 1e-15\n",
    "#     y_true = tf.cast(y_true, tf.float32)\n",
    "#     y_pred = tf.cast(y_pred, tf.float32)\n",
    "#     intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "#     union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "#     dice = (2. * intersection + smooth) / (union + smooth)\n",
    "#     return tf.reduce_mean(dice)\n",
    "\n",
    "# def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "#     class_weights = tf.constant([0.3794, 0.7521, 69.7061, 49.3458], dtype=tf.float32)\n",
    "\n",
    "#     # Ensure y_true has the same shape as y_pred\n",
    "#     y_true = tf.cast(y_true, tf.float32)  # Make sure it's float32 for numerical stability\n",
    "#     y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)  # Avoid log(0)\n",
    "\n",
    "#     # Compute categorical cross-entropy\n",
    "#     loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)  # Sum over the last axis (class axis)\n",
    "\n",
    "#     # Reshape the class weights to match the loss shape\n",
    "#     class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))  # [1, 1, 1, 4]\n",
    "\n",
    "#     # Apply the class weights\n",
    "#     weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)  # Broadcast weights over the batch and spatial dimensions\n",
    "\n",
    " #     return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# # âœ… Dice Loss\n",
    "# def dice_loss(y_true, y_pred):\n",
    "#     smooth = 1e-6\n",
    "#     y_true = tf.cast(y_true, y_pred.dtype)\n",
    "#     intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "#     union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "#     dice = (2. * intersection + smooth) / (union + smooth)\n",
    "#     return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# # âœ… Combined Loss Function\n",
    "# def combined_loss(y_true, y_pred):\n",
    "#     return weighted_categorical_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "# # âœ… Custom Dice Coefficient Metric for Each Class\n",
    "# class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "#     def __init__(self, class_idx, name=None, **kwargs):  \n",
    "#         if name is None:\n",
    "#             name = f\"DiceClass{class_idx}\"  \n",
    "#         super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "#         self.class_idx = class_idx\n",
    "#         self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         y_true_class = y_true[..., self.class_idx]\n",
    "#         y_pred_class = y_pred[..., self.class_idx]\n",
    "\n",
    "#         intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "#         union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "\n",
    "#         dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "#         self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "#     def result(self):\n",
    "#         return self.dice\n",
    "\n",
    "# # âœ… Function to Get Class-wise Metrics\n",
    "# def class_wise_metrics(num_classes=4):\n",
    "#     return [DiceCoefficient(i) for i in range(num_classes)] + [tf.keras.metrics.MeanIoU(num_classes=num_classes)]\n",
    "\n",
    "# # âœ… Compile the Model\n",
    "# model.compile(\n",
    "#     optimizer=Adam(learning_rate=1e-4),\n",
    "#     loss=combined_loss,\n",
    "#     metrics=class_wise_metrics(4)  # Number of classes (adjust if needed)\n",
    "# )\n",
    "\n",
    "# # âœ… Train the Model with the **Filtered Dataset**\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,  # Use the loaded and split data\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=50,\n",
    "#     batch_size=16,  # Adjust based on available resources\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(\n",
    "#             monitor='val_loss', \n",
    "#             patience=7, \n",
    "#             restore_best_weights=True\n",
    "#         ),\n",
    "#         ReduceLROnPlateau(\n",
    "#             monitor='val_loss', \n",
    "#             factor=0.5, \n",
    "#             patience=3, \n",
    "#             min_lr=1e-6\n",
    "#         ),\n",
    "#         ModelCheckpoint(\n",
    "#             '/kaggle/working/best_unet_model_filtered.keras',  # Save in Kaggle working directory with .keras extension\n",
    "#             monitor='val_loss',\n",
    "#             save_best_only=True\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# âœ… Dice Coefficient Metric\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "# âœ… Weighted Categorical Crossentropy\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    class_weights = tf.constant([0.3776, 0.7605, 65.8554, 46.2381], dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "    class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))\n",
    "    weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# âœ… Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# âœ… Custom Dice Coefficient Metric for Each Class\n",
    "class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "    def __init__(self, class_idx, name=None, **kwargs):  \n",
    "        if name is None:\n",
    "            name = f\"DiceClass{class_idx}\"  \n",
    "        super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.class_idx = class_idx\n",
    "        self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_class = y_true[..., self.class_idx]\n",
    "        y_pred_class = y_pred[..., self.class_idx]\n",
    "        intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "        union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "        dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "        self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "    def result(self):\n",
    "        return self.dice\n",
    "\n",
    "# âœ… Function to Get Class-wise Metrics\n",
    "def class_wise_metrics(num_classes=4):\n",
    "    return [DiceCoefficient(i) for i in range(num_classes)] + [tf.keras.metrics.MeanIoU(num_classes=num_classes)]\n",
    "\n",
    "# âœ… Create Data Generator\n",
    "def create_train_generator(X, y, batch_size=16):\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    seed = 42\n",
    "    image_generator = image_datagen.flow(X, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_datagen.flow(y, batch_size=batch_size, seed=seed)\n",
    "    \n",
    "    while True:\n",
    "        X_batch = next(image_generator)\n",
    "        y_batch = next(mask_generator)\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# âœ… Create the generator\n",
    "train_generator = create_train_generator(X_train, y_train, batch_size=8)\n",
    "\n",
    "# âœ… Compile the Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def lovasz_softmax_loss(y_true, y_pred, ignore_background=False):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    num_classes = tf.shape(y_true)[-1]\n",
    "    start_class = tf.constant(1 if ignore_background else 0)\n",
    "\n",
    "    def compute_class_loss(c):\n",
    "        y_true_class = y_true[..., c]\n",
    "        y_pred_class = y_pred[..., c]\n",
    "\n",
    "        y_true_flat = tf.reshape(y_true_class, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred_class, [-1])\n",
    "\n",
    "        errors = tf.abs(y_true_flat - y_pred_flat)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], sorted=True)\n",
    "        y_true_sorted = tf.gather(y_true_flat, perm)\n",
    "\n",
    "        gts = tf.reduce_sum(y_true_sorted)\n",
    "        intersection = gts - tf.cumsum(y_true_sorted)\n",
    "        union = gts + tf.cumsum(1. - y_true_sorted)\n",
    "        jaccard = 1. - intersection / union\n",
    "        grad = tf.concat([[jaccard[0]], jaccard[1:] - jaccard[:-1]], 0)\n",
    "\n",
    "        return tf.tensordot(errors_sorted, grad, axes=1)\n",
    "\n",
    "    # Loop through each class using tf.while_loop\n",
    "    losses = tf.TensorArray(dtype=tf.float32, size=num_classes)\n",
    "\n",
    "    def loop_cond(c, losses):\n",
    "        return tf.less(c, num_classes)\n",
    "\n",
    "    def loop_body(c, losses):\n",
    "        loss_c = compute_class_loss(c)\n",
    "        losses = losses.write(c, loss_c)\n",
    "        return c + 1, losses\n",
    "\n",
    "    _, losses = tf.while_loop(loop_cond, loop_body, [start_class, losses])\n",
    "    return tf.reduce_mean(losses.stack())\n",
    "\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss_val = 1 - tf.reduce_mean(dice)\n",
    "\n",
    "    lovasz_loss_val = lovasz_softmax_loss(y_true, tf.nn.softmax(y_pred), ignore_background=False)\n",
    "\n",
    "    return lovasz_loss_val + dice_loss_val\n",
    "\n",
    "\n",
    "# Usage in model compilation\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(4)  # Number of classes\n",
    ")\n",
    "\n",
    "# âœ… Train the Model with the Data Generator\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // 16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=3, \n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'lovaszloss_unet++_inceptionresnetv2.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:02:15.967963Z",
     "iopub.status.busy": "2025-04-07T03:02:15.967963Z",
     "iopub.status.idle": "2025-04-07T03:02:16.603106Z",
     "shell.execute_reply": "2025-04-07T03:02:16.603106Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting data from the history object\n",
    "history_dict = history.history\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict['loss'], label='Training Loss')\n",
    "plt.plot(history_dict['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# If accuracy is available, plot training and validation accuracy\n",
    "if 'accuracy' in history_dict:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_dict['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history_dict['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T03:02:16.605107Z",
     "iopub.status.busy": "2025-04-07T03:02:16.605107Z",
     "iopub.status.idle": "2025-04-07T03:02:55.543499Z",
     "shell.execute_reply": "2025-04-07T03:02:55.543499Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# âœ… RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# âœ… Function to convert RGB masks to class index masks\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    # Create a mask initialized with zeros (for background class)\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "\n",
    "    # Loop through the RGB_TO_CLASS dictionary\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        # Identify the pixels with the current RGB value and assign them the class index\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# âœ… Function to calculate Dice Similarity Coefficient (DSC)\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# âœ… Function to calculate IoU (Intersection over Union)\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# âœ… Function to calculate Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')  # Return inf if no points for either true or pred class\n",
    "\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# âœ… Function to calculate Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')  # Return inf if no points for either true or pred class\n",
    "\n",
    "    distances = []\n",
    "    for true_point in true_points:\n",
    "        distances.append(np.min(np.linalg.norm(pred_points - true_point, axis=1)))\n",
    "    return np.mean(distances)\n",
    "\n",
    "# âœ… Function to evaluate the model on the test set class-wise\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16):\n",
    "    # Predict in batches\n",
    "    y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)  # Convert to class index prediction\n",
    "\n",
    "    # Convert y_test to class index format (since it's one-hot encoded)\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    # Initialize lists to store class-wise metrics\n",
    "    class_metrics = {i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []} for i in range(num_classes)}\n",
    "\n",
    "    # Calculate metrics for each test sample\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]  # one-hot -> class index\n",
    "        pred_mask = y_pred[i]\n",
    "\n",
    "        # For each class (0: Background, 1: Brain, 2: CSP, 3: LV)\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            # Dice Coefficient\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            # IoU\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            # Precision\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # Recall\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # F1 Score\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # Accuracy\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # # Hausdorff Distance\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # # Average Surface Distance\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # Print class-wise metrics in percentage\n",
    "    print(f\"{'Class':<10}{'Dice Coefficient (%)':<20}{'IoU (%)':<20}{'Precision (%)':<20}{'Recall (%)':<20}{'F1 Score (%)':<20}{'Accuracy (%)':<20}{'Hausdorff Distance':<20}{'Avg Surface Distance':<20}\")\n",
    "    print('-' * 180)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"Class {class_idx}:\")\n",
    "        print(f\"  Dice Coefficient: {np.mean(class_metrics[class_idx]['dice']) * 100:.2f}%\")\n",
    "        print(f\"  IoU: {np.mean(class_metrics[class_idx]['iou']) * 100:.2f}%\")\n",
    "        print(f\"  Precision: {np.mean(class_metrics[class_idx]['precision']) * 100:.2f}%\")\n",
    "        print(f\"  Recall: {np.mean(class_metrics[class_idx]['recall']) * 100:.2f}%\")\n",
    "        print(f\"  F1 Score: {np.mean(class_metrics[class_idx]['f1']) * 100:.2f}%\")\n",
    "        print(f\"  Accuracy: {np.mean(class_metrics[class_idx]['accuracy']) * 100:.2f}%\")\n",
    "        # print(f\"  Hausdorff Distance: {np.mean(class_metrics[class_idx]['hausdorff']):.4f}\")\n",
    "        # print(f\"  Average Surface Distance: {np.mean(class_metrics[class_idx]['asd']):.4f}\")\n",
    "        print(\"-\" * 180)\n",
    "\n",
    "    # Evaluate on test set to print overall test accuracy and loss\n",
    "    test_loss, *test_metrics = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    for metric, value in zip(model.metrics_names[1:], test_metrics):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# âœ… Call the evaluation function on the test set class-wise\n",
    "evaluate_classwise_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6940010,
     "sourceId": 11127979,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
