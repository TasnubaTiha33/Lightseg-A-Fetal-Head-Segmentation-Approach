{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-07T04:36:52.488485Z",
     "iopub.status.busy": "2025-04-07T04:36:52.488485Z",
     "iopub.status.idle": "2025-04-07T04:44:26.931926Z",
     "shell.execute_reply": "2025-04-07T04:44:26.931926Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import re\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ‚úÖ Constants for 224x224\n",
    "IMG_HEIGHT = 224  # Ensure height is 224\n",
    "IMG_WIDTH = 224   # Ensure width is 224\n",
    "CHANNELS = 3  # RGB images\n",
    "NUM_CLASSES = 4  # Brain, CSP, LV, Background\n",
    "\n",
    "# ‚úÖ Class mapping from RGB to class index\n",
    "CLASS_MAP = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0,  # Background\n",
    "}\n",
    "\n",
    "image_dir = r\"D:\\augmented_dataset\\images\"\n",
    "mask_dir = r\"D:\\augmented_dataset\\masks\"\n",
    "\n",
    "# ‚úÖ Define destination directories\n",
    "train_image_dir = r\"D:\\Updated\\train\\images\"\n",
    "train_mask_dir = r\"D:\\Updated\\train\\masks\"\n",
    "val_image_dir = r\"D:\\Updated\\val\\images\"\n",
    "val_mask_dir = r\"D:\\Updated\\val\\masks\"\n",
    "test_image_dir = r\"D:\\Updated\\test\\images\"\n",
    "test_mask_dir = r\"D:\\Updated\\test\\masks\"\n",
    "\n",
    "# ‚úÖ Fix sorting issue using natural sorting\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Sort filenames numerically instead of lexicographically.\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "# ‚úÖ Convert RGB mask to class index mask\n",
    "def rgb_to_class(mask_array):\n",
    "    \"\"\"Convert RGB mask to single-channel class index mask.\"\"\"\n",
    "    height, width, _ = mask_array.shape\n",
    "    class_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for rgb, class_idx in CLASS_MAP.items():\n",
    "        matches = np.all(mask_array == rgb, axis=-1)  # Ensure exact match\n",
    "        class_mask[matches] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Preprocess Filtered Dataset for 224x224\n",
    "def preprocess_filtered_dataset(image_dir, mask_dir):\n",
    "    \"\"\"Preprocess images & masks: normalize, resize, and convert masks to one-hot encoding.\"\"\"\n",
    "\n",
    "    # ‚úÖ Load and sort filenames correctly\n",
    "    image_filenames = sorted(os.listdir(image_dir), key=natural_sort_key)\n",
    "    mask_filenames = sorted(os.listdir(mask_dir), key=natural_sort_key)\n",
    "\n",
    "    valid_image_paths = []\n",
    "    valid_mask_paths = []\n",
    "\n",
    "    # ‚úÖ Ensure each image has a corresponding mask\n",
    "    for img_file, mask_file in zip(image_filenames, mask_filenames):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            valid_image_paths.append(img_path)\n",
    "            valid_mask_paths.append(mask_path)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipping {img_file}: Missing image or mask\")\n",
    "\n",
    "    num_images = len(valid_image_paths)\n",
    "\n",
    "    # ‚úÖ Initialize arrays\n",
    "    X = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, CHANNELS), dtype=np.float32)\n",
    "    y = np.zeros((num_images, IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES), dtype=np.float32)  # One-hot encoded masks\n",
    "\n",
    "    print(f\"üöÄ Processing {num_images} filtered images and masks...\")\n",
    "\n",
    "    for idx, (img_path, mask_path) in enumerate(zip(valid_image_paths, valid_mask_paths)):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"‚úÖ Processed {idx}/{num_images} images\")\n",
    "\n",
    "        # ‚úÖ Load and Resize Image\n",
    "        img = cv2.imread(img_path)  # Read image in BGR format\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))  # Resize to (224,224)\n",
    "        img = img.astype(np.float32) / 255.0  # Normalize\n",
    "\n",
    "        # ‚úÖ Load and Resize Mask\n",
    "        mask = cv2.imread(mask_path)  # Read mask in BGR format\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        mask = cv2.resize(mask, (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)  # Resize mask correctly\n",
    "\n",
    "        # ‚úÖ Convert RGB mask to class mask\n",
    "        class_mask = rgb_to_class(mask)\n",
    "\n",
    "        # ‚úÖ One-hot encode the class mask\n",
    "        one_hot_mask = to_categorical(class_mask, num_classes=NUM_CLASSES)\n",
    "\n",
    "        # ‚úÖ Store preprocessed data\n",
    "        X[idx] = img\n",
    "        y[idx] = one_hot_mask\n",
    "\n",
    "        # ‚úÖ Clear memory to prevent memory leaks\n",
    "        del img, mask, class_mask, one_hot_mask\n",
    "        gc.collect()\n",
    "\n",
    "    return X, y\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ‚úÖ Process dataset splits\n",
    "X_train, y_train = preprocess_filtered_dataset(train_image_dir, train_mask_dir)\n",
    "X_val, y_val = preprocess_filtered_dataset(val_image_dir, val_mask_dir)\n",
    "X_test, y_test = preprocess_filtered_dataset(test_image_dir, test_mask_dir)\n",
    "\n",
    "# ‚úÖ Print dataset information\n",
    "print(\"\\n‚úÖ Dataset Splits:\")\n",
    "print(f\"  - Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"  - Validation set: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"  - Test set: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T04:44:26.933925Z",
     "iopub.status.busy": "2025-04-07T04:44:26.933925Z",
     "iopub.status.idle": "2025-04-07T04:44:27.460044Z",
     "shell.execute_reply": "2025-04-07T04:44:27.460044Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data Augmentation configuration for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# Fit the augmentation parameters on the training data\n",
    "train_datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T04:44:27.463045Z",
     "iopub.status.busy": "2025-04-07T04:44:27.463045Z",
     "iopub.status.idle": "2025-04-07T04:44:33.206332Z",
     "shell.execute_reply": "2025-04-07T04:44:33.206332Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Unique values in y_train:\", np.unique(np.argmax(y_train, axis=-1)))\n",
    "print(\"Unique values in y_val:\", np.unique(np.argmax(y_val, axis=-1)))\n",
    "print(\"Unique values in y_test:\", np.unique(np.argmax(y_test, axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T04:44:33.209333Z",
     "iopub.status.busy": "2025-04-07T04:44:33.208333Z",
     "iopub.status.idle": "2025-04-07T04:44:35.238788Z",
     "shell.execute_reply": "2025-04-07T04:44:35.238788Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import Input, Model\n",
    "# from tensorflow.keras.layers import Conv2DTranspose, Conv2D, Concatenate, BatchNormalization, Activation\n",
    "# from tensorflow.keras.applications import Xception\n",
    "\n",
    "# class ResizeLayer(tf.keras.layers.Layer):\n",
    "#     \"\"\"Custom layer to resize images.\"\"\"\n",
    "#     def __init__(self, target_size, **kwargs):\n",
    "#         super(ResizeLayer, self).__init__(**kwargs)\n",
    "#         self.target_size = target_size\n",
    "    \n",
    "#     def call(self, inputs):\n",
    "#         return tf.image.resize(inputs, self.target_size, method='bilinear')\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         config = super(ResizeLayer, self).get_config()\n",
    "#         config.update({\"target_size\": self.target_size})\n",
    "#         return config\n",
    "\n",
    "# def find_best_skip_layers(base_model, target_sizes):\n",
    "#     \"\"\"Finds the best matching layers for skip connections (based on spatial size).\"\"\"\n",
    "#     layer_dict = {layer.name: layer.output.shape for layer in base_model.layers}\n",
    "#     skip_connections = []\n",
    "#     for target_size in target_sizes:\n",
    "#         best_match = None\n",
    "#         min_diff = float('inf')\n",
    "#         for layer_name, shape in layer_dict.items():\n",
    "#             if len(shape) == 4:  # Ensure it's a conv layer\n",
    "#                 h, w, c = shape[1], shape[2], shape[3]  # Extract spatial dims & channels\n",
    "#                 # Prioritize spatial dimensions match\n",
    "#                 diff = abs(h - target_size[0]) + abs(w - target_size[1])\n",
    "#                 if diff < min_diff:\n",
    "#                     min_diff = diff\n",
    "#                     best_match = layer_name\n",
    "#         skip_connections.append(best_match)\n",
    "#     return skip_connections\n",
    "\n",
    "# def conv_block(x, filters, kernel_size=3, padding='same', activation='relu'):\n",
    "#     \"\"\"Helper function for creating a conv block with BN and activation.\"\"\"\n",
    "#     x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Activation(activation)(x)\n",
    "#     return x\n",
    "\n",
    "# def build_unetpp_with_xception(input_shape=(224, 224, 3), num_classes=4):\n",
    "#     \"\"\"\n",
    "#     Build UNet++ model with Xception backbone\n",
    "    \n",
    "#     Args:\n",
    "#         input_shape: Input shape of the image\n",
    "#         num_classes: Number of output classes\n",
    "        \n",
    "#     Returns:\n",
    "#         Keras Model instance with UNet++ architecture\n",
    "#     \"\"\"\n",
    "#     inputs = Input(shape=input_shape)\n",
    "    \n",
    "#     # Use Xception as the encoder backbone\n",
    "#     # Note: Xception requires input size >= 71x71\n",
    "#     base_model = Xception(\n",
    "#         input_tensor=inputs, \n",
    "#         include_top=False, \n",
    "#         weights='imagenet'\n",
    "#     )\n",
    "    \n",
    "#     # Define target sizes for skip connections at different levels\n",
    "#     # These are approximate sizes for a 224x224 input with Xception\n",
    "#     target_skip_sizes = [(56, 56, 128), (28, 28, 256), (14, 14, 728), (7, 7, 2048)]\n",
    "    \n",
    "#     # Find optimal skip connection layers\n",
    "#     skip_layer_names = find_best_skip_layers(base_model, target_skip_sizes)\n",
    "#     print(\"Selected skip connection layers:\", skip_layer_names)\n",
    "    \n",
    "#     # Extract skip connections\n",
    "#     skip1 = base_model.get_layer(skip_layer_names[0]).output  # ~56x56\n",
    "#     skip2 = base_model.get_layer(skip_layer_names[1]).output  # ~28x28\n",
    "#     skip3 = base_model.get_layer(skip_layer_names[2]).output  # ~14x14\n",
    "#     skip4 = base_model.get_layer(skip_layer_names[3]).output  # ~7x7\n",
    "    \n",
    "#     # Bottleneck (deepest feature map)\n",
    "#     bottleneck = skip4  # 7x7 feature map\n",
    "    \n",
    "#     # ----- UNet++ Architecture -----\n",
    "#     # UNet++ adds nested skip pathways compared to regular UNet\n",
    "    \n",
    "#     # First decoder level (L4)\n",
    "#     # From bottleneck to 14x14\n",
    "#     up4 = Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same')(bottleneck)\n",
    "#     up4 = BatchNormalization()(up4)\n",
    "#     up4 = Activation('relu')(up4)\n",
    "    \n",
    "#     # Match sizes if needed\n",
    "#     if up4.shape[1] != skip3.shape[1] or up4.shape[2] != skip3.shape[2]:\n",
    "#         up4 = ResizeLayer(target_size=(skip3.shape[1], skip3.shape[2]))(up4)\n",
    "    \n",
    "#     # Create node X^0_4 (first nested dense block)\n",
    "#     x0_4 = Concatenate()([up4, skip3])\n",
    "#     x0_4 = conv_block(x0_4, 512)\n",
    "#     x0_4 = conv_block(x0_4, 512)\n",
    "    \n",
    "#     # Second decoder level (L3)\n",
    "#     # From x0_4 to 28x28\n",
    "#     up3 = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(x0_4)\n",
    "#     up3 = BatchNormalization()(up3)\n",
    "#     up3 = Activation('relu')(up3)\n",
    "    \n",
    "#     # Match sizes if needed\n",
    "#     if up3.shape[1] != skip2.shape[1] or up3.shape[2] != skip2.shape[2]:\n",
    "#         up3 = ResizeLayer(target_size=(skip2.shape[1], skip2.shape[2]))(up3)\n",
    "    \n",
    "#     # Create node X^0_3\n",
    "#     x0_3 = Concatenate()([up3, skip2])\n",
    "#     x0_3 = conv_block(x0_3, 256)\n",
    "#     x0_3 = conv_block(x0_3, 256)\n",
    "    \n",
    "#     # Create node X^1_3 (second nested dense block)\n",
    "#     up1_3 = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(bottleneck)\n",
    "#     up1_3 = BatchNormalization()(up1_3)\n",
    "#     up1_3 = Activation('relu')(up1_3)\n",
    "#     if up1_3.shape[1] != x0_3.shape[1] or up1_3.shape[2] != x0_3.shape[2]:\n",
    "#         up1_3 = ResizeLayer(target_size=(x0_3.shape[1], x0_3.shape[2]))(up1_3)\n",
    "#     x1_3 = Concatenate()([up1_3, x0_3])\n",
    "#     x1_3 = conv_block(x1_3, 256)\n",
    "#     x1_3 = conv_block(x1_3, 256)\n",
    "    \n",
    "#     # Third decoder level (L2)\n",
    "#     # From x0_3 to 56x56\n",
    "#     up2 = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(x0_3)\n",
    "#     up2 = BatchNormalization()(up2)\n",
    "#     up2 = Activation('relu')(up2)\n",
    "    \n",
    "#     # Match sizes if needed\n",
    "#     if up2.shape[1] != skip1.shape[1] or up2.shape[2] != skip1.shape[2]:\n",
    "#         up2 = ResizeLayer(target_size=(skip1.shape[1], skip1.shape[2]))(up2)\n",
    "    \n",
    "#     # Create node X^0_2\n",
    "#     x0_2 = Concatenate()([up2, skip1])\n",
    "#     x0_2 = conv_block(x0_2, 128)\n",
    "#     x0_2 = conv_block(x0_2, 128)\n",
    "    \n",
    "#     # Create node X^1_2 (nested dense block)\n",
    "#     up1_2 = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(x0_3)\n",
    "#     up1_2 = BatchNormalization()(up1_2)\n",
    "#     up1_2 = Activation('relu')(up1_2)\n",
    "#     if up1_2.shape[1] != x0_2.shape[1] or up1_2.shape[2] != x0_2.shape[2]:\n",
    "#         up1_2 = ResizeLayer(target_size=(x0_2.shape[1], x0_2.shape[2]))(up1_2)\n",
    "#     x1_2 = Concatenate()([up1_2, x0_2])\n",
    "#     x1_2 = conv_block(x1_2, 128)\n",
    "#     x1_2 = conv_block(x1_2, 128)\n",
    "    \n",
    "#     # Create node X^2_2 (nested dense block)\n",
    "#     up2_2 = Conv2DTranspose(128, (3, 3), strides=(4, 4), padding='same')(x1_3)\n",
    "#     up2_2 = BatchNormalization()(up2_2)\n",
    "#     up2_2 = Activation('relu')(up2_2)\n",
    "#     if up2_2.shape[1] != x1_2.shape[1] or up2_2.shape[2] != x1_2.shape[2]:\n",
    "#         up2_2 = ResizeLayer(target_size=(x1_2.shape[1], x1_2.shape[2]))(up2_2)\n",
    "#     x2_2 = Concatenate()([up2_2, x1_2])\n",
    "#     x2_2 = conv_block(x2_2, 128)\n",
    "#     x2_2 = conv_block(x2_2, 128)\n",
    "    \n",
    "#     # Fourth decoder level (L1) - to original size\n",
    "#     # From x0_2 to 224x224\n",
    "#     up1 = Conv2DTranspose(64, (3, 3), strides=(4, 4), padding='same')(x0_2)\n",
    "#     up1 = BatchNormalization()(up1)\n",
    "#     up1 = Activation('relu')(up1)\n",
    "    \n",
    "#     # Create node X^0_1\n",
    "#     x0_1 = up1\n",
    "#     x0_1 = conv_block(x0_1, 64)\n",
    "#     x0_1 = conv_block(x0_1, 64)\n",
    "    \n",
    "#     # Create node X^1_1\n",
    "#     up1_1 = Conv2DTranspose(64, (3, 3), strides=(4, 4), padding='same')(x0_2)\n",
    "#     up1_1 = BatchNormalization()(up1_1)\n",
    "#     up1_1 = Activation('relu')(up1_1)\n",
    "#     if up1_1.shape[1] != x0_1.shape[1] or up1_1.shape[2] != x0_1.shape[2]:\n",
    "#         up1_1 = ResizeLayer(target_size=(x0_1.shape[1], x0_1.shape[2]))(up1_1)\n",
    "#     x1_1 = Concatenate()([up1_1, x0_1])\n",
    "#     x1_1 = conv_block(x1_1, 64)\n",
    "#     x1_1 = conv_block(x1_1, 64)\n",
    "    \n",
    "#     # Create node X^2_1\n",
    "#     up2_1 = Conv2DTranspose(64, (3, 3), strides=(8, 8), padding='same')(x1_2)\n",
    "#     up2_1 = BatchNormalization()(up2_1)\n",
    "#     up2_1 = Activation('relu')(up2_1)\n",
    "#     if up2_1.shape[1] != x1_1.shape[1] or up2_1.shape[2] != x1_1.shape[2]:\n",
    "#         up2_1 = ResizeLayer(target_size=(x1_1.shape[1], x1_1.shape[2]))(up2_1)\n",
    "#     x2_1 = Concatenate()([up2_1, x1_1])\n",
    "#     x2_1 = conv_block(x2_1, 64)\n",
    "#     x2_1 = conv_block(x2_1, 64)\n",
    "    \n",
    "#     # Create node X^3_1\n",
    "#     up3_1 = Conv2DTranspose(64, (3, 3), strides=(16, 16), padding='same')(x2_2)\n",
    "#     up3_1 = BatchNormalization()(up3_1)\n",
    "#     up3_1 = Activation('relu')(up3_1)\n",
    "#     if up3_1.shape[1] != x2_1.shape[1] or up3_1.shape[2] != x2_1.shape[2]:\n",
    "#         up3_1 = ResizeLayer(target_size=(x2_1.shape[1], x2_1.shape[2]))(up3_1)\n",
    "#     x3_1 = Concatenate()([up3_1, x2_1])\n",
    "#     x3_1 = conv_block(x3_1, 64)\n",
    "#     x3_1 = conv_block(x3_1, 64)\n",
    "    \n",
    "#     # Final resize to ensure output is exactly input_shape size\n",
    "#     if x3_1.shape[1] != input_shape[0] or x3_1.shape[2] != input_shape[1]:\n",
    "#         x3_1 = ResizeLayer(target_size=(input_shape[0], input_shape[1]))(x3_1)\n",
    "    \n",
    "#     # Segmentation head\n",
    "#     outputs = Conv2D(num_classes, (1, 1), activation='softmax')(x3_1)\n",
    "    \n",
    "#     # Create model\n",
    "#     model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Model parameters\n",
    "# IMG_HEIGHT = 224\n",
    "# IMG_WIDTH = 224\n",
    "# CHANNELS = 3\n",
    "# NUM_CLASSES = 4\n",
    "\n",
    "# # Build and summarize model\n",
    "# model = build_unetpp_with_xception(input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), num_classes=NUM_CLASSES)\n",
    "# model.summary()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, Concatenate, BatchNormalization, Activation\n",
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "# Enable mixed precision training to reduce memory usage\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "class ResizeLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom layer to resize images.\"\"\"\n",
    "    def __init__(self, target_size, **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, self.target_size, method='bilinear')\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(ResizeLayer, self).get_config()\n",
    "        config.update({\"target_size\": self.target_size})\n",
    "        return config\n",
    "\n",
    "def find_best_skip_layers(base_model, target_sizes):\n",
    "    \"\"\"Finds the best matching layers for skip connections (based on spatial size).\"\"\"\n",
    "    layer_dict = {layer.name: layer.output.shape for layer in base_model.layers}\n",
    "    skip_connections = []\n",
    "    for target_size in target_sizes:\n",
    "        best_match = None\n",
    "        min_diff = float('inf')\n",
    "        for layer_name, shape in layer_dict.items():\n",
    "            if len(shape) == 4:  # Ensure it's a conv layer\n",
    "                h, w, c = shape[1], shape[2], shape[3]  # Extract spatial dims & channels\n",
    "                # Prioritize spatial dimensions match\n",
    "                diff = abs(h - target_size[0]) + abs(w - target_size[1])\n",
    "                if diff < min_diff:\n",
    "                    min_diff = diff\n",
    "                    best_match = layer_name\n",
    "        skip_connections.append(best_match)\n",
    "    return skip_connections\n",
    "\n",
    "def conv_block(x, filters, kernel_size=3, padding='same', activation='relu'):\n",
    "    \"\"\"Helper function for creating a conv block with BN and activation.\"\"\"\n",
    "    x = Conv2D(filters, kernel_size, padding=padding)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x\n",
    "\n",
    "def build_memory_efficient_unetpp_xception(input_shape=(224, 224, 3), num_classes=4, batch_size=None):\n",
    "    \"\"\"\n",
    "    Build memory-efficient UNet++ model with Xception backbone\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input shape of the image\n",
    "        num_classes: Number of output classes\n",
    "        batch_size: Optional fixed batch size to optimize memory usage\n",
    "        \n",
    "    Returns:\n",
    "        Keras Model instance with UNet++ architecture\n",
    "    \"\"\"\n",
    "    # Optionally use fixed batch size to optimize memory allocation\n",
    "    if batch_size:\n",
    "        inputs = Input(batch_shape=(batch_size,) + input_shape)\n",
    "    else:\n",
    "        inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Use Xception as the encoder backbone\n",
    "    base_model = Xception(\n",
    "        input_tensor=inputs, \n",
    "        include_top=False, \n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers to reduce gradient memory usage\n",
    "    for layer in base_model.layers[:50]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Define target sizes for skip connections at different levels\n",
    "    target_skip_sizes = [(56, 56, 128), (28, 28, 256), (14, 14, 728), (7, 7, 2048)]\n",
    "    \n",
    "    # Find optimal skip connection layers\n",
    "    skip_layer_names = find_best_skip_layers(base_model, target_sizes=target_skip_sizes)\n",
    "    print(\"Selected skip connection layers:\", skip_layer_names)\n",
    "    \n",
    "    # Extract skip connections\n",
    "    skip1 = base_model.get_layer(skip_layer_names[0]).output  # ~56x56\n",
    "    skip2 = base_model.get_layer(skip_layer_names[1]).output  # ~28x28\n",
    "    skip3 = base_model.get_layer(skip_layer_names[2]).output  # ~14x14\n",
    "    skip4 = base_model.get_layer(skip_layer_names[3]).output  # ~7x7\n",
    "    \n",
    "    # Reduce channel dimensions in skip connections to save memory\n",
    "    skip1 = Conv2D(64, 1, padding='same')(skip1)\n",
    "    skip2 = Conv2D(128, 1, padding='same')(skip2)\n",
    "    skip3 = Conv2D(256, 1, padding='same')(skip3)\n",
    "    \n",
    "    # Bottleneck (deepest feature map) - reduce channels\n",
    "    bottleneck = Conv2D(512, 1, padding='same')(skip4)  # 7x7 feature map\n",
    "    \n",
    "    # ----- UNet++ Architecture with memory optimizations -----\n",
    "    \n",
    "    # First decoder level (L4) - From bottleneck to 14x14\n",
    "    # Use UpSampling2D instead of Conv2DTranspose to save memory\n",
    "    up4 = UpSampling2D(size=(2, 2))(bottleneck)\n",
    "    up4 = conv_block(up4, 256)  # Reduced filter count\n",
    "    \n",
    "    # Match sizes if needed\n",
    "    if up4.shape[1] != skip3.shape[1] or up4.shape[2] != skip3.shape[2]:\n",
    "        up4 = ResizeLayer(target_size=(skip3.shape[1], skip3.shape[2]))(up4)\n",
    "    \n",
    "    # Create node X^0_4 (first nested dense block) - with reduced filters\n",
    "    x0_4 = Concatenate()([up4, skip3])\n",
    "    x0_4 = conv_block(x0_4, 256)  # Single conv block instead of double\n",
    "    \n",
    "    # Second decoder level (L3) - From x0_4 to 28x28\n",
    "    up3 = UpSampling2D(size=(2, 2))(x0_4)\n",
    "    up3 = conv_block(up3, 128)  # Reduced filter count\n",
    "    \n",
    "    # Match sizes if needed\n",
    "    if up3.shape[1] != skip2.shape[1] or up3.shape[2] != skip2.shape[2]:\n",
    "        up3 = ResizeLayer(target_size=(skip2.shape[1], skip2.shape[2]))(up3)\n",
    "    \n",
    "    # Create node X^0_3\n",
    "    x0_3 = Concatenate()([up3, skip2])\n",
    "    x0_3 = conv_block(x0_3, 128)  # Single conv block\n",
    "    \n",
    "    # Create node X^1_3 (second nested dense block)\n",
    "    # Direct upsampling from bottleneck\n",
    "    up1_3 = UpSampling2D(size=(4, 4))(bottleneck)  # 28x28\n",
    "    up1_3 = conv_block(up1_3, 128)\n",
    "    \n",
    "    if up1_3.shape[1] != x0_3.shape[1] or up1_3.shape[2] != x0_3.shape[2]:\n",
    "        up1_3 = ResizeLayer(target_size=(x0_3.shape[1], x0_3.shape[2]))(up1_3)\n",
    "    \n",
    "    x1_3 = Concatenate()([up1_3, x0_3])\n",
    "    x1_3 = conv_block(x1_3, 128)\n",
    "    \n",
    "    # Third decoder level (L2) - From x0_3 to 56x56\n",
    "    up2 = UpSampling2D(size=(2, 2))(x0_3)\n",
    "    up2 = conv_block(up2, 64)\n",
    "    \n",
    "    # Match sizes if needed\n",
    "    if up2.shape[1] != skip1.shape[1] or up2.shape[2] != skip1.shape[2]:\n",
    "        up2 = ResizeLayer(target_size=(skip1.shape[1], skip1.shape[2]))(up2)\n",
    "    \n",
    "    # Create node X^0_2\n",
    "    x0_2 = Concatenate()([up2, skip1])\n",
    "    x0_2 = conv_block(x0_2, 64)\n",
    "    \n",
    "    # Create node X^1_2 (nested dense block)\n",
    "    up1_2 = UpSampling2D(size=(2, 2))(x0_3)\n",
    "    up1_2 = conv_block(up1_2, 64)\n",
    "    \n",
    "    if up1_2.shape[1] != x0_2.shape[1] or up1_2.shape[2] != x0_2.shape[2]:\n",
    "        up1_2 = ResizeLayer(target_size=(x0_2.shape[1], x0_2.shape[2]))(up1_2)\n",
    "    \n",
    "    x1_2 = Concatenate()([up1_2, x0_2])\n",
    "    x1_2 = conv_block(x1_2, 64)\n",
    "    \n",
    "    # Memory optimization: Skip X^2_2 to reduce memory usage\n",
    "    # Instead, directly create X^2_1 from X^1_2\n",
    "    \n",
    "    # Fourth decoder level (L1) - to original size\n",
    "    # From x0_2 to 224x224\n",
    "    up1 = UpSampling2D(size=(4, 4))(x0_2)\n",
    "    up1 = conv_block(up1, 32)  # Reduced filters\n",
    "    \n",
    "    # Create node X^0_1\n",
    "    x0_1 = up1\n",
    "    \n",
    "    # Create node X^1_1\n",
    "    up1_1 = UpSampling2D(size=(4, 4))(x1_2)\n",
    "    up1_1 = conv_block(up1_1, 32)  # Reduced filters\n",
    "    \n",
    "    if up1_1.shape[1] != x0_1.shape[1] or up1_1.shape[2] != x0_1.shape[2]:\n",
    "        up1_1 = ResizeLayer(target_size=(x0_1.shape[1], x0_1.shape[2]))(up1_1)\n",
    "    \n",
    "    x1_1 = Concatenate()([up1_1, x0_1])\n",
    "    x1_1 = conv_block(x1_1, 32)  # Reduced filters\n",
    "    \n",
    "    # Memory optimization: Skip some deeper connections\n",
    "    # Use x1_1 as the final output\n",
    "    \n",
    "    # Final resize to ensure output is exactly input_shape size\n",
    "    if x1_1.shape[1] != input_shape[0] or x1_1.shape[2] != input_shape[1]:\n",
    "        x1_1 = ResizeLayer(target_size=(input_shape[0], input_shape[1]))(x1_1)\n",
    "    \n",
    "    # Segmentation head\n",
    "    outputs = Conv2D(num_classes, (1, 1), activation='softmax', dtype='float32')(x1_1)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Model parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "CHANNELS = 3\n",
    "NUM_CLASSES = 4\n",
    "BATCH_SIZE = None  # Fix batch size to help with memory management\n",
    "\n",
    "# Build and summarize model\n",
    "model = build_memory_efficient_unetpp_xception(\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), \n",
    "    num_classes=NUM_CLASSES,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Print training tips\n",
    "print(\"\\nMemory Efficiency Tips:\")\n",
    "print(\"1. This model uses mixed precision (float16) for most operations\")\n",
    "print(\"2. Use a fixed batch size of 8 or less\")\n",
    "print(\"3. Consider reducing image size further if memory issues persist\")\n",
    "print(\"4. Clear session between epochs with tf.keras.backend.clear_session()\")\n",
    "print(\"5. Update your generator to match the fixed batch size:\")\n",
    "print(\"   train_generator = create_train_generator(X_train, y_train, batch_size=8)\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T04:44:35.301803Z",
     "iopub.status.busy": "2025-04-07T04:44:35.301803Z",
     "iopub.status.idle": "2025-04-07T06:21:15.399478Z",
     "shell.execute_reply": "2025-04-07T06:21:15.399478Z"
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# # ‚úÖ Dice Coefficient Metric\n",
    "# def dice_coefficient(y_true, y_pred):\n",
    "#     smooth = 1e-15\n",
    "#     y_true = tf.cast(y_true, tf.float32)\n",
    "#     y_pred = tf.cast(y_pred, tf.float32)\n",
    "#     intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "#     union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "#     dice = (2. * intersection + smooth) / (union + smooth)\n",
    "#     return tf.reduce_mean(dice)\n",
    "\n",
    "# def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "#     class_weights = tf.constant([0.3794, 0.7521, 69.7061, 49.3458], dtype=tf.float32)\n",
    "\n",
    "#     # Ensure y_true has the same shape as y_pred\n",
    "#     y_true = tf.cast(y_true, tf.float32)  # Make sure it's float32 for numerical stability\n",
    "#     y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)  # Avoid log(0)\n",
    "\n",
    "#     # Compute categorical cross-entropy\n",
    "#     loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)  # Sum over the last axis (class axis)\n",
    "\n",
    "#     # Reshape the class weights to match the loss shape\n",
    "#     class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))  # [1, 1, 1, 4]\n",
    "\n",
    "#     # Apply the class weights\n",
    "#     weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)  # Broadcast weights over the batch and spatial dimensions\n",
    "\n",
    " #     return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# # ‚úÖ Dice Loss\n",
    "# def dice_loss(y_true, y_pred):\n",
    "#     smooth = 1e-6\n",
    "#     y_true = tf.cast(y_true, y_pred.dtype)\n",
    "#     intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "#     union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "#     dice = (2. * intersection + smooth) / (union + smooth)\n",
    "#     return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# # ‚úÖ Combined Loss Function\n",
    "# def combined_loss(y_true, y_pred):\n",
    "#     return weighted_categorical_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "# # ‚úÖ Custom Dice Coefficient Metric for Each Class\n",
    "# class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "#     def __init__(self, class_idx, name=None, **kwargs):  \n",
    "#         if name is None:\n",
    "#             name = f\"DiceClass{class_idx}\"  \n",
    "#         super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "#         self.class_idx = class_idx\n",
    "#         self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         y_true_class = y_true[..., self.class_idx]\n",
    "#         y_pred_class = y_pred[..., self.class_idx]\n",
    "\n",
    "#         intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "#         union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "\n",
    "#         dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "#         self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "#     def result(self):\n",
    "#         return self.dice\n",
    "\n",
    "# # ‚úÖ Function to Get Class-wise Metrics\n",
    "# def class_wise_metrics(num_classes=4):\n",
    "#     return [DiceCoefficient(i) for i in range(num_classes)] + [tf.keras.metrics.MeanIoU(num_classes=num_classes)]\n",
    "\n",
    "# # ‚úÖ Compile the Model\n",
    "# model.compile(\n",
    "#     optimizer=Adam(learning_rate=1e-4),\n",
    "#     loss=combined_loss,\n",
    "#     metrics=class_wise_metrics(4)  # Number of classes (adjust if needed)\n",
    "# )\n",
    "\n",
    "# # ‚úÖ Train the Model with the **Filtered Dataset**\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,  # Use the loaded and split data\n",
    "#     validation_data=(X_val, y_val),\n",
    "#     epochs=50,\n",
    "#     batch_size=16,  # Adjust based on available resources\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(\n",
    "#             monitor='val_loss', \n",
    "#             patience=7, \n",
    "#             restore_best_weights=True\n",
    "#         ),\n",
    "#         ReduceLROnPlateau(\n",
    "#             monitor='val_loss', \n",
    "#             factor=0.5, \n",
    "#             patience=3, \n",
    "#             min_lr=1e-6\n",
    "#         ),\n",
    "#         ModelCheckpoint(\n",
    "#             '/kaggle/working/best_unet_model_filtered.keras',  # Save in Kaggle working directory with .keras extension\n",
    "#             monitor='val_loss',\n",
    "#             save_best_only=True\n",
    "#         )\n",
    "#     ]\n",
    "# )\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ‚úÖ Dice Coefficient Metric\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Weighted Categorical Crossentropy\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    class_weights = tf.constant([0.3776, 0.7605, 65.8554, 46.2381], dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0)\n",
    "    loss = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=-1)\n",
    "    class_weights = tf.reshape(class_weights, (1, 1, 1, NUM_CLASSES))\n",
    "    weighted_loss = loss * tf.reduce_sum(class_weights, axis=-1)\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "# ‚úÖ Dice Loss\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "# ‚úÖ Custom Dice Coefficient Metric for Each Class\n",
    "class DiceCoefficient(tf.keras.metrics.Metric):\n",
    "    def __init__(self, class_idx, name=None, **kwargs):  \n",
    "        if name is None:\n",
    "            name = f\"DiceClass{class_idx}\"  \n",
    "        super(DiceCoefficient, self).__init__(name=name, **kwargs)\n",
    "        self.class_idx = class_idx\n",
    "        self.dice = self.add_weight(name=\"dice\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true_class = y_true[..., self.class_idx]\n",
    "        y_pred_class = y_pred[..., self.class_idx]\n",
    "        intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "        union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "        dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "        self.dice.assign(tf.reduce_mean(dice))\n",
    "\n",
    "    def result(self):\n",
    "        return self.dice\n",
    "\n",
    "# ‚úÖ Function to Get Class-wise Metrics\n",
    "def class_wise_metrics(num_classes=4):\n",
    "    return [DiceCoefficient(i) for i in range(num_classes)] + [tf.keras.metrics.MeanIoU(num_classes=num_classes)]\n",
    "\n",
    "# ‚úÖ Create Data Generator\n",
    "def create_train_generator(X, y, batch_size=16):\n",
    "    data_gen_args = dict(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    seed = 42\n",
    "    image_generator = image_datagen.flow(X, batch_size=batch_size, seed=seed)\n",
    "    mask_generator = mask_datagen.flow(y, batch_size=batch_size, seed=seed)\n",
    "    \n",
    "    while True:\n",
    "        X_batch = next(image_generator)\n",
    "        y_batch = next(mask_generator)\n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# ‚úÖ Create the generator\n",
    "train_generator = create_train_generator(X_train, y_train, batch_size=8)\n",
    "\n",
    "# ‚úÖ Compile the Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def lovasz_softmax_loss(y_true, y_pred, ignore_background=False):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    num_classes = tf.shape(y_true)[-1]\n",
    "    start_class = tf.constant(1 if ignore_background else 0)\n",
    "\n",
    "    def compute_class_loss(c):\n",
    "        y_true_class = y_true[..., c]\n",
    "        y_pred_class = y_pred[..., c]\n",
    "\n",
    "        y_true_flat = tf.reshape(y_true_class, [-1])\n",
    "        y_pred_flat = tf.reshape(y_pred_class, [-1])\n",
    "\n",
    "        errors = tf.abs(y_true_flat - y_pred_flat)\n",
    "        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], sorted=True)\n",
    "        y_true_sorted = tf.gather(y_true_flat, perm)\n",
    "\n",
    "        gts = tf.reduce_sum(y_true_sorted)\n",
    "        intersection = gts - tf.cumsum(y_true_sorted)\n",
    "        union = gts + tf.cumsum(1. - y_true_sorted)\n",
    "        jaccard = 1. - intersection / union\n",
    "        grad = tf.concat([[jaccard[0]], jaccard[1:] - jaccard[:-1]], 0)\n",
    "\n",
    "        return tf.tensordot(errors_sorted, grad, axes=1)\n",
    "\n",
    "    # Loop through each class using tf.while_loop\n",
    "    losses = tf.TensorArray(dtype=tf.float32, size=num_classes)\n",
    "\n",
    "    def loop_cond(c, losses):\n",
    "        return tf.less(c, num_classes)\n",
    "\n",
    "    def loop_body(c, losses):\n",
    "        loss_c = compute_class_loss(c)\n",
    "        losses = losses.write(c, loss_c)\n",
    "        return c + 1, losses\n",
    "\n",
    "    _, losses = tf.while_loop(loop_cond, loop_body, [start_class, losses])\n",
    "    return tf.reduce_mean(losses.stack())\n",
    "\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2]) + tf.reduce_sum(y_pred, axis=[1, 2])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    dice_loss_val = 1 - tf.reduce_mean(dice)\n",
    "\n",
    "    lovasz_loss_val = lovasz_softmax_loss(y_true, tf.nn.softmax(y_pred), ignore_background=False)\n",
    "\n",
    "    return lovasz_loss_val + dice_loss_val\n",
    "\n",
    "\n",
    "# Usage in model compilation\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=combined_loss,\n",
    "    metrics=class_wise_metrics(4)  # Number of classes\n",
    ")\n",
    "\n",
    "# ‚úÖ Train the Model with the Data Generator\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(X_train) // 16,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=10, \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss', \n",
    "            factor=0.5, \n",
    "            patience=3, \n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            'lovaszloss_unet++_xception.keras',\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T06:21:15.401477Z",
     "iopub.status.busy": "2025-04-07T06:21:15.401477Z",
     "iopub.status.idle": "2025-04-07T06:21:15.957602Z",
     "shell.execute_reply": "2025-04-07T06:21:15.957602Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting data from the history object\n",
    "history_dict = history.history\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plotting loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict['loss'], label='Training Loss')\n",
    "plt.plot(history_dict['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# If accuracy is available, plot training and validation accuracy\n",
    "if 'accuracy' in history_dict:\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_dict['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history_dict['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T06:21:15.959603Z",
     "iopub.status.busy": "2025-04-07T06:21:15.959603Z",
     "iopub.status.idle": "2025-04-07T06:21:49.465584Z",
     "shell.execute_reply": "2025-04-07T06:21:49.465584Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# ‚úÖ RGB to Class Index Conversion (for the test masks)\n",
    "RGB_TO_CLASS = {\n",
    "    (255, 0, 0): 1,  # Brain\n",
    "    (0, 255, 0): 2,  # CSP\n",
    "    (0, 0, 255): 3,  # LV\n",
    "    (0, 0, 0): 0     # Background\n",
    "}\n",
    "\n",
    "# ‚úÖ Function to convert RGB masks to class index masks\n",
    "def rgb_to_class_mask(rgb_mask):\n",
    "    # Create a mask initialized with zeros (for background class)\n",
    "    class_mask = np.zeros(rgb_mask.shape[:2], dtype=int)\n",
    "\n",
    "    # Loop through the RGB_TO_CLASS dictionary\n",
    "    for rgb, class_idx in RGB_TO_CLASS.items():\n",
    "        # Identify the pixels with the current RGB value and assign them the class index\n",
    "        match_mask = np.all(rgb_mask == np.array(rgb), axis=-1)\n",
    "        class_mask[match_mask] = class_idx\n",
    "\n",
    "    return class_mask\n",
    "\n",
    "# ‚úÖ Function to calculate Dice Similarity Coefficient (DSC)\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-15\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred)\n",
    "    return (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "# ‚úÖ Function to calculate IoU (Intersection over Union)\n",
    "def iou(y_true, y_pred):\n",
    "    intersection = np.sum(y_true * y_pred)\n",
    "    union = np.sum(y_true) + np.sum(y_pred) - intersection\n",
    "    return intersection / (union + 1e-15)\n",
    "\n",
    "# ‚úÖ Function to calculate Hausdorff Distance\n",
    "def hausdorff_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')  # Return inf if no points for either true or pred class\n",
    "\n",
    "    forward_hausdorff = directed_hausdorff(true_points, pred_points)[0]\n",
    "    reverse_hausdorff = directed_hausdorff(pred_points, true_points)[0]\n",
    "    return max(forward_hausdorff, reverse_hausdorff)\n",
    "\n",
    "# ‚úÖ Function to calculate Average Surface Distance (ASD)\n",
    "def average_surface_distance(y_true, y_pred):\n",
    "    true_points = np.array(np.where(y_true == 1)).T\n",
    "    pred_points = np.array(np.where(y_pred == 1)).T\n",
    "\n",
    "    if len(true_points) == 0 or len(pred_points) == 0:\n",
    "        return float('inf')  # Return inf if no points for either true or pred class\n",
    "\n",
    "    distances = []\n",
    "    for true_point in true_points:\n",
    "        distances.append(np.min(np.linalg.norm(pred_points - true_point, axis=1)))\n",
    "    return np.mean(distances)\n",
    "\n",
    "# ‚úÖ Function to evaluate the model on the test set class-wise\n",
    "def evaluate_classwise_metrics(model, X_test, y_test, num_classes=4, batch_size=16):\n",
    "    # Predict in batches\n",
    "    y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)  # Convert to class index prediction\n",
    "\n",
    "    # Convert y_test to class index format (since it's one-hot encoded)\n",
    "    y_test_class = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    # Initialize lists to store class-wise metrics\n",
    "    class_metrics ={i: {'dice': [], 'iou': [], 'precision': [], 'recall': [], 'f1': [], 'accuracy': [], 'hausdorff': [], 'asd': []} for i in range(num_classes)}\n",
    "\n",
    "    # Calculate metrics for each test sample\n",
    "    for i in range(len(X_test)):\n",
    "        true_mask = y_test_class[i]  # one-hot -> class index\n",
    "        pred_mask = y_pred[i]\n",
    "\n",
    "        # For each class (0: Background, 1: Brain, 2: CSP, 3: LV)\n",
    "        for class_idx in range(num_classes):\n",
    "            true_class_mask = (true_mask == class_idx).astype(int)\n",
    "            pred_class_mask = (pred_mask == class_idx).astype(int)\n",
    "\n",
    "            # Dice Coefficient\n",
    "            class_metrics[class_idx]['dice'].append(dice_coefficient(true_class_mask, pred_class_mask))\n",
    "            # IoU\n",
    "            class_metrics[class_idx]['iou'].append(iou(true_class_mask, pred_class_mask))\n",
    "            # Precision\n",
    "            class_metrics[class_idx]['precision'].append(precision_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # Recall\n",
    "            class_metrics[class_idx]['recall'].append(recall_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # F1 Score\n",
    "            class_metrics[class_idx]['f1'].append(f1_score(true_class_mask.flatten(), pred_class_mask.flatten(), zero_division=0))\n",
    "            # Accuracy\n",
    "            class_metrics[class_idx]['accuracy'].append(accuracy_score(true_class_mask.flatten(), pred_class_mask.flatten()))\n",
    "            # Hausdorff Distance\n",
    "            # class_metrics[class_idx]['hausdorff'].append(hausdorff_distance(true_class_mask, pred_class_mask))\n",
    "            # # Average Surface Distance\n",
    "            # class_metrics[class_idx]['asd'].append(average_surface_distance(true_class_mask, pred_class_mask))\n",
    "\n",
    "    # Print class-wise metrics in percentage\n",
    "    print(f\"{'Class':<10}{'Dice Coefficient (%)':<20}{'IoU (%)':<20}{'Precision (%)':<20}{'Recall (%)':<20}{'F1 Score (%)':<20}{'Accuracy (%)':<20}{'Hausdorff Distance':<20}{'Avg Surface Distance':<20}\")\n",
    "    print('-' * 180)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        print(f\"Class {class_idx}:\")\n",
    "        print(f\"  Dice Coefficient: {np.mean(class_metrics[class_idx]['dice']) * 100:.2f}%\")\n",
    "        print(f\"  IoU: {np.mean(class_metrics[class_idx]['iou']) * 100:.2f}%\")\n",
    "        print(f\"  Precision: {np.mean(class_metrics[class_idx]['precision']) * 100:.2f}%\")\n",
    "        print(f\"  Recall: {np.mean(class_metrics[class_idx]['recall']) * 100:.2f}%\")\n",
    "        print(f\"  F1 Score: {np.mean(class_metrics[class_idx]['f1']) * 100:.2f}%\")\n",
    "        print(f\"  Accuracy: {np.mean(class_metrics[class_idx]['accuracy']) * 100:.2f}%\")\n",
    "        # print(f\"  Hausdorff Distance: {np.mean(class_metrics[class_idx]['hausdorff']):.4f}\")\n",
    "        # print(f\"  Average Surface Distance: {np.mean(class_metrics[class_idx]['asd']):.4f}\")\n",
    "        print(\"-\" * 180)\n",
    "\n",
    "    # Evaluate on test set to print overall test accuracy and loss\n",
    "    test_loss, *test_metrics = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    for metric, value in zip(model.metrics_names[1:], test_metrics):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# ‚úÖ Call the evaluation function on the test set class-wise\n",
    "evaluate_classwise_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6940010,
     "sourceId": 11127979,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
